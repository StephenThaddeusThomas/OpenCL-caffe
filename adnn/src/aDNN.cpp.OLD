/**********************************************************************
Copyright ©2015 Advanced Micro Devices, Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:

•	Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
•	Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or
 other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
 DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
 OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
********************************************************************/
// to share code with between CPU and GPU

#include "aDNNInternal.hpp"
#include <string>
#include <malloc.h>

#define CNN_VERIFY 1

namespace adnn
{


	double CalculateErr(aDType c_val, aDType g_val)
	{
		double delta = abs(c_val - g_val);
		double nextafter_delta = nextafterf(min(abs(c_val), abs(g_val)), (aDType)INFINITY) - min(abs(c_val), abs(g_val));
		double err = delta / nextafter_delta;
		return err;
	}

	static
	const CDNN_Object * findUniqString(const std::map<std::string, const CDNN_Object *> & m, const std::string & str)
	{
		CDNN_Object * ret = NULL;
		std::map<std::string, const CDNN_Object *>::const_iterator it;

		for(it = m.begin(); it!=m.end(); ++it)
		{
			if (!(*it).first.compare(str))
			{
				return (it->second);
			}
		}
		return ret;
	}

	static
	const CDNN_Object * findUniqString(std::map<std::string, const CDNN_Object *> & m, const std::string & str)
	{
		CDNN_Object * ret = NULL;
		std::map<std::string, const CDNN_Object *>::iterator it;

		for(it = m.begin(); it!=m.end(); ++it)
		{
			if (!(*it).first.compare(str))
			{
				return (it->second);
			}
		}
		return ret;
	}

	static
	int addUniqString(std::map<std::string, const CDNN_Object *> & m, const std::string & str, const CDNN_Object * obj)
	{
		int ret = 1;
		const CDNN_Object * f_obj;
		if ( (f_obj = findUniqString(m, str) ) == NULL )
		{
			m[str] = obj;
		}
		return ret;
	}


	/*------------------------------------------------------------------------------------------------------------*/
	// CDNN_Object
	//
	/*------------------------------------------------------------------------------------------------------------*/


	CDNN_Object :: CDNN_Object()
	{
		parent_ = 0;
		wrapper_ = 0;
		ref_obj_ = 0;
		name_ = "";
		type_ = ADNN_OBJECT_GENERIC;

	}
		
	CDNN_Object::CDNN_Object(CDNN_OVX * _parent, void *_wrapper, std::string _name, ANN_OBJECT_TYPE _type, CDNN_Object * ref_obj)
	{
		parent_ = _parent;
		wrapper_ = _wrapper;
		name_ = _name;
		type_ = _type;
		ref_obj_ = ref_obj;
		if ( parent_ )
		{
			parent_->attachObj(this);
		}
	}

	CDNN_Object::~CDNN_Object(void)
	{
	}



/////////////////////////////////////////////////////////////////////////////
//
// CDNN_OVX
//
//////////////////////////////////////////////////////////////////////////////

	CDNN_OVX::CDNN_OVX(void) : CDNN_Object()
	{
	}

	CDNN_OVX:: ~CDNN_OVX(void)
	{

		cleanUp();
	}

	CDNN_OVX :: CDNN_OVX(CDNN_OVX * parent, void * wrapper, std:: string name) :
		CDNN_Object(0, wrapper, name,ADNN_OBJECT_LIBRARY)
		{
			attachObj(this);
			parent_ = this;

		}

	int CDNN_OVX::Init(void)
	{
		int ret = ADNN_SUCCESS;
		generic_comp_otions_ = 
#if CNN_VERIFY == 0

		std::string(" -cl-std=CL2.0  ") +


//		std::string("  -save-temps ") +
//		std::string("  -Wb,-hsail-reg-slots=8 -Wb,-hsail-reg32-pressure-limit=48 -Wb,-hsail-reg64-pressure-limit=48 ") +
//		std::string(" -march=hsail-64  ") +
#endif
//		std::string(" -cl-std=CL2.0  ") +
			std::string(" -I ") + std::string(_getcwd(NULL, 0)) + std::string("\\..\\aDNN-OVXimpl\\ ")
			;

		getaDNNOCL().setupCL();

// OCL setup
		cl_context Ctxt = getaDNNOCL().getClContext();
		cl_queue_properties prop[] = { 0 };
		cl_command_queue kernelQ = getaDNNOCL().getClQueue(0, prop);

		/* Setup clblas. */
		ret = clblasSetup();
		if (ret != CL_SUCCESS) {
			printf("clblasSetup() failed with %d\n", ret);
			adnn::getaDNNOCL().cleanup();
			return 1;
		}
		return(ret);
	}

	void CDNN_OVX :: attachObj(void * _obj)
	{
		objs_.push_back(_obj);
	}

	void CDNN_OVX :: removeObj(void * _obj)
	{
	  std::vector<void *>::iterator it;

		it = std::find (objs_.begin(), objs_.end(), _obj);
		if (it == objs_.end())
		{
			std::cout << "Object not found in LibDNN\n";
			return;
		}
		objs_.erase (it);
	}
	int CDNN_OVX::cleanUp(void)
	{
		int ret = ADNN_SUCCESS;
		for( std::vector<void*>::iterator i = objs_.begin(); i != objs_.end(); i++)
		{
			CDNN_Object * obj = (CDNN_Object*)(*i);
			if( obj->getType() != ADNN_OBJECT_LIBRARY)
			{
				delete obj;
			}
		}
		ret = adnn::getaDNNOCL().cleanup();
		objs_.clear();
		return (ret);
	}



///////////////////////////////////////////////////////
//
// DNN_OCL_kern_exe
//
//////////////////////////////////////////////////////
//	typedef std::pair<void*, size_t> ocl_arg;
//	typedef std::map<int, ocl_arg> ocl_args;
//	typedef std::vector<cl_event> ocl_wait_events;

CDNN_OCL_kern_exe::CDNN_OCL_kern_exe() : CDNN_Object()
{
	kernel_ = 0;
	queue_ = 0;
	completion_event_ = 0;

}

CDNN_OCL_kern_exe::CDNN_OCL_kern_exe(CDNN_OVX * parent, std::string name,
	std::string file_nm, std::string build_options,
	cl_kernel ocl_kern,
	std::vector<size_t> * glb_sz,
	std::vector<size_t> * lcl_sz,
	ocl_args * args,
	cl_command_queue queue
	)

{
	parent_ = parent;
	kern_src_file_ = file_nm;
	kernel_ = ocl_kern;
	if (glb_sz)
	{
		glb_sz_ = *glb_sz;
	}
	if (lcl_sz)
	{
		lcl_sz_ = *lcl_sz;
	}
	if (args)
	{
		args_ = *args;
	}
	queue_ = queue;
	completion_event_ = 0;
	kern_nm_ = name;
	kern_build_options_ = build_options;

}

		
CDNN_OCL_kern_exe::CDNN_OCL_kern_exe(const CDNN_OCL_kern_exe & copy)
{

	parent_ = copy.parent_;
	kern_src_file_ = copy.kern_src_file_;
	kern_nm_ = copy.kern_nm_;
	kern_src_string_ = copy.kern_src_string_;
	kern_build_options_ = copy.kern_build_options_;
	args_ = copy.args_;
	kernel_ = copy.kernel_;

	if (kernel_)
	{
		clRetainKernel(kernel_);
	}

	lcl_sz_ = copy.lcl_sz_;
	glb_sz_ = copy.glb_sz_;
	queue_ = copy.queue_;
	completion_event_ = copy.completion_event_;
	wait_events_ = copy.wait_events_;
}

int CDNN_OCL_kern_exe::ExecuteNoWait(
		ocl_args * args,
		cl_command_queue queue
	)
{
	int ret = CL_SUCCESS;

	cl_kernel ocl_kernel = getOclKern();

	if (args)
	{
		ocl_args::iterator ai;
		for (ai = (*args).begin(); ai != (*args).end(); ++ai)
		{
			int i = (*ai).first;
			ocl_arg arg = (*ai).second;
			ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);
		}

		CHECK_OPENCL_ERROR(ret, "parmeters failed.");

	}
	cl_command_queue ocl_queue = (!queue) ? getOclQueue() : queue;

	size_t g_wk[3] = { getGblSize()[0], getGblSize()[1], getGblSize()[2]};

	size_t l_wk[3] = { getLclSize()[0], getLclSize()[1], getLclSize()[2] };
	ret = clEnqueueNDRangeKernel(ocl_queue, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);

	CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");

	return(ret);
}

CDNN_OCL_kern_exe::~CDNN_OCL_kern_exe()
{
	if (kernel_)
	{
		clReleaseKernel(kernel_);
		kernel_ = 0;
	}
}


///////////////////////////////////////////////////////
//
// DNN_Dnet_layer
//
//////////////////////////////////////////////////////

	CDNN_Dnet_layer::CDNN_Dnet_layer(CDNN_OVX * parent, void * wrapper, std::string name, const void* parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Object(parent, wrapper, name,	ADNN_OBJECT_DNETLAYER, ref_layer)
{
	bottom_lyr_ = NULL;
	top_lyr_ = NULL;
	net_ = NULL;
	parameters_ = *(dnet_layer_parameters*)parameters;
	name_ = parameters_.name;
	inputs_.clear();
	outputs_.clear();
	internal_.clear();

	top_fwd_ = NULL;
	bot_fwd_ = NULL;
	top_fwd2_ = NULL;
	bot_fwd2_ = NULL;

	top_diff_ = NULL;
	bot_diff_ = NULL;
	bottom_lyr_ = NULL;
	top_lyr_ = NULL;

	n_passes_ = 0;
	cl_kernels_.clear();
	n_out_pix_horiz_ = 1;
	n_out_pix_vert_ = 1;
	ocl_group_sz0_ = 8;
	ocl_group_sz1_ = 8;
	ocl_group_lg2sz0_ = 3;
	ocl_group_lg2sz1_ = 3;
	n_outs_ = 1;
	n_out_pix_horiz_bwd_ = 1;
	n_out_pix_vert_bwd_ = 1;
	ocl_group_bwd_sz0_ = 8;
	ocl_group_bwd_sz1_ = 8;
	ocl_group_bwd_lg2sz0_ = 3;
	ocl_group_bwd_lg2sz1_ = 3;


	do_timing_ = false;
	processing_time_ = 0;

//	n_iter_ = parent->
}

CDNN_Dnet_layer :: ~CDNN_Dnet_layer( void )
{
	for (std::map<std::string, cl_kernel>::iterator it = cl_kernels_.begin(); it != cl_kernels_.end(); ++it)
	{
		if ((*it).second)
		{
			clReleaseKernel((*it).second);
		}
	}
	cl_kernels_.clear();

}


int CDNN_Dnet_layer::getEdgesFwd(CDNN_Object **bot_tens, CDNN_Object **top_tens, bool & run_it)
{
	int ret = 0;

	internalSetup();

	(*bot_tens) = &getBotFwd();
	(*top_tens) = &getTopFwd();
	run_it = true;

	return(ret);
}

int CDNN_Dnet_layer::getEdgesBwd(CDNN_Object **bot_tens, CDNN_Object **top_tens, bool & run_it)
{
	int ret = 0;
	if (doNeedBackProp())
	{
		internalSetupBwd();

	
		(*bot_tens) = &getBotDiff();
		(*top_tens) = &getTopDiff();
		run_it = true;

	}

	return(ret);
}

int CDNN_Dnet_layer::makeBwdDiff(void)
{
	int ret = 0;
	CDNN_Tensor<aDType> & bot = getBotFwd();


	CDNN_Tensor<aDType> & bot_diff_tens = getBotDiff();
	if (!bot_diff_tens.IsInited())
	{
		bot_diff_tens.setName(getBotNameBwd());
		bot_diff_tens.setParent(parent_);

		bot_diff_tens.initTensor(bot.getNDim(), bot.getDims());

	}
	return(ret);
}


int CDNN_Dnet_layer :: Initialize(void)
{
	int ret = 0;
	CDNN_Tensor<aDType> * weights_tens = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
	if ( weights_tens) 
	{
		weights_tens->allocTensor();
		aDType *data = weights_tens->accessTensor(CL_MAP_WRITE_INVALIDATE_REGION);
		if (data)
		{
		    size_t size = weights_tens->getSize();

			for(size_t i = 0; i < size; i++)
			{
				data[i] = (aDType)((2.f * (aDType)rand() / (aDType)RAND_MAX - 1.0f))* (aDType)getStd() * 3;
			}
		}
		weights_tens->commitTensor();
#if CNN_VERIFY
		{
			CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));

			CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
			weights_v.deepCopyBlob(weights);

		}

#endif

	}

	CDNN_Tensor<aDType> * bias_tens = (CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
	if (bias_tens)
	{
		bias_tens->allocTensor();
		aDType *data = bias_tens->accessTensor(CL_MAP_WRITE_INVALIDATE_REGION);
		if (data)
		{
			size_t size = bias_tens->getSize();

			for (size_t i = 0; i < size; i++)
			{
				data[i] = (aDType)((2.f * (aDType)rand() / (aDType)RAND_MAX - 1.0f)) * (aDType)getStd() * 3;
			}
		}
		bias_tens->commitTensor();
#if CNN_VERIFY
		{
			CDNN_Tensor<aDType> & bias = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
			CDNN_Tensor<aDType> & bias_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.verify"));
			bias_v.deepCopyBlob(bias);

		}

#endif
	}
	return(ret);
}


int CDNN_Dnet_layer::UpdateRates(aDType & local_rate, aDType & local_decay,
	aDType layer_learning_rate, aDType layer_weight_decay)
{
	int ret = 0;
	double rate = getNet().getNetParams().get_base_lr();
	double weight_decay = getNet().getNetParams().get_weight_decay();
	local_rate = (aDType)(rate * layer_learning_rate);
	local_decay = (aDType)(weight_decay * layer_weight_decay);

	return(ret);
}

int CDNN_Dnet_layer::MakeVerificationDouble(const CDNN_Tensor<aDType> & orig)
{
	int ret = 0;
	std::string v_name = orig.getName() + ".verify";
	MakeDoubleInternal(orig, v_name, _CBUF_MEM_SYS_ONLY);
	return (ret);
}


int CDNN_Dnet_layer::MakeDoubleInternal(const CDNN_Tensor<aDType> & orig, const std::string d_name, int d_mem_flags)
{
	int ret = 0;
	CDNN_Tensor<aDType> * d = new CDNN_Tensor<aDType>(&getParent(), NULL, d_name);
	assert(d);
	d->initTensor(orig.getNDim(), orig.getDims());
	addInternal(d_name, d);
	d->allocTensor(d_mem_flags);
	return (ret);
}


int CDNN_Dnet_layer::Update(void)
{

	return(0);
}

int CDNN_Dnet_layer :: Deinitialize(void)
{
	return(0);
}

int CDNN_Dnet_layer :: VerifyInput(void)
{
	return(0);
}

int CDNN_Dnet_layer :: VerifyOutput(void)
{
	return(0);
}


int CDNN_Dnet_layer::internalSetup(void)
{
	return(0);
}


int CDNN_Dnet_layer :: Execute(void)
{
	return(0);
}

int CDNN_Dnet_layer::ExecuteHost(void)
{
	int ret = 0;
	return(ret);
}

int CDNN_Dnet_layer::ExecuteBwd(void)
{
	int ret = 0;
	return(ret);
}

int CDNN_Dnet_layer::ExecuteBwdHost(void)
{
	int ret = 0;
	return(ret);
}

int CDNN_Dnet_layer::internalVerifyBwd(void)
{
	int ret = 0;
	return(ret);
}


int CDNN_Dnet_layer:: internalVerify(void)
{
	int ret = 0;
	return(ret);
}

int CDNN_Dnet_layer::UpdateHost(void)
{
	int ret = 0;
	return(ret);
}

int CDNN_Dnet_layer::internalVerifyUpdate(void)
{
	int ret = 0;
	return(ret);
}



int CDNN_Dnet_layer::internalSetupBwd(void)
{
	return(0);
}

int CDNN_Dnet_layer :: addInput(const std::string &tens, const CDNN_Object * obj)
{
	int ret = 0;
	ret = addUniqString(inputs_, tens, obj);
	return(ret);
}

int CDNN_Dnet_layer :: addOutput(const std::string &tens, const CDNN_Object * obj)
{
	int ret = 0;
	ret = addUniqString(outputs_, tens, obj);
	return(ret);
}

int CDNN_Dnet_layer :: addInternal(const std::string &tens, const CDNN_Object * obj)
{
	int ret = 0;
	ret = addUniqString(internal_, tens, obj);
	return(ret);
}

const CDNN_Object * CDNN_Dnet_layer :: findInput(const std::string & tens) const
{
	const CDNN_Object *ret = findUniqString(inputs_, tens);
	return(ret);
}

const CDNN_Object * CDNN_Dnet_layer :: findOutput(const std::string & tens) const
{

	const CDNN_Object *ret = findUniqString(outputs_, tens);
	return(ret);
}

const CDNN_Object * CDNN_Dnet_layer :: findInternal(const std::string &  tens) const
{

	const CDNN_Object *ret = findUniqString(internal_, tens);
	return(ret);
}



//////////////////////////////////////////////////////////////////////////////////////////////
//
//
////////////////////////////////////////////////////////////////////////////////////////////////


CDNN_Dnet_layer_data :: CDNN_Dnet_layer_data(CDNN_OVX * parent, void * wrapper, std:: string name, const void* parameters):
			CDNN_Dnet_layer(parent, wrapper, name,	parameters)
{

// pass n channel as n of outputs for data
	parameters_.num_output = parameters_.n_channels;
}

int CDNN_Dnet_layer_data :: internalSetup(void)
{
	int ret = 0;
	size_t outputs = getNOutputs();
	size_t cols =  getInputWidth();
	size_t rows =   getInpuHeight( );
	size_t batch_sz = getBatchSz();

	if (!bottom_lyr_)
	{
		outputs = getNOutputs();
		cols =  getInputWidth();
		rows =   getInpuHeight( );
		batch_sz = getBatchSz();
		std::string top_nm = getTop();
		CDNN_Tensor<aDType> * top = (CDNN_Tensor<aDType> *)findOutput(top_nm);
		if (top && !top->IsInited())
		{
			top->setName(top_nm);
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = cols;
			dims[ANN_TENSOR_HEIGHT] = rows;
			dims[ANN_TENSOR_DEPTH] = outputs;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);
		}
	}



	return(ret);
}




int CDNN_Dnet_layer_data :: Execute(void)
{
	int ret = 0;
	return(ret);
}

////////////////////////////////////////////////////////////
//
///////////////////////////////////////////////////////////
#define _ANN_MM_TRANSPOSE 1
template <typename Dtype>
void _ANN_mm_cpu(const Dtype * a_ptr, size_t a_cols, size_t a_rows, size_t a_stride, int a_flags,
	const Dtype * b_ptr, size_t b_cols, size_t b_rows, size_t b_stride, int b_flags,
	Dtype * c_ptr, size_t c_cols, size_t c_rows, size_t c_stride, int c_flags,
	Dtype alpha, Dtype beta)
{
	// mA

	// mB

	// mC

	if ((!(a_flags & _ANN_MM_TRANSPOSE) && !(b_flags & _ANN_MM_TRANSPOSE) && ((a_cols != b_rows) || (a_rows != c_rows) || (b_cols != c_cols)))
		|| ((a_flags & _ANN_MM_TRANSPOSE) && (b_flags & _ANN_MM_TRANSPOSE) && ((a_rows != b_cols) || (a_cols != c_rows) || (b_rows != c_cols)))
		|| ((a_flags & _ANN_MM_TRANSPOSE) && !(b_flags & _ANN_MM_TRANSPOSE) && ((a_rows != b_rows) || (a_cols != c_rows) || (b_cols != c_cols)))
		|| (!(a_flags & _ANN_MM_TRANSPOSE) && (b_flags & _ANN_MM_TRANSPOSE) && ((a_cols != b_cols) || (a_rows != c_rows) || (b_rows != c_cols)))
		)
	{
		printf("MM_CPU EROOE; %d %d   %d %d   %d %d\n", a_cols, a_rows, b_cols, b_rows, c_rows, c_cols);
	}

	size_t inner_loop = (!(a_flags & _ANN_MM_TRANSPOSE)) ? a_cols : a_rows;

	if (!(a_flags & _ANN_MM_TRANSPOSE) && !(b_flags & _ANN_MM_TRANSPOSE))
	{
		for (size_t n = 0; n < c_rows; ++n)
		{
			for (size_t k = 0; k < c_cols; ++k)
			{
				Dtype mm_e = 0;
				for (size_t m = 0; m < inner_loop; ++m)
				{
					mm_e += a_ptr[n*a_stride + m] * b_ptr[m*b_stride + k];
				}
				c_ptr[n*c_stride + k] = beta * c_ptr[n*c_stride + k] + alpha * mm_e;
			}
		}
	}
	else if ((a_flags & _ANN_MM_TRANSPOSE) && !(b_flags & _ANN_MM_TRANSPOSE))
	{
		for (size_t n = 0; n < c_rows; ++n)
		{
			for (size_t k = 0; k < c_cols; ++k)
			{
				Dtype mm_e = 0;
				for (size_t m = 0; m < inner_loop; ++m)
				{
					mm_e += a_ptr[m*a_stride + n] * b_ptr[m*b_stride + k];
#if 0
					if (
						(n == 0 && k == 7
						|| n == 2 && k == 6
						|| n == 4 && k == 5
						|| n == 10 && k == 2		
						|| n == 12 && k == 1
						|| n == 14 && k == 0
						/*
						|| n == 7 && k == 9
						|| n == 8 && k == 8
						|| n == 10 && k == 3
						|| n == 11 && k == 2
						|| n == 12 && k == 1
						|| n == 13 && k == 0
						*/
						)
						&& a_ptr[m*a_stride + n] * b_ptr[m*b_stride + k] != 0
						)
					{
						printf("C:mm: %d %d %d   %11.9f %11.9f %11.9f %11.9f\n",
							n, k, m,
							mm_e, a_ptr[m*a_stride + n], b_ptr[m*b_stride + k], a_ptr[m*a_stride + n] * b_ptr[m*b_stride + k]);
					}
#endif
				}
				c_ptr[n*c_stride + k] = beta * c_ptr[n*c_stride + k] + alpha * mm_e;
			}
		}
	}
	else if (!(a_flags & _ANN_MM_TRANSPOSE) && (b_flags & _ANN_MM_TRANSPOSE))
	{
		for (size_t n = 0; n < c_rows; ++n)
		{
			for (size_t k = 0; k < c_cols; ++k)
			{
				Dtype mm_e = 0;

				for (size_t m = 0; m < inner_loop; ++m)
				{
					mm_e += a_ptr[n*a_stride + m] * b_ptr[k*b_stride + m];
#if 0
					if (n == 0 && k == 6 && a_ptr[n*a_stride + m] * b_ptr[k*b_stride + m] != 0)
					{
						printf("%4d  %11.9f %11.9f %11.9f\n", m, mm_e, a_ptr[n*a_stride + m], b_ptr[k*b_stride + m]);
					}
#endif
				}
				c_ptr[n*c_stride + k] = beta * c_ptr[n*c_stride + k] + alpha * mm_e;
			}
		}
	}
	else
	{
		for (size_t n = 0; n < c_rows; ++n)
		{
			for (size_t k = 0; k < c_cols; ++k)
			{
				Dtype mm_e = 0;
				for (size_t m = 0; m < inner_loop; ++m)
				{
					c_ptr[n*c_stride + k] += a_ptr[m*a_stride + n] * b_ptr[k*b_stride + m];
				}
				c_ptr[n*c_stride + k] = beta * c_ptr[n*c_stride + k] + alpha * mm_e;
			}
		}
	}

}



template <typename Dtype>
void _ANN_im2col_cpu(const Dtype* data_im, const int channels,
	const int height, const int width, const int ksize, const int pad,
	const int stride, Dtype* data_col) {
	int height_col = (height + 2 * pad - ksize) / stride + 1;
	int width_col = (width + 2 * pad - ksize) / stride + 1;
	int channels_col = channels * ksize * ksize;
	for (int c = 0; c < channels_col; ++c) {
		int w_offset = c % ksize;
		int h_offset = (c / ksize) % ksize;
		int c_im = c / ksize / ksize;
		for (int h = 0; h < height_col; ++h) {
			for (int w = 0; w < width_col; ++w) {
				int h_pad = h * stride - pad + h_offset;
				int w_pad = w * stride - pad + w_offset;
				if (h_pad >= 0 && h_pad < height && w_pad >= 0 && w_pad < width)
				{
					data_col[(c * height_col + h) * width_col + w] =
						data_im[(c_im * height + h_pad) * width + w_pad];
				}
				else
				{
					data_col[(c * height_col + h) * width_col + w] = 0;
				}
			}
		}
	}
}

template <typename Dtype>
void _ANN_col2im_cpu(const Dtype* data_col, const int channels,
	const int height, const int width, const int ksize, const int pad,
	const int stride, Dtype* data_im) {
	memset(data_im, 0, sizeof(Dtype) * height * width * channels);
	int height_col = (height + 2 * pad - ksize) / stride + 1;
	int width_col = (width + 2 * pad - ksize) / stride + 1;
	int channels_col = channels * ksize * ksize;
	for (int c = 0; c < channels_col; ++c) {
		int w_offset = c % ksize;
		int h_offset = (c / ksize) % ksize;
		int c_im = c / ksize / ksize;
		for (int h = 0; h < height_col; ++h) {
			for (int w = 0; w < width_col; ++w) {
				int h_pad = h * stride - pad + h_offset;
				int w_pad = w * stride - pad + w_offset;
				if (h_pad >= 0 && h_pad < height && w_pad >= 0 && w_pad < width)
				{
					data_im[(c_im * height + h_pad) * width + w_pad] +=
						data_col[(c * height_col + h) * width_col + w];
#if 0
					if (c_im == 0 && h_pad == 0 && w_pad == 2)
					{
						printf("C:c2i: %d %d %d %d %d    %14.12f %14.12f\n", c, w, h, w_pad, h_pad, data_im[(c_im * height + h_pad) * width + w_pad], data_col[(c * height_col + h) * width_col + w]);
					}
#endif
				}
			}
		}
	}
}


///////////////////////////////////////////////////////////////////////////////////
//
// layer fconnect
//
////////////////////////////////////////////////////////////////////////////////////

CDNN_Dnet_layer_fconnect::CDNN_Dnet_layer_fconnect(CDNN_OVX * parent, void * wrapper, std::string name, const void* parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Dnet_layer(parent, wrapper, name,	parameters, ref_layer)
{
	n_mt_out_pix_horiz_ = 0;
	n_mt_out_pix_vert_ = 0;
	ocl_mt_group_sz0_ = 0;
	ocl_mt_group_sz1_ = 0;

	ocl_sr_group_sz0_ = 0;
}

/*------------------------------------------------------------------------------------------------------------------

M - batch size
K - number of inputs
N - number of outputs

forward pass:
bot: MxK (M - number of rows)
weights: NxK
bias: 1XN

bot = bot * transpose(weights): MxN + M * bias

--------------------------------------------------------------------------------------------------------------------*/


int CDNN_Dnet_layer_fconnect :: internalSetup(void)
{
	int ret = 0;

	size_t outputs = getNOutputs();
	size_t batch_sz = 0;
	size_t inputs = 0;
	CDNN_Tensor<aDType> * bot = NULL;
	

	if (getBottomLyr())
	{
		bot = (CDNN_Tensor<aDType> *)&getBottomLyr()->getTopFwd();
		size_t w = bot->getDim(ANN_TENSOR_WIDTH);
		size_t h = bot->getDim(ANN_TENSOR_HEIGHT);
		size_t c = bot->getDim(ANN_TENSOR_DEPTH);
		inputs = w*h*c;
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);

	}
	CDNN_Tensor<aDType> * top = NULL;
	if (getTopLyr() && (top = (CDNN_Tensor<aDType> *)&getTopFwd()))
	{
		if (!top->IsInited())
		{
			top->setName(getTopName());
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = outputs;
			dims[ANN_TENSOR_HEIGHT] = 1;
			dims[ANN_TENSOR_DEPTH] = 1;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);
			top->allocTensor();

		}



		// img to gemm???

		CDNN_Tensor<aDType> * weights_tens = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		if (!weights_tens)
		{
			weights_tens = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights"), (size_t)inputs,(size_t)outputs);
			addInternal(std::string("weights"), weights_tens);
			weights_tens->allocTensor();
		}
#if 0
		// add for verification
		CDNN_Tensor<aDType> * top_verify2 = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.verify2"), top->getDim(ANN_TENSOR_WIDTH), top->getDim(ANN_TENSOR_HEIGHT), top->getDim(ANN_TENSOR_DEPTH), top->getDim(ANN_TENSOR_4THDIM));
		addInternal(std::string("top.verify2"), top_verify2);
		top_verify2->allocTensor();

		CDNN_Tensor<aDType> * weights_transp = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.transposed"));
		if (!weights_transp)
		{
			weights_transp = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.transposed"), (size_t)inputs, (size_t)outputs);
			addInternal(std::string("weights.transposed"), weights_transp);
			weights_transp->allocTensor();
		}

#endif
		CDNN_Tensor<aDType> * bias = (CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
		if (!bias)
		{
			// instansiate with proper parameters

			// TO DO!!!!
			bias = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("bias"), (size_t)outputs, 1);
			addInternal(std::string("bias"), bias);
			bias->allocTensor();
		}
		// verify tensor parameters
		else
		{
		}
	


#if CNN_VERIFY

		// add system only for verification
		CDNN_Tensor<aDType> * top_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.verify"), top->getDim(ANN_TENSOR_WIDTH), top->getDim(ANN_TENSOR_HEIGHT), top->getDim(ANN_TENSOR_DEPTH), top->getDim(ANN_TENSOR_4THDIM));
		addInternal(std::string("top.verify"), top_verify);
		top_verify->allocTensor(_CBUF_MEM_SYS_ONLY);
#endif


		CDNN_Tensor<aDType> * mB = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		CDNN_Tensor<aDType> * mA = bot;
		CDNN_Tensor<aDType> * mC = top;

		int mA_width = (int)(mA->getDim(ANN_TENSOR_WIDTH) * mA->getDim(ANN_TENSOR_HEIGHT) * mA->getDim(ANN_TENSOR_DEPTH));
		int mA_height = (int)(mA->getDim(ANN_TENSOR_4THDIM));
		int mA_stride = (int)mA->getStride(ANN_TENSOR_DEPTH);
		int mB_width = (int)(mB->getDim(ANN_TENSOR_WIDTH));
		int mB_height = (int)(mB->getDim(ANN_TENSOR_HEIGHT));
		int mB_stride = (int)mB->getStride(ANN_TENSOR_WIDTH);
		int mC_width = (int)(mC->getDim(ANN_TENSOR_WIDTH) * mC->getDim(ANN_TENSOR_HEIGHT) * mC->getDim(ANN_TENSOR_DEPTH));
		int mC_height = (int)(mC->getDim(ANN_TENSOR_4THDIM));
		int mC_stride = (int)mC->getStride(ANN_TENSOR_DEPTH);

		int horiz_read_len = 16;
		int horiz_read_loop = 1;
		int horiz_read_lg2len = (int)ceil(log((double)horiz_read_len) / log(2.));
		int outer_loop = (mA_width + horiz_read_len - 1) / horiz_read_len;
		int out_n_cols;
		int out_n_rows;
		int priv_buf_len = 4;
		int accum_loop = (horiz_read_len * horiz_read_loop) / priv_buf_len;

		ocl_group_sz0_ = 8;
		ocl_group_sz1_ = 8;
		ocl_group_lg2sz1_ = (int)ceil(log((double)ocl_group_sz1_) / log(2.));
		ocl_group_lg2sz0_ = (int)ceil(log((double)ocl_group_sz0_) / log(2.));

		out_n_cols = mC_width / ocl_group_sz0_;
		n_out_pix_horiz_ = (out_n_cols > 4) ? 4 : (out_n_cols == 0) ? 1 : out_n_cols;
//		n_out_pix_horiz_ = (out_n_cols > 2) ? 2 : (out_n_cols == 0) ? 1 : out_n_cols;
		out_n_rows = mC_height / ocl_group_sz1_;
		n_out_pix_vert_ = (out_n_rows > 4) ? 4 : (out_n_rows == 0) ? 1 : out_n_rows;
//		n_out_pix_vert_ = (out_n_rows > 2) ? 2 : (out_n_rows == 0) ? 1 : out_n_rows;

		int vert_read_step = (ocl_group_sz0_ * ocl_group_sz1_) / horiz_read_len;
		int vert_mB_read_loop = (ocl_group_sz0_ * n_out_pix_horiz_) / vert_read_step;
		int vert_mA_read_loop = (ocl_group_sz1_ * n_out_pix_vert_) / vert_read_step;

		std::string comp_options =
			std::string(" -D _DNN_MM_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz_)
			+ std::string(" -D _DNN_MM_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert_)
			+ std::string(" -D _DNN_MM_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0_)
			+ std::string(" -D _DNN_MM_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1_)
			+ std::string(" -D _DNN_MM_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0_)
			+ std::string(" -D _DNN_MM_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1_)
			+ std::string(" -D _DNN_MM_READ_LG2=") + std::to_string((long long)horiz_read_lg2len)
			+ std::string(" -D _DNN_MM_HORIZ_READ_LOOP=") + std::to_string((long long)horiz_read_loop)
			+ std::string(" -D _DNN_MM_OUTER_LOOP=") + std::to_string((long long)outer_loop)
			+ std::string(" -D _DNN_MM_MA_VERT_READ_LOOP=") + std::to_string((long long)vert_mA_read_loop)
			+ std::string(" -D _DNN_MM_MA_VERT_READ_STEP=") + std::to_string((long long)vert_read_step)
			+ std::string(" -D _DNN_MM_MB_VERT_READ_LOOP=") + std::to_string((long long)vert_mB_read_loop)
			+ std::string(" -D _DNN_MM_MB_VERT_READ_STEP=") + std::to_string((long long)vert_read_step)
			+ std::string(" -D _DNN_MM_MA_WIDTH=") + std::to_string((long long)mA_width)
			+ std::string(" -D _DNN_MM_MA_HEIGHT=") + std::to_string((long long)mA_height)
			+ std::string(" -D _DNN_MM_MA_STRIDE=") + std::to_string((long long)mA_stride)
			+ std::string(" -D _DNN_MM_MB_WIDTH=") + std::to_string((long long)mB_width)
			+ std::string(" -D _DNN_MM_MB_HEIGHT=") + std::to_string((long long)mB_height)
			+ std::string(" -D _DNN_MM_MB_STRIDE=") + std::to_string((long long)mB_stride)
			+ std::string(" -D _DNN_MM_MC_WIDTH=") + std::to_string((long long)mC_width)
			+ std::string(" -D _DNN_MM_MC_HEIGHT=") + std::to_string((long long)mC_height)
			+ std::string(" -D _DNN_MM_MC_STRIDE=") + std::to_string((long long)mC_stride)
			+ std::string(" -D _DNN_MM_ACCUM_LOOP=") + std::to_string((long long)accum_loop)
			+ std::string(" -D _DNN_MM_PRV_BUF=") + std::to_string((long long)priv_buf_len)
	
			;


#if 1
		// THIS ARTIFICIAL for NOW to KEEP COMPILER HAPPY
		{
			CDNN_Tensor<aDType> * mB = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
			CDNN_Tensor<aDType> * mA = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));

			int mA_width = (int)mA->getDim(ANN_TENSOR_WIDTH);
			int mA_height = (int)mA->getDim(ANN_TENSOR_HEIGHT);
			int mA_stride = (int)mA->getStride(ANN_TENSOR_WIDTH);
			int mB_stride = (int)mB->getStride(ANN_TENSOR_WIDTH);


			ocl_mt_group_sz0_ = 16;
			ocl_mt_group_sz1_ = 16;

			out_n_cols = mA_height / ocl_mt_group_sz0_;
			n_mt_out_pix_horiz_ = (out_n_cols > 2) ? 2 : (out_n_cols == 0) ? 1 : out_n_cols;
			out_n_rows = mA_width / ocl_mt_group_sz1_;
			n_mt_out_pix_vert_ = (out_n_rows > 2) ? 2 : (out_n_rows == 0) ? 1 : out_n_rows;

			int min_n_mt = min(n_mt_out_pix_horiz_, n_mt_out_pix_vert_);
			n_mt_out_pix_horiz_ = n_mt_out_pix_vert_ = min_n_mt;

			comp_options +=
				(
				std::string(" -D _DNN_MT_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_mt_out_pix_horiz_)
				+ std::string(" -D _DNN_MT_N_VERT_OUT_PIX=") + std::to_string((long long)n_mt_out_pix_vert_)
				+ std::string(" -D _DNN_MT_GROUP_SZ0=") + std::to_string((long long)ocl_mt_group_sz0_)
				+ std::string(" -D _DNN_MT_GROUP_SZ1=") + std::to_string((long long)ocl_mt_group_sz1_)
				+ std::string(" -D _DNN_MT_MA_WIDTH=") + std::to_string((long long)mA_width)
				+ std::string(" -D _DNN_MT_MA_HEIGHT=") + std::to_string((long long)mA_height)
				+ std::string(" -D _DNN_MT_MA_STRIDE=") + std::to_string((long long)mA_stride)
				+ std::string(" -D _DNN_MT_MB_STRIDE=") + std::to_string((long long)mB_stride)
				)
				;
		}
		{

			int grp_sz0 = 256; //_DNN_SR_GROUP_SZ0
			int mA_stride = 1; // _DNN_SR_MA_STRIDE
			int mA_row_loop = 1; // _DNN_SR_MA_ROW_LOOP
			int mA_width = 1; // _DNN_SR_MA_WIDTH)
			comp_options +=
				(
				std::string(" -D _DNN_SR_GROUP_SZ0=") + std::to_string((long long)grp_sz0)
				+ std::string(" -D _DNN_SR_MA_WIDTH=") + std::to_string((long long)mA_width)
				+ std::string(" -D _DNN_SR_MA_STRIDE=") + std::to_string((long long)mA_stride)
				+ std::string(" -D _DNN_SR_MA_ROW_LOOP=") + std::to_string((long long)mA_row_loop)
				)
				;

		}
#endif
		comp_options += 
//			std::string("  -Wb,-hsail-reg-slots=8 -Wb,-hsail-reg32-pressure-limit=48 -Wb,-hsail-reg64-pressure-limit=48 ") +
			parent_->getGenericCompOptions();

		std::string kernel_file = "aDNNMatMat.cl";
		std::string kernel_name = "aDNN_FC"; // "aDNN_MM_TP";


		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;

#if 0
		kernel_file = "aDNNMatMat.cl";
		kernel_name = "aDNN_MatTrans";


		ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;
#endif
	}
	return(ret);
}




int CDNN_Dnet_layer_fconnect :: Execute(void)
{
	int ret = -1;
	cl_command_queue convQ = getaDNNOCL().getClQueue(0);
	std::string bot_nm = getBottom();
	std::string top_nm = getTop();
	CDNN_Tensor<aDType> * mA = (CDNN_Tensor<aDType> *)findInput(bot_nm);
//	CDNN_Tensor<aDType> * top = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify2"));
	CDNN_Tensor<aDType> * mB, *weights;
	mB = weights = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
	CDNN_Tensor<aDType> * mC = (CDNN_Tensor<aDType> *)findOutput(top_nm);
	CDNN_Tensor<aDType> * bias = (CDNN_Tensor<aDType> *)findInternal(std::string("bias"));



	if (mC && mA && mB && weights /*&& top*/ && bias)
	{

		int iter = getNTimingIter();
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{
			cl_kernel ocl_kernel = 0;
			cl_mem mA_mem = 0;
			cl_mem mB_mem = 0;
			int n_arg = 0;


			// run MM
			ocl_kernel = cl_kernels_[getName() + "." + "aDNN_FC"];
			mA_mem = mA->getCLMem();
			cl_mem mC_mem = mC->getCLMem();
			mB_mem = mB->getCLMem();
			cl_mem Bias_mem = bias->getCLMem();
			int w = (int)mC->getDim(ANN_TENSOR_WIDTH);
			int h = (int)mC->getDim(ANN_TENSOR_HEIGHT);
			int c = (int)mC->getDim(ANN_TENSOR_DEPTH);
			int height_out = (int)mC->getDim(ANN_TENSOR_4THDIM);
			int width_out = w*h*c;

			int i_n_group_horiz = (width_out + ocl_group_sz0_ * n_out_pix_horiz_ - 1) / (ocl_group_sz0_ * n_out_pix_horiz_);
			int i_n_group_vert = (height_out + ocl_group_sz1_ * n_out_pix_vert_ - 1) / (ocl_group_sz1_ * n_out_pix_vert_);


			size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };

			size_t g_wk[3] = { i_n_group_horiz * l_wk[0], i_n_group_vert * l_wk[1], 1 };

			n_arg = 0;

			ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mA_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mB_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &Bias_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mC_mem);

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
			CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");
			//TO DO: SELECT MUL2 with a proper width/height ratio

#if 0
			int transposeA = 0;
			int transposeB = 0;
			aDType alpha = 1;
			aDType beta = 0;
			top->mul2(*mA, *weights, transposeA, transposeB, alpha, beta);
#endif
		}
		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


		// TODO : ADD BIAS !!!!



		// verify
#if CNN_VERIFY
		internalVerify();
#endif
		int height = (int)weights->getDim(ANN_TENSOR_HEIGHT);
		int width = (int)weights->getDim(ANN_TENSOR_WIDTH);
		int batch_sz = (int)mA->getDim(ANN_TENSOR_4THDIM);
		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: fully connected: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: LxFxB : %dx%dx%d\n", ident, " ", width, height, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, (double)(2 * width* height* batch_sz * iter) / (processing_time_ * 1000000000));
			}
		}
		ret = 0;
	}

	return(ret);
}

int CDNN_Dnet_layer_fconnect::ExecuteHost(void)
{
	int ret = 0;
	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	CDNN_Tensor<aDType> * bot = &getBotFwd();
	CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal("weights");
//	CDNN_Tensor<aDType> * mB = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
	CDNN_Tensor<aDType> * bias = (CDNN_Tensor<aDType> *)findInternal(std::string("bias"));


	if (top_verify && bot && weights && bias)
	{
		int w = (int)bot->getDim(ANN_TENSOR_WIDTH);
		int h = (int)bot->getDim(ANN_TENSOR_HEIGHT);
		int c = (int)bot->getDim(ANN_TENSOR_DEPTH);
		int in_cols = w*h*c;
		int in_rows = (int)bot->getDim(ANN_TENSOR_4THDIM);
		int weight_cols = (int)weights->getDim(ANN_TENSOR_WIDTH);
		int weights_rows = (int)weights->getDim(ANN_TENSOR_HEIGHT);

		int bot_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);
		int weights_stride = (int)weights->getStride(ANN_TENSOR_WIDTH);
		int top_stride = (int)top_verify->getStride(ANN_TENSOR_WIDTH);
//		int we_t_str = (int)mB->getStride(ANN_TENSOR_WIDTH);

		int top_rows = (int)top_verify->getDim(ANN_TENSOR_4THDIM);
		int top_cols = (int)top_verify->getDim(ANN_TENSOR_WIDTH);

		aDType * bot_ptr = bot->accessTensor(CL_MAP_READ);
		aDType * top_ptr = top_verify->accessTensor(CL_MAP_WRITE);
		aDType * weights_ptr = weights->accessTensor(CL_MAP_READ);
	//	aDType * weights_t_ptr = mB->accessTensor(CL_MAP_READ);
		aDType * bias_ptr = bias->accessTensor(CL_MAP_READ);


		aDType * run_bot_ptr = bot_ptr;
		aDType * run_top_ptr = top_ptr;
		aDType * run_weights_ptr = weights_ptr;

		for (int k = 0; k < top_rows; ++k, run_top_ptr += top_stride)
		{
			for (int l = 0; l < top_cols; ++l)
			{
// bias
				run_top_ptr[l] = bias_ptr[l];

#if 0
				printf("C:%d %d   %f\n", l, k, bias_ptr[l]);

#endif

				for (int j = 0; j < in_cols; ++j)
				{



// transposed weight
					run_top_ptr[l] += bot_ptr[k*bot_stride + j] * weights_ptr[l*weights_stride + j];

				}
//				run_top_ptr[l] += bias_ptr[l];
			}
		}

		top_verify->commitTensor();
		bot->commitTensor();
		weights->commitTensor();
		bias->commitTensor();
//		mB->commitTensor();

	}
	return(ret);
}

int CDNN_Dnet_layer_fconnect::internalVerify(void)
{
	int ret = 0;
	ExecuteHost();
	std::string top_nm = getTop();

	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	CDNN_Tensor<aDType> * top = (CDNN_Tensor<aDType> *)findOutput(top_nm); //(CDNN_Tensor<aDType> *)findInternal(std::string("top.verify2")); 
	int match = 1;
	if (top && top_verify)
	{
		//	top->readTensor();

		aDType * top_ptr = top->accessTensor(CL_MAP_READ);
		//	aDType * top_ptr = top->getHostTensor();

		aDType * top_verify_ptr = top_verify->accessTensor(CL_MAP_READ);
		int width = (int)top->getDim(ANN_TENSOR_WIDTH);
		int height = (int)top->getDim(ANN_TENSOR_4THDIM);
		int top_v_batch_stride = (int)top_verify->getStride(ANN_TENSOR_DEPTH);
		int top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);
		const double allowedEps = 3;
		for (int j = 0; j < height && match; j++)
		{
			for (int i = 0; i < width && match; i++)
			{
				aDType c_val = top_verify_ptr[j*top_v_batch_stride + i];
				aDType g_val = top_ptr[j*top_batch_stride + i];
				double err = CalculateErr(c_val, g_val);
				if (err > allowedEps)
				{
					std::cout << "Difference " << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}
		}
		top_verify->commitTensor();
		top->commitTensor();

	}
	if (match)
	{
		std::cout << "Passed varifier: layer: fully connected: " << getName() << std::endl;
	}

	return(ret);
}



/*------------------------------------------------------------------------------------------------------------------

M - batch size
K - number of inputs
N - number of outputs

backward pass:
top_diff: MxN (M - number of rows)
weights_dif: NxK: transpose(top_dif) * top: NxM * MxK = NxK
bias_dif: 1XN

bot_dif = bot * transpose(weights): MxN + M * bias

--------------------------------------------------------------------------------------------------------------------*/


int CDNN_Dnet_layer_fconnect::internalSetupBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		makeBwdDiff();

		CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		CDNN_Tensor<aDType> * weights_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		int inputs = (int)weights->getDim(ANN_TENSOR_WIDTH);
		int outputs = (int)weights->getDim(ANN_TENSOR_HEIGHT);
		if (weights && !weights_diff)
		{
			weights_diff = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.diff"), (size_t)inputs, (size_t)outputs);
			addInternal(weights_diff->getName(), weights_diff);
			weights_diff->allocTensor();
		}
		CDNN_Tensor<aDType> * bias = (CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
		CDNN_Tensor<aDType> * bias_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));

		if (bias && !bias_diff)
		{
			bias_diff = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("bias.diff"), (size_t)outputs, 1);
			addInternal(bias_diff->getName(), bias_diff);
			bias_diff->allocTensor();
		}

		CDNN_Tensor<aDType> & top_diff_tens = getTopDiff();
		CDNN_Tensor<aDType> * top_diff_transp = (CDNN_Tensor<aDType> *)findInternal(std::string("top.diff.transp"));
		if (!top_diff_transp)
		{
			top_diff_transp = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.diff.transp"), top_diff_tens.getDim(ANN_TENSOR_4THDIM), 1, 1, top_diff_tens.getDim(ANN_TENSOR_WIDTH));
			addInternal(top_diff_transp->getName(), top_diff_transp);
			top_diff_transp->allocTensor();
		}
#if CNN_VERIFY

		// add system only for verification
		CDNN_Tensor<aDType> * bot_diff_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("bot.diff.verify"));
		CDNN_Tensor<aDType> & bot = getBotFwd();
		assert(bot_diff_verify);
		bot_diff_verify->initTensor(bot.getNDim(), bot.getDims());
		addInternal(bot_diff_verify->getName(), bot_diff_verify);
		bot_diff_verify->allocTensor(_CBUF_MEM_SYS_ONLY);

		// add system only for verification
		CDNN_Tensor<aDType> * weights_diff_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.diff.verify"), (size_t)inputs, (size_t)outputs);
		addInternal(weights_diff_verify->getName(), weights_diff_verify);
		weights_diff_verify->allocTensor(_CBUF_MEM_SYS_ONLY);
#endif
		std::string comp_options = "";
// keep compiler happy
		{

			int n_out_pix_horiz = 1;
			int n_out_pix_vert = 1;
			int ocl_group_sz0 = 1;
			int ocl_group_sz1 = 1;
			int ocl_group_lg2sz0 = 1;
			int ocl_group_lg2sz1 = 1;
			int horiz_read_lg2len = 1;
			int horiz_read_loop = 1;
			int outer_loop = 1;
			int vert_mA_read_loop = 1;
			int vert_read_step = 1;
			int vert_mB_read_loop = 1;
	//		int vert_read_step = 1;
			int mA_width = 1;
			int mA_height = 1;
			int mA_stride = 1;
			int mB_width = 1;
			int mB_height = 1;
			int mB_stride = 1;
			int mC_width = 1;
			int mC_height = 1;
			int mC_stride = 1;
			int accum_loop = 1;
			int priv_buf_len = 1;

			comp_options +=
				std::string(" -D _DNN_MM_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz)
				+ std::string(" -D _DNN_MM_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert)
				+ std::string(" -D _DNN_MM_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0)
				+ std::string(" -D _DNN_MM_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1)
				+ std::string(" -D _DNN_MM_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0)
				+ std::string(" -D _DNN_MM_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1)
				+ std::string(" -D _DNN_MM_READ_LG2=") + std::to_string((long long)horiz_read_lg2len)
				+ std::string(" -D _DNN_MM_HORIZ_READ_LOOP=") + std::to_string((long long)horiz_read_loop)
				+ std::string(" -D _DNN_MM_OUTER_LOOP=") + std::to_string((long long)outer_loop)
				+ std::string(" -D _DNN_MM_MA_VERT_READ_LOOP=") + std::to_string((long long)vert_mA_read_loop)
				+ std::string(" -D _DNN_MM_MA_VERT_READ_STEP=") + std::to_string((long long)vert_read_step)
				+ std::string(" -D _DNN_MM_MB_VERT_READ_LOOP=") + std::to_string((long long)vert_mB_read_loop)
				+ std::string(" -D _DNN_MM_MB_VERT_READ_STEP=") + std::to_string((long long)vert_read_step)
				+ std::string(" -D _DNN_MM_MA_WIDTH=") + std::to_string((long long)mA_width)
				+ std::string(" -D _DNN_MM_MA_HEIGHT=") + std::to_string((long long)mA_height)
				+ std::string(" -D _DNN_MM_MA_STRIDE=") + std::to_string((long long)mA_stride)
				+ std::string(" -D _DNN_MM_MB_WIDTH=") + std::to_string((long long)mB_width)
				+ std::string(" -D _DNN_MM_MB_HEIGHT=") + std::to_string((long long)mB_height)
				+ std::string(" -D _DNN_MM_MB_STRIDE=") + std::to_string((long long)mB_stride)
				+ std::string(" -D _DNN_MM_MC_WIDTH=") + std::to_string((long long)mC_width)
				+ std::string(" -D _DNN_MM_MC_HEIGHT=") + std::to_string((long long)mC_height)
				+ std::string(" -D _DNN_MM_MC_STRIDE=") + std::to_string((long long)mC_stride)
				+ std::string(" -D _DNN_MM_ACCUM_LOOP=") + std::to_string((long long)accum_loop)
				+ std::string(" -D _DNN_MM_PRV_BUF=") + std::to_string((long long)priv_buf_len)

				;

		}
// transpose top diff
		{
			CDNN_Tensor<aDType> * mB = (CDNN_Tensor<aDType> *)findInternal(std::string("top.diff.transp"));
			CDNN_Tensor<aDType> * mA = &getTopDiff();

			int mA_width = (int)mA->getDim(ANN_TENSOR_WIDTH);
			int mA_height = (int)mA->getDim(ANN_TENSOR_4THDIM);
			int mA_stride = (int)mA->getStride(ANN_TENSOR_WIDTH);
			int mB_stride = (int)mB->getStride(ANN_TENSOR_WIDTH);


			ocl_mt_group_sz0_ = 16;
			ocl_mt_group_sz1_ = 16;

			int out_n_cols = mA_height / ocl_mt_group_sz0_;
			n_mt_out_pix_horiz_ = (out_n_cols > 2) ? 2 : (out_n_cols == 0) ? 1 : out_n_cols;
			int out_n_rows = mA_width / ocl_mt_group_sz1_;
			n_mt_out_pix_vert_ = (out_n_rows > 2) ? 2 : (out_n_rows == 0) ? 1 : out_n_rows;

			int min_n_mt = min(n_mt_out_pix_horiz_, n_mt_out_pix_vert_);
			n_mt_out_pix_horiz_ = n_mt_out_pix_vert_ = min_n_mt;

			comp_options +=
				(
				std::string(" -D _DNN_MT_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_mt_out_pix_horiz_)
				+ std::string(" -D _DNN_MT_N_VERT_OUT_PIX=") + std::to_string((long long)n_mt_out_pix_vert_)
				+ std::string(" -D _DNN_MT_GROUP_SZ0=") + std::to_string((long long)ocl_mt_group_sz0_)
				+ std::string(" -D _DNN_MT_GROUP_SZ1=") + std::to_string((long long)ocl_mt_group_sz1_)
				+ std::string(" -D _DNN_MT_MA_WIDTH=") + std::to_string((long long)mA_width)
				+ std::string(" -D _DNN_MT_MA_HEIGHT=") + std::to_string((long long)mA_height)
				+ std::string(" -D _DNN_MT_MA_STRIDE=") + std::to_string((long long)mA_stride)
				+ std::string(" -D _DNN_MT_MB_STRIDE=") + std::to_string((long long)mB_stride)
				)
				;

		}

		{
			CDNN_Tensor<aDType> * mA = (CDNN_Tensor<aDType> *)findInternal(std::string("top.diff.transp"));

			int mA_stride = (int)mA->getStride(ANN_TENSOR_WIDTH); // _DNN_SR_MA_STRIDE
			int mA_width = (int)mA->getDim(ANN_TENSOR_WIDTH); // _DNN_SR_MA_WIDTH)
			int grp_sz0 = ocl_sr_group_sz0_ = (mA_width <= 64) ? 64 : (mA_width <= 128)? 128 : 256; //_DNN_SR_GROUP_SZ0

			int mA_row_loop = (mA_width + grp_sz0 - 1) / grp_sz0; // _DNN_SR_MA_ROW_LOOP
			comp_options +=
				(
				std::string(" -D _DNN_SR_GROUP_SZ0=") + std::to_string((long long)grp_sz0)
				+ std::string(" -D _DNN_SR_MA_WIDTH=") + std::to_string((long long)mA_width)
				+ std::string(" -D _DNN_SR_MA_STRIDE=") + std::to_string((long long)mA_stride)
				+ std::string(" -D _DNN_SR_MA_ROW_LOOP=") + std::to_string((long long)mA_row_loop)
				)
				;

		}
		
		comp_options +=
			parent_->getGenericCompOptions();

		std::string kernel_file = "aDNNMatMat.cl";
		std::string kernel_name = "aDNN_SumRow"; 


		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;


		kernel_file = "aDNNMatMat.cl";
		kernel_name = "aDNN_MatTrans";


		ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;
// weights/bias update
		{
			{
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
				MakeDoubleInternal(weights_df, weights_df.getName() + ".history", 0);
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history");

				weights_df_hist.setTensorInitialValue(0);

				CDNN_Tensor<aDType> & bias_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));
				MakeDoubleInternal(bias_df, bias_df.getName() + ".history", 0);
				CDNN_Tensor<aDType> & bias_df_hist = *(CDNN_Tensor<aDType> *)findInternal(bias_df.getName() + ".history");

				bias_df_hist.setTensorInitialValue(0);

			}

			std::string comp_options = getParent().getGenericCompOptions();

#if CNN_VERIFY
			{
				// add system only for verification
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history");
				MakeVerificationDouble(weights_df_hist);
				CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history.verify");
				weights_df_hist_v.setTensorInitialValue(0);

				// add system only for verification
				CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
				MakeVerificationDouble(weights);

//				CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
//				weights_v.deepCopyBlob(weights);


				// add system only for verification
				CDNN_Tensor<aDType> & bias_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));
				CDNN_Tensor<aDType> & bias_df_hist = *(CDNN_Tensor<aDType> *)findInternal(bias_df.getName() + ".history");
				MakeVerificationDouble(bias_df_hist);
				CDNN_Tensor<aDType> & bias_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(bias_df.getName() + ".history.verify");
				bias_df_hist_v.setTensorInitialValue(0);

				// add system only for verification
				CDNN_Tensor<aDType> & bias = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
				MakeVerificationDouble(bias);
//				CDNN_Tensor<aDType> & bias_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.verify"));
//				bias_v.deepCopyBlob(bias);

			}

#endif
// weights
			{

				CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history"));

				int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
				int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
				int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
				int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
				int weights_df_hist_stride = (int)weights_df_hist.getStride(ANN_TENSOR_WIDTH);

				int bias_pos = weights_width - 1;

				int ocl_grp_sz0 = (weights_width <= 12) ? 8 : 16;
				int ocl_grp_sz1 = (weights_height <= 12) ? 8 : 16;

				comp_options += std::string(" -D _DNN_GROUP_SZ0=") + std::to_string((long long)ocl_grp_sz0)
					+ std::string(" -D _DNN_GROUP_SZ1=") + std::to_string((long long)ocl_grp_sz1)
					+ std::string(" -D _DNN_CONV_BIAS_POS=") + std::to_string((long long)bias_pos)
					+ std::string(" -D _DNN_CONV_WEIGHTS_STRIDE=") + std::to_string((long long)weights_stride)
					+ std::string(" -D _DNN_CONV_WEIGHTS_DF_STRIDE=") + std::to_string((long long)weights_df_stride)
					+ std::string(" -D _DNN_CONV_WEIGHTS_DF_HIST_STRIDE=") + std::to_string((long long)weights_df_hist_stride)
					;

				CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNWeightUpdateSGD", "aDNNConvWeightsUpdate.cl");


				cl_kernel  ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);


				kern_exe.setOclKern(ocl_kernel);
				kern_exe.setKernBuildOptions(comp_options);

				std::vector<size_t> l_wk;
				l_wk.push_back(ocl_grp_sz0);
				l_wk.push_back(ocl_grp_sz1);
				l_wk.push_back(1);
				kern_exe.setLclSize(l_wk);


				std::vector<size_t> g_wk;
				g_wk.push_back(weights_width);
				g_wk.push_back(weights_height);
				g_wk.push_back(1);
				kern_exe.setGblSize(g_wk);

				/*
				const __global _FLOAT * weights_df,
				__global _FLOAT * weights,
				__global _FLOAT * weights_hist,
				_FLOAT momentum,
				_FLOAT weights_rate,
				_FLOAT weights_decay,

				*/
				cl_mem weights_df_mem = weights_df.getCLMem();
				cl_mem weights_mem = weights.getCLMem();
				cl_mem weights_hist_mem = weights_df_hist.getCLMem();
				aDType momentum = (aDType)getNet().getNetParams().get_momentum();



				int n_arg = 0;
				ocl_args kern_args;
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_hist_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(aDType), &momentum);


				ret = CL_SUCCESS;

				ocl_args::iterator ai;
				for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
				{
					int i = (*ai).first;
					ocl_arg arg = (*ai).second;
					ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

				}

				CHECK_OPENCL_ERROR(ret, "parmeters failed.");

				kern_exe.setKernArgs(kern_args);

				// the same queue
				cl_command_queue convQ = getaDNNOCL().getClQueue(0);

				kern_exe.setOclQueue(convQ);


				ocl_bwd_execs_.push_back(kern_exe);
			}


			// bias
			{

				CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff.history"));

				int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
				int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
				int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
				int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
				int weights_df_hist_stride = (int)weights_df_hist.getStride(ANN_TENSOR_WIDTH);

				int bias_pos = 0;

				int ocl_grp_sz0 = (weights_width <= 12) ? 8 : 16;
				int ocl_grp_sz1 = (weights_height <= 1) ? 1 : (weights_height <= 12) ? 8 : 16;

				comp_options += std::string(" -D _DNN_GROUP_SZ0=") + std::to_string((long long)ocl_grp_sz0)
					+ std::string(" -D _DNN_GROUP_SZ1=") + std::to_string((long long)ocl_grp_sz1)
					+ std::string(" -D _DNN_CONV_BIAS_POS=") + std::to_string((long long)bias_pos)
					+ std::string(" -D _DNN_CONV_WEIGHTS_STRIDE=") + std::to_string((long long)weights_stride)
					+ std::string(" -D _DNN_CONV_WEIGHTS_DF_STRIDE=") + std::to_string((long long)weights_df_stride)
					+ std::string(" -D _DNN_CONV_WEIGHTS_DF_HIST_STRIDE=") + std::to_string((long long)weights_df_hist_stride)
					;

				CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNWeightUpdateSGD", "aDNNConvWeightsUpdate.cl");


				cl_kernel  ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);


				kern_exe.setOclKern(ocl_kernel);
				kern_exe.setKernBuildOptions(comp_options);

				std::vector<size_t> l_wk;
				l_wk.push_back(ocl_grp_sz0);
				l_wk.push_back(ocl_grp_sz1);
				l_wk.push_back(1);
				kern_exe.setLclSize(l_wk);


				std::vector<size_t> g_wk;
				g_wk.push_back(weights_width);
				g_wk.push_back(weights_height);
				g_wk.push_back(1);
				kern_exe.setGblSize(g_wk);

				/*
				const __global _FLOAT * weights_df,
				__global _FLOAT * weights,
				__global _FLOAT * weights_hist,
				_FLOAT momentum,
				_FLOAT weights_rate,
				_FLOAT weights_decay,

				*/
				cl_mem weights_df_mem = weights_df.getCLMem();
				cl_mem weights_mem = weights.getCLMem();
				cl_mem weights_hist_mem = weights_df_hist.getCLMem();
				aDType momentum = (aDType)getNet().getNetParams().get_momentum();


				int n_arg = 0;
				ocl_args kern_args;
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_hist_mem);
				kern_args[n_arg++] = std::make_pair(sizeof(aDType), &momentum);


				ret = CL_SUCCESS;

				ocl_args::iterator ai;
				for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
				{
					int i = (*ai).first;
					ocl_arg arg = (*ai).second;
					ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

				}

				CHECK_OPENCL_ERROR(ret, "parmeters failed.");

				kern_exe.setKernArgs(kern_args);

				// the same queue
				cl_command_queue convQ = getaDNNOCL().getClQueue(0);

				kern_exe.setOclQueue(convQ);


				ocl_bwd_execs_.push_back(kern_exe);
			}

		}

	}

	return(ret);
}


int CDNN_Dnet_layer_fconnect::ExecuteBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		cl_command_queue convQ = getaDNNOCL().getClQueue(0);
		double s = 0, e = 0;
		CDNN_Tensor<aDType> * weights_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		CDNN_Tensor<aDType> & top_diff = getTopDiff();
		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & bot_diff = getBotDiff();


		int iter = getNTimingIter();

		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}

		for (int i = 0; i < iter; i++)
		{
// weights grad
#if 1
			{
				int transposeA = 1;
				int transposeB = 0;
				aDType alpha = 1;
				aDType beta = 0;
				// mA
				size_t a_cols = top_diff.getDim(ANN_TENSOR_WIDTH) * top_diff.getDim(ANN_TENSOR_HEIGHT) * top_diff.getDim(ANN_TENSOR_DEPTH);
				size_t a_rows = top_diff.getDim(ANN_TENSOR_4THDIM);
				// mB
				size_t b_cols = bot.getDim(ANN_TENSOR_WIDTH) * bot.getDim(ANN_TENSOR_HEIGHT) * bot.getDim(ANN_TENSOR_DEPTH);
				size_t b_rows = bot.getDim(ANN_TENSOR_4THDIM);
				// mC
				size_t c_cols = weights_diff->getDim(ANN_TENSOR_WIDTH);
				size_t c_rows = weights_diff->getDim(ANN_TENSOR_HEIGHT);

				weights_diff->mul2(c_cols, c_rows, top_diff, a_cols, a_rows, bot, b_cols, b_rows, transposeA, transposeB, alpha, beta);
			}
#endif

//bottom diff
			{
				int transposeA = 0;
				int transposeB = 0;
				aDType alpha = 1;
				aDType beta = 0;
				// mA
				size_t a_cols = top_diff.getDim(ANN_TENSOR_WIDTH) * top_diff.getDim(ANN_TENSOR_HEIGHT) * top_diff.getDim(ANN_TENSOR_DEPTH);
				size_t a_rows = top_diff.getDim(ANN_TENSOR_4THDIM);
				// mB
				size_t b_cols = weights->getDim(ANN_TENSOR_WIDTH);
				size_t b_rows = weights->getDim(ANN_TENSOR_HEIGHT);
				// mC
				size_t c_cols = bot_diff.getDim(ANN_TENSOR_WIDTH) * bot_diff.getDim(ANN_TENSOR_HEIGHT) * bot_diff.getDim(ANN_TENSOR_DEPTH);
				size_t c_rows = bot_diff.getDim(ANN_TENSOR_4THDIM);

				bot_diff.mul2(c_cols, c_rows, top_diff, a_cols, a_rows, *weights, b_cols, b_rows, transposeA, transposeB, alpha, beta);
			}

// bias grad
			{

				CDNN_Tensor<aDType> * mB = (CDNN_Tensor<aDType> *)findInternal(std::string("top.diff.transp"));
				CDNN_Tensor<aDType> * mA = &getTopDiff();
				cl_mem mA_mem = mA->getCLMem();
				cl_mem mB_mem = mB->getCLMem();
				int n_arg = 0;
				std::string kernel_name = "aDNN_MatTrans";
				cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];


				int mA_width = (int)mA->getDim(ANN_TENSOR_WIDTH);
				int mA_height = (int)mA->getDim(ANN_TENSOR_4THDIM);



				int i_n_group_horiz = (mA_width + ocl_mt_group_sz0_ * n_mt_out_pix_horiz_ - 1) / (ocl_mt_group_sz0_ * n_mt_out_pix_horiz_);
				int i_n_group_vert = (mA_height + ocl_mt_group_sz1_ * n_mt_out_pix_vert_ - 1) / (ocl_mt_group_sz1_ * n_mt_out_pix_vert_);


				size_t l_wk[3] = { ocl_mt_group_sz0_, ocl_mt_group_sz1_, 1 };

				size_t g_wk[3] = { i_n_group_horiz * l_wk[0], i_n_group_vert * l_wk[1], 1 };

				n_arg = 0;

				ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mA_mem);
				ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mB_mem);

				CHECK_OPENCL_ERROR(ret, "parmeters failed.");

				ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
				CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");
			}
			{
				CDNN_Tensor<aDType> * mA = (CDNN_Tensor<aDType> *)findInternal(std::string("top.diff.transp"));
				CDNN_Tensor<aDType> * bias_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));

				std::string kernel_name = "aDNN_SumRow";
				cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];

				cl_mem mA_mem = mA->getCLMem();
				cl_mem vS_mem = bias_diff->getCLMem();

				int mA_width = (int)mA->getDim(ANN_TENSOR_WIDTH);
				int mA_height = (int)mA->getDim(ANN_TENSOR_4THDIM);

				size_t l_wk[3] = { ocl_sr_group_sz0_, 1, 1 };
				int i_n_group_horiz = (mA_width + ocl_sr_group_sz0_ - 1) / ocl_sr_group_sz0_;

				size_t g_wk[3] = { i_n_group_horiz * l_wk[0], mA_height, 1 };

				int n_arg = 0;

				ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &mA_mem);
				ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &vS_mem);

				CHECK_OPENCL_ERROR(ret, "parmeters failed.");

				ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
				CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");

			}
		}
		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


#if CNN_VERIFY
		internalVerifyBwd();
#endif

		int height = (int)(top_diff.getDim(ANN_TENSOR_WIDTH) * top_diff.getDim(ANN_TENSOR_HEIGHT) * top_diff.getDim(ANN_TENSOR_DEPTH));
		int width = (int)top_diff.getDim(ANN_TENSOR_4THDIM);
		int width2 = (int)(bot.getDim(ANN_TENSOR_WIDTH) * bot.getDim(ANN_TENSOR_HEIGHT) * bot.getDim(ANN_TENSOR_DEPTH));
		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: fully connected back-propagation: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: LxFxB : %dx%dx%d\n", ident, " ", width, height, width2);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, (double)(2 * width* height* width2 * iter) / (processing_time_ * 1000000000));
			}
		}
	}

	return(ret);
}

int CDNN_Dnet_layer_fconnect::ExecuteBwdHost(void)
{
	int ret = 0;
	{
		CDNN_Tensor<aDType> & top_diff = getTopDiff();

		CDNN_Tensor<aDType> & bot = getBotFwd();

		CDNN_Tensor<aDType> * weights_diff_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.verify"));
		CDNN_Tensor<aDType> * bot_diff_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("bot.diff.verify"));
		CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal("weights");
		assert(weights_diff_verify && bot_diff_verify && weights);
		// mA
		size_t a_cols = top_diff.getDim(ANN_TENSOR_WIDTH) * top_diff.getDim(ANN_TENSOR_HEIGHT) * top_diff.getDim(ANN_TENSOR_DEPTH);
		size_t a_rows = top_diff.getDim(ANN_TENSOR_4THDIM);
		int top_stride = (int)top_diff.getStride(ANN_TENSOR_DEPTH);
		aDType * top_diff_ptr = top_diff.accessTensor(CL_MAP_READ);
		assert(top_diff_ptr);


		// mB
		size_t b_cols = bot.getDim(ANN_TENSOR_WIDTH) * bot.getDim(ANN_TENSOR_HEIGHT) * bot.getDim(ANN_TENSOR_DEPTH);
		size_t b_rows = bot.getDim(ANN_TENSOR_4THDIM);
		int bot_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);
		aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
		assert(bot_ptr);

		// mC
		size_t c_cols = weights_diff_verify->getDim(ANN_TENSOR_WIDTH);
		size_t c_rows = weights_diff_verify->getDim(ANN_TENSOR_HEIGHT);
		int weights_diff_stride = (int)weights_diff_verify->getStride(ANN_TENSOR_WIDTH);
		aDType * weights_diff_v_ptr = weights_diff_verify->accessTensor(CL_MAP_WRITE);
		assert(weights_diff_v_ptr);

		for (int n = 0; n < c_rows; ++n)
		{
			for (int k = 0; k < c_cols; ++k)
			{
				weights_diff_v_ptr[n*weights_diff_stride + k] = 0;
				// transposed top_diff
				for (int m = 0; m < a_rows; ++m)
				{
					weights_diff_v_ptr[n*weights_diff_stride + k] += top_diff_ptr[m*top_stride + n] * bot_ptr[m*bot_stride + k];
				}
			}
		}


		top_diff.commitTensor();
		bot.commitTensor();
		weights_diff_verify->commitTensor();
	}

	{
		CDNN_Tensor<aDType> & top_diff = getTopDiff();
		CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal("weights");
		CDNN_Tensor<aDType> & bot_diff = *(CDNN_Tensor<aDType> *)findInternal("bot.diff.verify");
		// mA
		size_t a_cols = top_diff.getDim(ANN_TENSOR_WIDTH) * top_diff.getDim(ANN_TENSOR_HEIGHT) * top_diff.getDim(ANN_TENSOR_DEPTH);
		size_t a_rows = top_diff.getDim(ANN_TENSOR_4THDIM);
		int top_stride = (int)top_diff.getStride(ANN_TENSOR_DEPTH);
		aDType * top_diff_ptr = top_diff.accessTensor(CL_MAP_READ);
		// mB
		size_t b_cols = weights.getDim(ANN_TENSOR_WIDTH);
		size_t b_rows = weights.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
		aDType * weights_ptr = weights.accessTensor(CL_MAP_READ);
		// mC
		size_t c_cols = bot_diff.getDim(ANN_TENSOR_WIDTH) * bot_diff.getDim(ANN_TENSOR_HEIGHT) * bot_diff.getDim(ANN_TENSOR_DEPTH);
		size_t c_rows = bot_diff.getDim(ANN_TENSOR_4THDIM);
		int bot_stride = (int)bot_diff.getStride(ANN_TENSOR_DEPTH);
		aDType * bot_ptr = bot_diff.accessTensor(CL_MAP_WRITE);

//		bot_diff.mul2(c_cols, c_rows, top_diff, a_cols, a_rows, *weights, b_cols, b_rows, transposeA, transposeB, alpha, beta);


		for (int n = 0; n < c_rows; ++n)
		{			
			for (int k = 0; k < c_cols; ++k)
			{
				bot_ptr[n*bot_stride + k] = 0;
				for (int m = 0; m < a_cols; ++m)
				{
					bot_ptr[n*bot_stride + k] += top_diff_ptr[n*top_stride + m] * weights_ptr[m*weights_stride + k];
				}
			}
		}


		top_diff.commitTensor();
		weights.commitTensor();
		bot_diff.commitTensor();
	}
	return(ret);
}

int CDNN_Dnet_layer_fconnect::internalVerifyBwd(void)
{
	int ret = 0;
	ExecuteBwdHost();
	int match = 1;
	const double allowedEps = 3;

	{
		CDNN_Tensor<aDType> * weights_diff_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.verify"));
		CDNN_Tensor<aDType> * weights_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		if (weights_diff_verify && weights_diff)
		{
			//	top->readTensor();

			aDType * weights_diff_ptr = weights_diff->accessTensor(CL_MAP_READ);

			aDType * weights_diff_verify_ptr = weights_diff_verify->accessTensor(CL_MAP_READ);
			int width = (int)weights_diff->getDim(ANN_TENSOR_WIDTH);
			int height = (int)weights_diff->getDim(ANN_TENSOR_HEIGHT);
			int weights_diff_v_stride = (int)weights_diff_verify->getStride(ANN_TENSOR_WIDTH);
			int weights_diff_stride = (int)weights_diff->getStride(ANN_TENSOR_WIDTH);
			for (int j = 0; j < height && match; j++)
			{
				for (int i = 0; i < width && match; i++)
				{
					aDType c_val = weights_diff_verify_ptr[j*weights_diff_v_stride + i];
					aDType g_val = weights_diff_ptr[j*weights_diff_stride + i];
					double err = CalculateErr(c_val, g_val);
					if (err > allowedEps)
					{
						std::cout << "Difference in weights diff" << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
						match = 0;
					}
				}
			}
			weights_diff_verify->commitTensor();
			weights_diff->commitTensor();

		}
	}
	{
		CDNN_Tensor<aDType> & bot_diff_v = *(CDNN_Tensor<aDType> *)findInternal("bot.diff.verify");
		CDNN_Tensor<aDType> & bot_diff = getBotDiff();
		size_t cols = bot_diff.getDim(ANN_TENSOR_WIDTH) * bot_diff.getDim(ANN_TENSOR_HEIGHT) * bot_diff.getDim(ANN_TENSOR_DEPTH);
		size_t rows = bot_diff.getDim(ANN_TENSOR_4THDIM);
		int bot_stride = (int)bot_diff.getStride(ANN_TENSOR_DEPTH);
		aDType * bot_ptr = bot_diff.accessTensor(CL_MAP_READ);
		int bot_v_stride = (int)bot_diff_v.getStride(ANN_TENSOR_DEPTH);
		aDType * bot_v_ptr = bot_diff_v.accessTensor(CL_MAP_READ);
		for (int j = 0; j < rows && match; j++)
		{
			for (int i = 0; i < cols && match; i++)
			{
				aDType c_val = bot_v_ptr[j*bot_v_stride + i];
				aDType g_val = bot_ptr[j*bot_stride + i];
				double err = CalculateErr(c_val, g_val);
				if (err > allowedEps)
				{
					std::cout << "Difference in bottom diff" << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}
		}
		bot_diff_v.commitTensor();
		bot_diff.commitTensor();


	}
	if (match)
	{
		std::cout << "Passed varifier: layer: fully connected back-propagation: " << getName() << std::endl;
	}

	return(ret);
}




/************************************************************************************************************************
**
**				UPDATE WEIGHTS
**
************************************************************************************************************************/


int CDNN_Dnet_layer_fconnect::Update(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		int iter = getNTimingIter();


		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}

		for (int i = 0; i < iter; i++)
		{
			// rates changes
			aDType cur_weights_l_rate;
			aDType cur_weights_decay;
			aDType cur_bias_l_rate;
			aDType cur_bias_decay;
			UpdateRates(cur_weights_l_rate, cur_weights_decay,
				(aDType)getWeightsLr(), (aDType)getWeightsDecay());
			UpdateRates(cur_bias_l_rate, cur_bias_decay,
				(aDType)getBiasLr(), (aDType)getBiasDecay());
// weights
			// execute through specialized object
			ocl_args additional_args;
			additional_args[4] = std::make_pair(sizeof(aDType), &cur_weights_l_rate);
			additional_args[5] = std::make_pair(sizeof(aDType), &cur_weights_decay);


			ocl_bwd_execs_[0].ExecuteNoWait(&additional_args);
// bias

			additional_args.clear();
			additional_args[4] = std::make_pair(sizeof(aDType), &cur_bias_l_rate);
			additional_args[5] = std::make_pair(sizeof(aDType), &cur_bias_decay);

			ocl_bwd_execs_[1].ExecuteNoWait(&additional_args);

		}

		if (IsDoTiming())
		{
			clFinish(ocl_bwd_execs_[1].getOclQueue());
			e = mach_absolute_time();
		}
		// verify
#if CNN_VERIFY
		internalVerifyUpdate();
#endif
		{
			// APPROX for now
			CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));

			int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH) + 1;
			int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
			iter = (iter <= 0) ? 1 : iter;
			processing_time_ = subtractTimes(e, s);
			int ident = 4;
			double comp_compexity = (double)weights_width * weights_height * 6;  // multuiply by 2 due to 2 issues
			if (IsOutMessages())
			{
				printf("Passed layer: update fully connected weights: \"%s\"\n", getName().c_str());
				printf("%*s" "Arguments: WxH: %dx%d\n", ident, " ", weights_width, weights_height);
				if (IsDoTiming())
				{
					printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, (comp_compexity * iter) / (processing_time_ * 1000000000));
				}
			}
		}

//		printf("Passed layer: update fully connected weights: \"%s\"\n", getName().c_str());

		ret = 0;
	}

	return(ret);
}


int CDNN_Dnet_layer_fconnect::UpdateHost(void)
{
	int ret = 0;
	// rates changes
	aDType cur_weights_l_rate;
	aDType cur_weights_decay;
	aDType cur_bias_l_rate;
	aDType cur_bias_decay;
	UpdateRates(cur_weights_l_rate, cur_weights_decay,
		(aDType)getWeightsLr(), (aDType)getWeightsDecay());
	UpdateRates(cur_bias_l_rate, cur_bias_decay,
		(aDType)getBiasLr(), (aDType)getBiasDecay());
	aDType momentum = (aDType)getNet().getNetParams().get_momentum();

//weights
	{

		CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history.verify"));

		int weights_width = (int)weights_v.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights_v.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);
		int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
		int weights_df_hist_stride = (int)weights_df_hist_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		aDType * weights_df_ptr = weights_df.accessTensor(CL_MAP_READ);
		aDType * weights_df_hist_v_ptr = weights_df_hist_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		for (int j = 0; j < weights_height; ++j)
		{
			for (int i = 0; i < weights_width; ++i)
			{
				aDType we_out;
				aDType we_hist_out;
				aDType we_hist = weights_df_hist_v_ptr[j * weights_df_hist_stride + i];
				aDType we_df = weights_df_ptr[j * weights_df_stride + i];
				aDType we = weights_v_ptr[j * weights_stride + i];
				annCalculateWeightsUpdate(&we_hist_out, &we_out,
					we_hist, we_df, we,
					momentum,
					cur_weights_l_rate, cur_weights_decay
					);
				weights_df_hist_v_ptr[j * weights_df_hist_stride + i] = we_hist_out;
				weights_v_ptr[j * weights_stride + i] = we_out;
			}
		}

		weights_v.commitTensor();
		weights_df.commitTensor();
		weights_df_hist_v.commitTensor();

	}

	//bias
	{

		CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.verify"));
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));
		CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff.history.verify"));

		int weights_width = (int)weights_v.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights_v.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);
		int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
		int weights_df_hist_stride = (int)weights_df_hist_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		aDType * weights_df_ptr = weights_df.accessTensor(CL_MAP_READ);
		aDType * weights_df_hist_v_ptr = weights_df_hist_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		for (int j = 0; j < weights_height; ++j)
		{
			for (int i = 0; i < weights_width; ++i)
			{
				aDType we_out;
				aDType we_hist_out;
				aDType we_hist = weights_df_hist_v_ptr[j * weights_df_hist_stride + i];
				aDType we_df = weights_df_ptr[j * weights_df_stride + i];
				aDType we = weights_v_ptr[j * weights_stride + i];
				annCalculateWeightsUpdate(&we_hist_out, &we_out,
					we_hist, we_df, we,
					momentum,
					cur_bias_l_rate, cur_bias_decay
					);
				weights_df_hist_v_ptr[j * weights_df_hist_stride + i] = we_hist_out;
				weights_v_ptr[j * weights_stride + i] = we_out;
			}
		}

		weights_v.commitTensor();
		weights_df.commitTensor();
		weights_df_hist_v.commitTensor();

	}

	return(ret);
}

int CDNN_Dnet_layer_fconnect::internalVerifyUpdate(void)
{
	int ret = 0;

	UpdateHost();

	int match = 1;
	//weights
	{

		CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
		CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history.verify"));

		int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
		int weights_v_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_ptr = weights.accessTensor(CL_MAP_READ);
		aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ);
		double allowedEps = 2;
		for (int j = 0; j < weights_height && match; ++j)
		{
			for (int i = 0; i < weights_width && match; ++i)
			{
				aDType c_val = weights_v_ptr[j * weights_v_stride + i];
				aDType g_val = weights_ptr[j * weights_stride + i];
				double err = CalculateErr(c_val, g_val);
				if (err > allowedEps && abs(c_val - g_val) > 0.0000001)
				{
					std::cout << "Difference in weights update " << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}
		}

		weights_v.commitTensor();
		weights.commitTensor();

	}

	//bias
	if (match)
	{

		CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.verify"));
		CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias"));
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff"));
		CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bias.diff.history.verify"));

		int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
		int weights_v_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_ptr = weights.accessTensor(CL_MAP_READ);
		aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ);
		double allowedEps = 2;
		for (int j = 0; j < weights_height && match; ++j)
		{
			for (int i = 0; i < weights_width && match; ++i)
			{
				aDType c_val = weights_v_ptr[j * weights_v_stride + i];
				aDType g_val = weights_ptr[j * weights_stride + i];
				double err = CalculateErr(c_val, g_val);
				if (err > allowedEps && abs(c_val - g_val) > 0.0000001)
				{
					std::cout << "Difference bias update " << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}
		}

		weights_v.commitTensor();
		weights.commitTensor();
	}

	if (match)
	{
		std::cout << "Passed varifier: layer: fully connected weights update: " << getName() << std::endl;
	}

	return(ret);
}

/************************************************************************************************************************
**
**			CONVOLUTIONAL LAYER
**
************************************************************************************************************************/


CDNN_Dnet_layer_conv :: CDNN_Dnet_layer_conv() : CDNN_Dnet_layer()
{
	n_out_pix_horiz_ = 1;
	n_out_pix_vert_ = 1;
	ocl_group_sz0_ = 8;
	ocl_group_sz1_ = 8;
	ocl_group_lg2sz0_ = 3;
	ocl_group_lg2sz1_ = 3;


}


CDNN_Dnet_layer_conv::CDNN_Dnet_layer_conv(CDNN_OVX * parent, void * wrapper, std::string name, const void * parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Dnet_layer(parent, wrapper, name,	parameters, ref_layer)
{
	n_out_pix_horiz_ = 1;
	n_out_pix_vert_ = 1;
	ocl_group_sz0_ = 8;
	ocl_group_sz1_ = 8;
	ocl_group_lg2sz0_ = 3;
	ocl_group_lg2sz1_ = 3;

}


/************************************************************************************************************************
**
**			FORWARD PROPAGATION
**
************************************************************************************************************************/

int CDNN_Dnet_layer_conv:: Execute(void)
{
	int ret = -1;
	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top = getTopFwd();

	int iter = getNTimingIter();


	double s = 0, e = 0;
	if (IsDoTiming())
	{
		s = mach_absolute_time();
	}

	for (int i = 0; i < iter; i++)
	{

		cl_mem bot_mem = bot.getCLMem();
		cl_mem top_mem = top.getCLMem();


			// execute through specialized object
		ocl_args additional_args;
		additional_args[0] = std::make_pair(sizeof(cl_mem), &bot_mem);
		additional_args[2] = std::make_pair(sizeof(cl_mem), &top_mem);
		ocl_fwd_execs_[0].ExecuteNoWait(&additional_args);
	}

	if (IsDoTiming())
	{
		clFinish(ocl_fwd_execs_[0].getOclQueue());
		e = mach_absolute_time();
	}
		// verify
#if CNN_VERIFY
		internalVerify();
#endif

	int width = (int)top.getDim(ANN_TENSOR_WIDTH);
	int height = (int)top.getDim(ANN_TENSOR_HEIGHT);
		// TO DO: check top, bot dim are equal
	int kernel_size = getKernelSz();
	int pad = getPad();
	int stride = getStep();
	int batch_sz = (int)bot.getDim(ANN_TENSOR_4THDIM);
	int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
	int outputs = (int)getNOutputs();
	iter = (iter <= 0) ? 1 : iter;
	processing_time_ = subtractTimes(e, s);
	int ident = 4;
	if (IsOutMessages())
	{
		printf("Passed layer: convolution: \"%s\"\n", getName().c_str());
		printf("%*s" "Arguments: CxWxHxKxKxOxB: %dx%dx%dx%dx%dx%dx%d\n", ident, " ", inputs, width, height, kernel_size, kernel_size, outputs, batch_sz);
		if (IsDoTiming())
		{
			printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, ((double)2 * inputs*width*height*kernel_size*kernel_size*outputs*batch_sz * iter) / (processing_time_ * 1000000000));
		}
	}
	ret = 0;
	
	return(ret);

}

int CDNN_Dnet_layer_conv:: internalSetup(void)
{
	int ret = 0;

	size_t outputs = getNOutputs();
	int pad = getPad();
	int stride = getStep();
	int kernel_size = getKernelSz();
	size_t batch_sz = 0;
	size_t inputs = 0;
	size_t width = 0;
	size_t height = 0;
	CDNN_Tensor<aDType> * bot = NULL;
	CDNN_Tensor<aDType> * top = NULL;

	if (bottom_lyr_)
	{
		bot = &bottom_lyr_->getTopFwd();
		width = bot->getDim(ANN_TENSOR_WIDTH);
		height = bot->getDim(ANN_TENSOR_HEIGHT);
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);
		inputs = bot->getDim(ANN_TENSOR_DEPTH);

	}
	if (top_lyr_)
	{
		top = &getTopFwd();
		CDNN_Tensor<aDType> * bot = &top_lyr_->getBotFwd();
		if (top==bot && !top->IsInited())
		{
			top->setName(getTopNameFwd());
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = (width + 2 * pad - kernel_size) / stride + 1;
			dims[ANN_TENSOR_HEIGHT] = (height + 2 * pad - kernel_size) / stride + 1;
			dims[ANN_TENSOR_DEPTH] = outputs;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);

#if CNN_VERIFY
			// add system only for verification
			CDNN_Tensor<aDType> * top_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.verify"), top->getDim(ANN_TENSOR_WIDTH), top->getDim(ANN_TENSOR_HEIGHT), top->getDim(ANN_TENSOR_DEPTH), top->getDim(ANN_TENSOR_4THDIM));
			addInternal(std::string("top.verify"), top_verify);
#endif
		}
	}
// to gemm

// # of passes = batch size

	setNPasses((int)batch_sz);


// TO DO interleave weights to have simple load of weights for diff outputs
	CDNN_Tensor<aDType> * weights = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights"), inputs * (size_t)kernel_size *(size_t)kernel_size + 1, (size_t)outputs);
	addInternal(std::string("weights"), weights);
	weights->allocTensor();
	int weights_stride = (int)weights->getStride(ANN_TENSOR_WIDTH);

	assert(bot && top);

	int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top->getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top->getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);

	int height_out = (int)top->getDim(ANN_TENSOR_HEIGHT);
	int width_out = (int)top->getDim(ANN_TENSOR_WIDTH);
	int vis_height = (int)height;
	int vis_width = (int)width;

	int ocl_group_sz0 = 8;
	int ocl_group_sz1 = 8;
	int ocl_group_lg2sz1 = (int)ceil(log((double)ocl_group_sz1)/log(2.));
	int ocl_group_lg2sz0 = (int)ceil(log((double)ocl_group_sz0)/log(2.));

	int n_out_pix_horiz = (width_out <= ocl_group_sz0) ? 1 : (width_out < 4 * ocl_group_sz0 || stride > 1) ? 2 : 4;
	int n_out_pix_vert = (height_out <= ocl_group_sz1) ? 1 : 2; // (height_out <= 192) ? 2 : 4;


	int n_outs = ((outputs & 1) == 1) ? 1 : (kernel_size == 3) && ((outputs / 4) * 4 == outputs) ? 4 : 2; // (n_out_pix_horiz >= 4) ? 1 : 2;

	int n_outputs = (int)top->getDim(ANN_TENSOR_DEPTH);
	n_outputs /= n_outs;

	std::string comp_options = 
		std::string("-D _DNN_CONV_KERNEL_SZ=") + std::to_string((long long)kernel_size)
		+ std::string(" -D _DNN_CONV_N_OUTPUTS=") + std::to_string((long long)n_outputs)
		+ std::string(" -D _DNN_CONV_N_CHANNELS=") + std::to_string((long long)inputs)
		+ std::string(" -D _DNN_CONV_PAD=") + std::to_string((long long)pad)
		+ std::string(" -D _DNN_CONV_STRIDE=") + std::to_string((long long)stride)
		+ std::string(" -D _DNN_CONV_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz)
		+ std::string(" -D _DNN_CONV_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert)
		+ std::string(" -D _DNN_CONV_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0)
		+ std::string(" -D _DNN_CONV_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1)
		+ std::string(" -D _DNN_CONV_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0)
		+ std::string(" -D _DNN_CONV_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1_)
		+ std::string(" -D _DNN_CONV_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
		+ std::string(" -D _DNN_CONV_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
		+ std::string(" -D _DNN_CONV_BOT_STRIDE=") + std::to_string((long long)bot_stride)
		+ std::string(" -D _DNN_CONV_TOP_BATCH_STRIDE=") + std::to_string((long long)top_batch_stride)
		+ std::string(" -D _DNN_CONV_TOP_CHANNEL_STRIDE=") + std::to_string((long long)top_channel_stride)
		+ std::string(" -D _DNN_CONV_TOP_STRIDE=") + std::to_string((long long)top_stride)
		+ std::string(" -D _DNN_CONV_BOT_VIS_WIDTH=") + std::to_string((long long)vis_width)
		+ std::string(" -D _DNN_CONV_BOT_VIS_HEIGHT=") + std::to_string((long long)vis_height)
		+ std::string(" -D _DNN_CONV_WEIGHTS_STRIDE=") + std::to_string((long long)weights_stride)
		+ std::string(" -D _DNN_CONV_TOP_WIDTH=") + std::to_string((long long)width_out)
		+ std::string(" -D _DNN_CONV_TOP_HEIGHT=") + std::to_string((long long)height_out)
		+ std::string(" -D _DNN_CONV_N_OUTS=") + std::to_string((long long)n_outs)
		+ parent_->getGenericCompOptions()
		;

	int do_bias = 1;
	comp_options += std::string(" -D _DNN_CONV_BIAS=") + std::to_string((long long)do_bias);


	std::string kernel_file = "aDNNConv.cl";
	std::string kernel_name = "aDNNConvMO_LD_S";


	cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

	cl_command_queue convQ = getaDNNOCL().getClQueue(0);


// execution setup

	int i_n_group_horiz = (width_out + ocl_group_sz0 * n_out_pix_horiz - 1) / (ocl_group_sz0 * n_out_pix_horiz);
	int i_n_group_vert = (height_out + ocl_group_sz1 * n_out_pix_vert - 1) / (ocl_group_sz1 * n_out_pix_vert);


	std::vector<size_t> l_wk;
	l_wk.push_back(ocl_group_sz0);
	l_wk.push_back(ocl_group_sz1);
	l_wk.push_back(1);


	std::vector<size_t> g_wk;
	g_wk.push_back(i_n_group_horiz * l_wk[0]);
	g_wk.push_back(i_n_group_vert * l_wk[1]);
	g_wk.push_back(top->getDim(ANN_TENSOR_4THDIM) * n_outputs);

	//	cl_mem bot_mem = bot->getCLMem();
	//	cl_mem top_mem = top->getCLMem();
	cl_mem weights_mem = weights->getCLMem();

	aDType padding_value = getPaddingVal();


	int n_arg = 0;
	ocl_args kern_args;
	n_arg++;
//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_mem);
	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_mem);
//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_mem);
	n_arg++;
	kern_args[n_arg++] = std::make_pair(sizeof(aDType), &padding_value);


	ret = CL_SUCCESS;

	ocl_args::iterator ai;
	for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
	{
		int i = (*ai).first;
		ocl_arg arg = (*ai).second;
		ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

	}

	CHECK_OPENCL_ERROR(ret, "parmeters failed.");


	CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNConvMO_LD_S", "aDNNConv.cl", comp_options, ocl_kernel, &g_wk, &l_wk, &kern_args, convQ);

	ocl_fwd_execs_.push_back(kern_exe);

	return(ret);
}

int CDNN_Dnet_layer_conv::ExecuteHost(void)
{
	int ret = 0;
	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	std::string bot_nm = getBottom();
	CDNN_Tensor<aDType> * bot = (CDNN_Tensor<aDType> *)findInput(bot_nm);
	CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal("weights");

	if (top_verify && bot && weights)
	{
		top_verify->allocTensor(_CBUF_MEM_SYS_ONLY);

		aDType padding_value = getPaddingVal();

		int bot_width = (int)bot->getDim(ANN_TENSOR_WIDTH);
		int bot_height = (int)bot->getDim(ANN_TENSOR_HEIGHT);
		// TO DO: check top, bot dim are equal
		int kernel_size = getKernelSz();
		int pad = getPad();
		int stride = getStep();

		int height_out = (int)top_verify->getDim(ANN_TENSOR_HEIGHT);
		int width_out = (int)top_verify->getDim(ANN_TENSOR_WIDTH);
		int vis_height = bot_height;
		int vis_width = bot_width;

		int n_batchs = (int)top_verify->getDim(ANN_TENSOR_4THDIM);
		int n_outputs = (int)top_verify->getDim(ANN_TENSOR_DEPTH);
		int n_inputs = (int)bot->getDim(ANN_TENSOR_DEPTH);

		int top_batch_stride = (int)top_verify->getStride(ANN_TENSOR_DEPTH);
		int top_channel_stride = (int)top_verify->getStride(ANN_TENSOR_HEIGHT);
		int top_stride = (int)top_verify->getStride(ANN_TENSOR_WIDTH);
		int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);
		int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
		int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
		int weights_stride = (int)weights->getStride(ANN_TENSOR_WIDTH);
		aDType * bot_ptr = bot->accessTensor(CL_MAP_READ);
		aDType * top_ptr = top_verify->accessTensor(CL_MAP_WRITE);
		aDType * weights_ptr = weights->accessTensor(CL_MAP_READ);
		aDType * run_bot_ptr = bot_ptr;
		aDType * run_top_ptr = top_ptr;
		aDType * run_weights_ptr = weights_ptr;

		for (int b = 0; b < n_batchs; b++, run_bot_ptr += bot_batch_stride, run_top_ptr += top_batch_stride)
		{
			run_weights_ptr = weights_ptr;
			for (int o = 0; o < n_outputs; o++)
			{
				for (int j = 0; j < height_out; j++)
				{
					for (int i = 0; i < width_out; i++)
					{
						aDType accum = 0;
						for (int c = 0; c < n_inputs; c++)
						{
							for (int k_j = 0; k_j < kernel_size; k_j++)
							{

								int in_y = (j*stride + k_j - pad);
								for (int k_i = 0; k_i < kernel_size; k_i++)
								{
									int in_x = (i*stride + k_i - pad);
									aDType data_val = padding_value;
									if (!(in_y < 0 || in_x < 0 || in_y >= vis_height || in_x >= vis_width))
									{
										int in_data_off = c*bot_channel_stride  + in_y * bot_stride + in_x;
										data_val = run_bot_ptr[in_data_off];
									}
									aDType wei_val = run_weights_ptr[o*weights_stride + c*kernel_size *kernel_size + k_j*kernel_size + k_i];
									accum += data_val * wei_val;
#if 0
									if (b==0 && o == 1 && i == 0 && j == 0 )
									{
										printf("c:%f %f %f\n", accum, data_val, wei_val);
									}
#endif
								}
							}
						}

						run_top_ptr[o*top_channel_stride + j*top_stride + i] = accum + run_weights_ptr[o*weights_stride + n_inputs*kernel_size *kernel_size]; // + bias
#if 0
						if (b == 0 && o == 1 && i == 0 && j == 0)
						{
							printf("c:%f %f\n", accum, run_weights_ptr[o*weights_stride + n_inputs*kernel_size *kernel_size]);
						}
#endif
					}
						
				}
				
			}
		}

		bot->commitTensor();
		top_verify->commitTensor();
		weights->commitTensor();


	}
	return(ret);
}

int CDNN_Dnet_layer_conv::internalVerify(void)
{
	int ret = 0; 
	ExecuteHost();

//	cl_command_queue convQ = getaDNNOCL().getClQueue(0);
//	clFinish(convQ);

	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	std::string top_nm = getTop();
	CDNN_Tensor<aDType> * top = (CDNN_Tensor<aDType> *)findOutput(top_nm);


	aDType * top_ptr = top->accessTensor(CL_MAP_READ);


	aDType * top_verify_ptr = top_verify->accessTensor(CL_MAP_READ);
	int width = (int)top->getDim(ANN_TENSOR_WIDTH);
	int height = (int)top->getDim(ANN_TENSOR_HEIGHT);
	int top_v_batch_stride = (int)top_verify->getStride(ANN_TENSOR_DEPTH);
	int top_v_channel_stride = (int)top_verify->getStride(ANN_TENSOR_HEIGHT);
	int top_v_stride = (int)top_verify->getStride(ANN_TENSOR_WIDTH);
	int top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);
	int top_channel_stride = (int)top->getStride(ANN_TENSOR_HEIGHT);
	int top_stride = (int)top->getStride(ANN_TENSOR_WIDTH);
	int n_outputs = (int)top_verify->getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top_verify->getDim(ANN_TENSOR_4THDIM);

	int match = 1;
	const double allowedEps = 3;
	for (int b = 0; b < n_batchs && match; b++)
	{
		for (int o = 0; o < n_outputs && match; o++)
		{
			for (int j = 0; j < height && match; j++)
			{
				for (int i = 0; i < width && match; i++)
				{
					aDType c_val = top_verify_ptr[b*top_v_batch_stride + o*top_v_channel_stride + j*top_v_stride + i];
					aDType g_val = top_ptr[b*top_batch_stride + o*top_channel_stride + j*top_stride + i];
					double err = CalculateErr(c_val, g_val);
					if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val)  )
					{
						std::cout << "Difference " << err << " too large at " << b << "," << o << ", " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
						match = 0;
					}
				}
			}
		}
	}
	top_verify->commitTensor();
	top->commitTensor();
	if (match)
	{
		std::cout << "Passed varifier: layer: conv: " << getName() << std::endl;
	}
	return(ret);
}

/************************************************************************************************************************
**
**				BACKWARD
**
************************************************************************************************************************/


int CDNN_Dnet_layer_conv::internalSetupBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{
		makeBwdDiff();


		// TODO : STRIDE !!!

		CDNN_Tensor<aDType> & bot = getBotFwd();

		int batch_sz = (int)bot.getDim(ANN_TENSOR_4THDIM);

		CDNN_Tensor<aDType> * weights = (CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
		CDNN_Tensor<aDType> * weights_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		CDNN_Tensor<aDType> * weights_df_psum = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.psum"));
		if (weights && !weights_diff)
		{
			weights_diff = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.diff"));
			weights_diff->initTensor(weights->getNDim(), weights->getDims());
			addInternal(weights_diff->getName(), weights_diff);
			weights_diff->allocTensor();
			CDNN_Tensor<aDType> * weights_df_psum = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.diff.psum"), weights->getDim(ANN_TENSOR_WIDTH), weights->getDim(ANN_TENSOR_HEIGHT),
				1, batch_sz);
			weights_df_psum->allocTensor();
			addInternal(weights_df_psum->getName(), weights_df_psum);

		}

		std::string comp_options = getParent().getGenericCompOptions();

#if CNN_VERIFY
		{
			// add system only for verification
			CDNN_Tensor<aDType> * weights_diff = (CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
			CDNN_Tensor<aDType> * weights_diff_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("weights.diff.verify"));
			weights_diff_verify->initTensor(weights_diff->getNDim(), weights_diff->getDims());
			addInternal(weights_diff_verify->getName(), weights_diff_verify);
			weights_diff_verify->allocTensor(_CBUF_MEM_SYS_ONLY);


			CDNN_Tensor<aDType> & bot_df = getBotDiff();
			CDNN_Tensor<aDType> * bot_diff_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("bot.diff.verify"));
			bot_diff_verify->initTensor(bot_df.getNDim(), bot_df.getDims());
			addInternal(bot_diff_verify->getName(), bot_diff_verify);
			bot_diff_verify->allocTensor(_CBUF_MEM_SYS_ONLY);

		}
#else
		comp_options += std::string("  -Wb,-hsail-reg-slots=8 -Wb,-hsail-reg32-pressure-limit=64 -Wb,-hsail-reg64-pressure-limit=64 ");

#endif


		CDNN_Tensor<aDType> & top_df = getTopDiff();

		int pad = getPad();
		int stride = getStep();
		int kernel_size = getKernelSz();
		int outputs = (int)top_df.getDim(ANN_TENSOR_DEPTH);
		int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);


		int top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);
		int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
		int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);
		int top_width = (int)top_df.getDim(ANN_TENSOR_WIDTH);
		int top_height = (int)top_df.getDim(ANN_TENSOR_HEIGHT);
		int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);
		int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
		int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
		int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
		int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);

		int weights_stride = (int)weights_diff->getStride(ANN_TENSOR_WIDTH);
		int weights_channel_stride = (int)weights_diff->getStride(ANN_TENSOR_HEIGHT);
		//	int batch_sz = (int)bot.getDim(ANN_TENSOR_4THDIM);




		int prv_h = 4;
		int prv_w = 4;

		// FIX IT!!!!
		int ocl_group_sz0 = 8;
		int ocl_group_sz1 = 8;

		if (stride == 1)
		{
			if (top_width <= 8)
			{
				prv_w = 1;
			}
			else if (top_width <= 16)
			{
				prv_w = 2;
			}
			else if (top_width <= 32)
			{
				prv_w = 4;
			}
			else
			{
				ocl_group_sz0 = 16;
				prv_w = 4;
			}
		}
		else
		{
			if (top_width <= 8)
			{
				prv_w = 1;
			}
			else
			{
				prv_w = 2;
			}
		}

		if (stride == 1)
		{
			if (top_height <= 8)
			{
				prv_h = 1;
			}
			else if (top_height <= 16)
			{
				prv_h = 2;
			}
			else if (top_height <= 32)
			{
				prv_h = 4;
			}
			else
			{
				ocl_group_sz1 = 16;
				prv_h = 2;
			}
		}
		else
		{
			if (top_height <= 8)
			{
				prv_h = 1;
			}
			else
			{
				prv_h = 2;
			}
		}

		int ocl_group_lg2sz1 = (int)ceil(log((double)ocl_group_sz1) / log(2.));
		int ocl_group_lg2sz0 = (int)ceil(log((double)ocl_group_sz0) / log(2.));

		int convbwd_tile_w = prv_w * ocl_group_sz0;
		int convbwd_tile_h = prv_h * ocl_group_sz1;

		int convbwd_n_tile_h = (top_width + convbwd_tile_w - 1) / convbwd_tile_w;
		int convbwd_n_tile_v = (top_height + convbwd_tile_h - 1) / convbwd_tile_h;


		comp_options +=
			std::string("-D _DNN_CONV_KERNEL_SZ=") + std::to_string((long long)kernel_size)
			+ std::string(" -D _DNN_CONV_N_OUTPUTS=") + std::to_string((long long)outputs)
			+ std::string(" -D _DNN_CONV_N_INPUTS=") + std::to_string((long long)inputs)
			+ std::string(" -D _DNN_CONV_PAD=") + std::to_string((long long)pad)
			+ std::string(" -D _DNN_CONV_STRIDE=") + std::to_string((long long)stride)
			+ std::string(" -D _DNN_CONVBWD_PRV_TOPDF_W=") + std::to_string((long long)prv_w)
			+ std::string(" -D _DNN_CONVBWD_PRV_TOPDF_H=") + std::to_string((long long)prv_h)
			+ std::string(" -D _DNN_CONVBWD_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0)
			+ std::string(" -D _DNN_CONVBWD_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1)
			+ std::string(" -D _DNN_CONVBWD_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0)
			+ std::string(" -D _DNN_CONVBWD_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1)
			+ std::string(" -D _DNN_CONV_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
			+ std::string(" -D _DNN_CONV_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
			+ std::string(" -D _DNN_CONV_BOT_STRIDE=") + std::to_string((long long)bot_stride)
			+ std::string(" -D _DNN_CONVBWD_TOPDF_BATCH_STRIDE=") + std::to_string((long long)top_df_batch_stride)
			+ std::string(" -D _DNN_CONVBWD_TOPDF_CHANNEL_STRIDE=") + std::to_string((long long)top_df_channel_stride)
			+ std::string(" -D _DNN_CONVBWD_TOPDF_STRIDE=") + std::to_string((long long)top_df_stride)
			+ std::string(" -D _DNN_CONV_BOT_WIDTH=") + std::to_string((long long)bot_width)
			+ std::string(" -D _DNN_CONV_BOT_HEIGHT=") + std::to_string((long long)bot_height)
			+ std::string(" -D _DNN_CONV_WEIGHTS_CHANNEL_STRIDE=") + std::to_string((long long)weights_channel_stride)
			+ std::string(" -D _DNN_CONV_WEIGHTS_STRIDE=") + std::to_string((long long)weights_stride)
			+ std::string(" -D _DNN_CONV_TOP_WIDTH=") + std::to_string((long long)top_width)
			+ std::string(" -D _DNN_CONV_TOP_HEIGHT=") + std::to_string((long long)top_height)
			+ std::string(" -D _DNN_CONV_BATCH_SZ=") + std::to_string((long long)batch_sz)
			+ std::string(" -D _DNN_CONVBWD_N_TILES_V=") + std::to_string((long long)convbwd_n_tile_v)
			+ std::string(" -D _DNN_CONVBWD_N_TILES_H=") + std::to_string((long long)convbwd_n_tile_h)
			;

		int do_bias = 1;
		comp_options += std::string(" -D _DNN_CONV_BIAS=") + std::to_string((long long)do_bias);


		// sum over batch
		int sum_grp_sz0 = 256;
		comp_options += std::string(" -D _DNN_CONVBWD_GRP_SZ0=") + std::to_string((long long)sum_grp_sz0);

// wrt B
		CDNN_Tensor<aDType> & bot_df = getBotDiff();

		int bot_df_batch_stride = (int)bot_df.getStride(ANN_TENSOR_DEPTH);
		int bot_df_channel_stride = (int)bot_df.getStride(ANN_TENSOR_HEIGHT);
		int bot_df_stride = (int)bot_df.getStride(ANN_TENSOR_WIDTH);



		// FIX THIS !!!
		int n_outs = 1;

		int n_out_pix_horiz_bwd = 4;
		int n_out_pix_vert_bwd = 2;

		int ocl_group_bwd_sz0 = 8;
		int ocl_group_bwd_sz1 = 8;


		if (bot_width <= 8)
		{
			n_out_pix_horiz_bwd = 1;
		}
		else if (bot_width <= 16 || stride > 1)
		{
			n_out_pix_horiz_bwd = 2;
		}
		else
		{
			n_out_pix_horiz_bwd = 4;
		}


		if (bot_height <= 8)
		{
			n_out_pix_vert_bwd = 1;
		}
		else if (bot_height <= 16)
		{
			n_out_pix_vert_bwd = 2;
		}

		n_outs = (inputs & 1) ? 1 : 2;


		int ocl_group_bwd_lg2sz0 = (int)ceil(log((double)ocl_group_bwd_sz1) / log(2.));;
		int ocl_group_bwd_lg2sz1 = (int)ceil(log((double)ocl_group_bwd_sz0) / log(2.));;

		comp_options += std::string(" -D _DNN_CONV_GROUP_SZ0=") + std::to_string((long long)ocl_group_bwd_sz0)
			+ std::string(" -D _DNN_CONV_GROUP_SZ1=") + std::to_string((long long)ocl_group_bwd_sz1)
			+ std::string(" -D _DNN_CONV_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_bwd_lg2sz0)
			+ std::string(" -D _DNN_CONV_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_bwd_lg2sz1)
			+ std::string(" -D _DNN_CONV_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz_bwd)
			+ std::string(" -D _DNN_CONV_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert_bwd)
			+ std::string(" -D _DNN_CONVBWD_BOTDF_BATCH_STRIDE=") + std::to_string((long long)bot_df_batch_stride)
			+ std::string(" -D _DNN_CONVBWD_BOTDF_CHANNEL_STRIDE=") + std::to_string((long long)bot_df_channel_stride)
			+ std::string(" -D _DNN_CONVBWD_BOTDF_STRIDE=") + std::to_string((long long)bot_df_stride)
			+ std::string(" -D _DNN_CONV_N_OUTS=") + std::to_string((long long)n_outs);

//		comp_options += parent_->getGenericCompOptions();
		// wrt to W
		{
			CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNConvBwd_wrt_W", "aDNNConvBwd1.cl");
			cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);
			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);


			std::vector<size_t> l_wk;
			l_wk.push_back(ocl_group_sz0);
			l_wk.push_back(ocl_group_sz1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);


			std::vector<size_t> g_wk;
			g_wk.push_back(ocl_group_sz0);
			g_wk.push_back(ocl_group_sz1);
			g_wk.push_back(inputs*outputs *batch_sz);
			kern_exe.setGblSize(g_wk);

			//		cl_mem top_df_mem = getTopDiff().getCLMem();
			//		cl_mem bot_mem = getBotFwd().getCLMem();
			//		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
			CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.psum"));
			cl_mem weights_df_mem = weights_df.getCLMem();
			aDType padding_value = getPaddingVal();

			int n_arg = 0;
			ocl_args kern_args;
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_df_mem);
			n_arg++;
			//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(aDType), &padding_value);


			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// could use separate queue

			ocl_bwd_execs_.push_back(kern_exe);

		}
		// sum over batch
		{
			CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNConvBwd_wrt_W_Bsum", "aDNNConvBwd1.cl");
			cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);
			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);


			std::vector<size_t> l_wk;
			l_wk.push_back(sum_grp_sz0);
			l_wk.push_back(1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);

			CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
			CDNN_Tensor<aDType> & weights_df_psum = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.psum"));

			std::vector<size_t> g_wk;
			int weights_df_channel_stride = (int)weights_df.getStride(ANN_TENSOR_HEIGHT);

			g_wk.push_back(weights_df_channel_stride);
			g_wk.push_back(1);
			g_wk.push_back(1);
			kern_exe.setGblSize(g_wk);

			cl_mem weights_df_psum_mem = weights_df_psum.getCLMem();
			cl_mem weights_df_mem = weights_df.getCLMem();

			int n_arg = 0;
			ocl_args kern_args;
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_psum_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_mem);


			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// could use separate queue

			ocl_bwd_execs_.push_back(kern_exe);
		}

	//	comp_options += std::string("  -Wb,-hsail-reg-slots=8 -Wb,-hsail-reg32-pressure-limit=64 -Wb,-hsail-reg64-pressure-limit=64 ");
	// wrt B
		{
			std::string kernel_nm = (stride == 1) ? "aDNNConvBwd_wrt_B" : "aDNNConvBwd_wrt_B2";
			CDNN_OCL_kern_exe kern_exe(&getParent(), kernel_nm, "aDNNConvBwd1.cl");
			cl_kernel  ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);
			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);

			std::vector<size_t> l_wk;
			l_wk.push_back(ocl_group_bwd_sz0);
			l_wk.push_back(ocl_group_bwd_sz1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);

			int n_horiz = (bot_width + n_out_pix_horiz_bwd * ocl_group_bwd_sz0 - 1) / (n_out_pix_horiz_bwd * ocl_group_bwd_sz0);
			int n_vert = (bot_height + n_out_pix_vert_bwd * ocl_group_bwd_sz1 - 1) / (n_out_pix_vert_bwd * ocl_group_bwd_sz1);

			std::vector<size_t> g_wk;
			g_wk.push_back(n_horiz * ocl_group_bwd_sz0);
			g_wk.push_back(n_vert * ocl_group_bwd_sz1);
			g_wk.push_back(inputs*batch_sz / n_outs);
			kern_exe.setGblSize(g_wk);

			//		cl_mem top_df_mem = getTopDiff().getCLMem();
			CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
			cl_mem weights_mem = weights.getCLMem();
			//		cl_mem bot_df_mem = getBotDiff().getCLMem();
			aDType padding_value = getPaddingVal();


			int n_arg = 0;
			ocl_args kern_args;
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_df_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_mem);
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_df_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(aDType), &padding_value);


			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// the same queue
			cl_command_queue convQ = getaDNNOCL().getClQueue(0);

			kern_exe.setOclQueue(convQ);

			ocl_bwd_execs_.push_back(kern_exe);


		}
// weights update

		{
			{
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
				MakeDoubleInternal(weights_df, weights_df.getName() + ".history", CL_MEM_READ_WRITE);
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history");

				weights_df_hist.setTensorInitialValue(0);
			}

	

			std::string comp_options = getParent().getGenericCompOptions();

#if CNN_VERIFY
			{

				// add system only for verification
				CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
				CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history");
				MakeVerificationDouble(weights_df_hist);
				CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(weights_df.getName() + ".history.verify");
				weights_df_hist_v.setTensorInitialValue(0);

				// add system only for verification
				CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
				MakeVerificationDouble(weights);

			}
#else
			comp_options += std::string("  -Wb,-hsail-reg-slots=8 -Wb,-hsail-reg32-pressure-limit=64 -Wb,-hsail-reg64-pressure-limit=64 ");

#endif
			int kernel_size = getKernelSz();

			CDNN_Tensor<aDType> & top_df = getTopDiff();
			CDNN_Tensor<aDType> & bot_df = getBotDiff();

			int outputs = (int)top_df.getDim(ANN_TENSOR_DEPTH);
			int inputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);

			CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
			CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
			CDNN_Tensor<aDType> & weights_df_hist = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history"));

			int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
			int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
			int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
			int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
			int weights_df_hist_stride = (int)weights_df_hist.getStride(ANN_TENSOR_WIDTH);

			int bias_pos = inputs * kernel_size * kernel_size;

			int ocl_grp_sz0 = (weights_width <= 12) ? 8 : 16;
			int ocl_grp_sz1 = (weights_height <= 12) ? 8 : 16;

			comp_options += std::string(" -D _DNN_GROUP_SZ0=") + std::to_string((long long)ocl_grp_sz0)
				+ std::string(" -D _DNN_GROUP_SZ1=") + std::to_string((long long)ocl_grp_sz1)
				+ std::string(" -D _DNN_CONV_BIAS_POS=") + std::to_string((long long)bias_pos)
				+ std::string(" -D _DNN_CONV_WEIGHTS_STRIDE=") + std::to_string((long long)weights_stride)
				+ std::string(" -D _DNN_CONV_WEIGHTS_DF_STRIDE=") + std::to_string((long long)weights_df_stride)
				+ std::string(" -D _DNN_CONV_WEIGHTS_DF_HIST_STRIDE=") + std::to_string((long long)weights_df_hist_stride)
				;

			CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNConvWeightUpdateSGD", "aDNNConvWeightsUpdate.cl");
			cl_kernel  ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);
			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);

			std::vector<size_t> l_wk;
			l_wk.push_back(ocl_grp_sz0);
			l_wk.push_back(ocl_grp_sz1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);


			std::vector<size_t> g_wk;
			g_wk.push_back(weights_width);
			g_wk.push_back(weights_height);
			g_wk.push_back(1);
			kern_exe.setGblSize(g_wk);

			/*
			const __global _FLOAT * weights_df,
			__global _FLOAT * weights,
			__global _FLOAT * weights_hist,
			_FLOAT momentum,
			_FLOAT weights_rate,
			_FLOAT weights_decay,
			_FLOAT bias_rate,
			_FLOAT bias_decay

			*/
			cl_mem weights_df_mem = weights_df.getCLMem();
			cl_mem weights_mem = weights.getCLMem();
			cl_mem weights_hist_mem = weights_df_hist.getCLMem();
			aDType momentum = (aDType)getNet().getNetParams().get_momentum();



			int n_arg = 0;
			ocl_args kern_args;
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_df_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &weights_hist_mem);
			kern_args[n_arg++] = std::make_pair(sizeof(aDType), &momentum);


			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// the same queue
			cl_command_queue convQ = getaDNNOCL().getClQueue(0);

			kern_exe.setOclQueue(convQ);


			ocl_bwd_execs_.push_back(kern_exe);

		}
	}
	return(ret);
}


int CDNN_Dnet_layer_conv::ExecuteBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		int iter = getNTimingIter();


		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}

		for (int i = 0; i < iter; i++)
		{

			{
				// wrt Weights
				cl_mem top_df_mem = getTopDiff().getCLMem();
				cl_mem bot_mem = getBotFwd().getCLMem();
				// execute through specialized object
				ocl_args additional_args;
				additional_args[0] = std::make_pair(sizeof(cl_mem), &top_df_mem);
				additional_args[1] = std::make_pair(sizeof(cl_mem), &bot_mem);

				// might be a separate but currently the same queue
				cl_command_queue convQ = getaDNNOCL().getClQueue(0);

				ocl_bwd_execs_[0].ExecuteNoWait(&additional_args, convQ);
				ocl_bwd_execs_[1].ExecuteNoWait(NULL, convQ);
			}


			{
				// wrt Botttom data

				cl_mem top_df_mem = getTopDiff().getCLMem();
				cl_mem bot_df_mem = getBotDiff().getCLMem();
				ocl_args additional_args;
				additional_args[0] = std::make_pair(sizeof(cl_mem), &top_df_mem);
				additional_args[2] = std::make_pair(sizeof(cl_mem), &bot_df_mem);

				ocl_bwd_execs_[2].ExecuteNoWait(&additional_args);
			}


		}


		if (IsDoTiming())
		{
			clFinish(ocl_bwd_execs_[3].getOclQueue());
			e = mach_absolute_time();
		}
		// verify
#if CNN_VERIFY
		internalVerifyBwd();
#endif
		{
// APPROX for now

			int width = (int)getTopDiff().getDim(ANN_TENSOR_WIDTH);
			int height = (int)getTopDiff().getDim(ANN_TENSOR_HEIGHT);
			// TO DO: check top, bot dim are equal
			int kernel_size = getKernelSz();
			int pad = getPad();
			int stride = getStep();
			int batch_sz = (int)getBotDiff().getDim(ANN_TENSOR_4THDIM);
			int inputs = (int)getBotDiff().getDim(ANN_TENSOR_DEPTH);
			int outputs = (int)getNOutputs();
			iter = (iter <= 0) ? 1 : iter;
			processing_time_ = subtractTimes(e, s);
			int ident = 4;
			double comp_compexity = (double)2 * inputs*width*height*kernel_size*kernel_size*outputs*batch_sz * 2;  // multuiply by 2 due to 2 issues
			if (IsOutMessages())
			{
				printf("Passed layer: convolution back-propagation: \"%s\"\n", getName().c_str());
				printf("%*s" "Arguments: CxWxHxKxKxOxB: %dx%dx%dx%dx%dx%dx%d\n", ident, " ", inputs, width, height, kernel_size, kernel_size, outputs, batch_sz);
				if (IsDoTiming())
				{
					printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, (comp_compexity * iter) / (processing_time_ * 1000000000));
				}
			}
		}
		ret = 0;
	}

	return(ret);
}

int CDNN_Dnet_layer_conv::ExecuteBwdHost(void)
{
	int ret = 0;
	int pad = getPad();
	int stride = getStep();
	int kernel_size = getKernelSz();

	CDNN_Tensor<aDType> & bot = getBotFwd();
	int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
	int batch_sz = (int)bot.getDim(ANN_TENSOR_4THDIM);
	int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);


	CDNN_Tensor<aDType> & top_df = getTopDiff();
	int outputs = (int)top_df.getDim(ANN_TENSOR_DEPTH);
	int top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);
	int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
	int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);
	int top_width = (int)top_df.getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top_df.getDim(ANN_TENSOR_HEIGHT);

	aDType * top_df_ptr = top_df.accessTensor(CL_MAP_READ);


// wrt W
	{

		int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);
		int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
		int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);

		CDNN_Tensor<aDType> & weights_df_v = *(CDNN_Tensor<aDType>*)findInternal("weights.diff.verify");

		int weights_width = (int)weights_df_v.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights_df_v.getDim(ANN_TENSOR_HEIGHT);
		int weights_df_v_stride = (int)weights_df_v.getStride(ANN_TENSOR_WIDTH);
//		int weights_df_v_stride = (int)weights_df_v.getStride(ANN_TENSOR_WIDTH);

		aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
		aDType * weights_df_v_ptr = weights_df_v.accessTensor(CL_MAP_WRITE);

		int im2col_batch_stride = (weights_width - 1) * top_width * top_height; // - bias
		aDType * im2col_ptr = new aDType[im2col_batch_stride * batch_sz];


		memset(weights_df_v_ptr, 0, weights_df_v.getSizeInBytes());
		for (int b = 0; b < batch_sz; ++b)
		{
			_ANN_im2col_cpu<aDType>((const aDType*)&bot_ptr[bot_batch_stride * b], inputs,
				bot_height, bot_width, kernel_size, pad,
				stride, &im2col_ptr[im2col_batch_stride * b]);
// sum up over mini-batch without bias
			_ANN_mm_cpu<aDType>((const aDType*)&top_df_ptr[top_df_batch_stride * b], top_width * top_height, outputs, top_df_channel_stride, 0,
				(const aDType *)&im2col_ptr[im2col_batch_stride * b], top_width * top_height, (weights_width -1), top_width * top_height, _ANN_MM_TRANSPOSE,
				weights_df_v_ptr, (weights_width-1), weights_height, weights_df_v_stride, 0,
				1, 1);
// sum up bias
			for (int o = 0; o < outputs; ++o)
			{
				for (int j = 0; j < top_height; ++j)
				{
					for (int i = 0; i < top_width; i++)
					{
						weights_df_v_ptr[weights_df_v_stride * o + (weights_width - 1)] += top_df_ptr[top_df_batch_stride * b + top_df_channel_stride * o + top_df_stride *j + i];
					}
				}
			}
		
		}

		bot.commitTensor();
		weights_df_v.commitTensor();
		delete[] im2col_ptr;
	}

// wrt Bottom
	{
		// if propogate???

		CDNN_Tensor<aDType> & bot_df_v = *(CDNN_Tensor<aDType>*)findInternal("bot.diff.verify");
		int bot_df_v_batch_stride = (int)bot_df_v.getStride(ANN_TENSOR_DEPTH);
		int bot_df_vchannel_stride = (int)bot_df_v.getStride(ANN_TENSOR_HEIGHT);
		int bot_df_vstride = (int)bot_df_v.getStride(ANN_TENSOR_WIDTH);

		aDType *bot_df_v_ptr = bot_df_v.accessTensor(CL_MAP_WRITE);

		CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType>*)findInternal("weights");

		int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
		int weights_batch_stride = (int)weights.getStride(ANN_TENSOR_DEPTH);
		const aDType * weights_ptr = weights.accessTensor(CL_MAP_READ);

	
		int col_we_df_width = top_width*top_height;
		int col_we_df_height = (weights_width - 1); // - bias
		int col_we_batch_stride = col_we_df_width * col_we_df_height;
		int col_we_stride = col_we_df_width;
		aDType * col_we_df_ptr = new aDType[col_we_batch_stride * batch_sz];


		for (int b = 0; b < batch_sz; ++b)
		{
			_ANN_mm_cpu<aDType>(weights_ptr, (weights_width - 1), weights_height, weights_stride, _ANN_MM_TRANSPOSE,
				(const aDType *)&top_df_ptr[top_df_batch_stride * b], top_width * top_height, outputs, top_df_channel_stride, 0,
				&col_we_df_ptr[col_we_batch_stride * b], col_we_df_width, col_we_df_height, col_we_stride, 0,
				1, 0); //- bias

			_ANN_col2im_cpu<aDType>(&col_we_df_ptr[col_we_batch_stride * b], inputs, bot_height, bot_width, kernel_size, pad,
				stride, &bot_df_v_ptr[bot_df_v_batch_stride*b]);

		}


		delete[] col_we_df_ptr;
		bot_df_v.commitTensor();
		weights.commitTensor();

	}

	top_df.commitTensor();

	return(ret);
}

int CDNN_Dnet_layer_conv::internalVerifyBwd(void)
{
	int ret = 0;
	ExecuteBwdHost();
	int total_match = 1;

	// wrt W
	{

		CDNN_Tensor<aDType> & weights_df_v = *(CDNN_Tensor<aDType>*)findInternal("weights.diff.verify");
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType>*)findInternal("weights.diff");

		int weights_width = (int)weights_df.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights_df.getDim(ANN_TENSOR_HEIGHT);
		int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
		int weights_df_v_stride = (int)weights_df_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_df_ptr = weights_df.accessTensor(CL_MAP_READ);
		aDType * weights_df_v_ptr = weights_df_v.accessTensor(CL_MAP_READ);



		double sqr_accum = 0;
		double max_err = -FLT_MIN;
		int max_b = 0, max_o = 0, max_i = 0, max_j = 0;
		const double allowedEps = 4;
		for (int j = 0; j < weights_height; j++)
		{
			// without bias
			for (int i = 0; i < weights_width; i++)
			{
				aDType c_val = weights_df_v_ptr[j*weights_df_v_stride + i];
				aDType g_val = weights_df_ptr[j*weights_df_stride + i];
				sqr_accum += (c_val - g_val) * (c_val - g_val);
				if (abs(c_val - g_val) > max_err)
				{
					max_err = abs(c_val - g_val);

					max_i = i;
					max_j = j;
				}

			}
		}
		sqr_accum = sqrt(sqr_accum / ((double)weights_height * weights_width));

		int match = 1;

		if (sqr_accum > 0)
		{
			std::cout << "Error in conv back-propagation: " << getName() + " wrt W : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
				" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << std::endl;

			if (sqr_accum > (1. / 1000000000))
			{
				for (int j = 0; j < weights_height && match; j++)
				{
					// without bias
					for (int i = 0; i < weights_width && match; i++)
					{
						aDType c_val = weights_df_v_ptr[j*weights_df_v_stride + i];
						aDType g_val = weights_df_ptr[j*weights_df_stride + i];
						sqr_accum += (c_val - g_val) * (c_val - g_val);


						double err = CalculateErr(c_val, g_val);
						if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val) /*&& abs(c_val - g_val) > 0.00000001*/)
						{
							std::cout << "Difference conv back-propagation: " << getName() << " wrt W " << err << " too large at " << i << "," << j <<
								" c_v = " << std::fixed << std::setw(13) << std::setprecision(11) << c_val <<
								" vs g_val = " << std::fixed << std::setw(13) << std::setprecision(11) << g_val << std::endl;
							match = 0;
							total_match &= match;
						}

					}
				}
			}
		}

		weights_df.commitTensor();
		weights_df_v.commitTensor();


	}

	// wrt to B

	{

		CDNN_Tensor<aDType> & bot_df_v = *(CDNN_Tensor<aDType>*)findInternal("bot.diff.verify");
		int bot_df_v_batch_stride = (int)bot_df_v.getStride(ANN_TENSOR_DEPTH);
		int bot_df_v_channel_stride = (int)bot_df_v.getStride(ANN_TENSOR_HEIGHT);
		int bot_df_v_stride = (int)bot_df_v.getStride(ANN_TENSOR_WIDTH);

		aDType *bot_df_v_ptr = bot_df_v.accessTensor(CL_MAP_READ);

		CDNN_Tensor<aDType> & bot_df = getBotDiff();
		int bot_width = (int)bot_df.getDim(ANN_TENSOR_WIDTH);
		int bot_height = (int)bot_df.getDim(ANN_TENSOR_HEIGHT);
		int bot_df_batch_stride = (int)bot_df.getStride(ANN_TENSOR_DEPTH);
		int bot_df_channel_stride = (int)bot_df.getStride(ANN_TENSOR_HEIGHT);
		int bot_df_stride = (int)bot_df.getStride(ANN_TENSOR_WIDTH);


		int inputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);
		int batchs = (int)bot_df.getDim(ANN_TENSOR_4THDIM);

		aDType *bot_df_ptr = bot_df.accessTensor(CL_MAP_READ);


		CDNN_Tensor<aDType> & top_df = getBotDiff();
		int outputs = (int)top_df.getDim(ANN_TENSOR_DEPTH);

		double sqr_accum = 0;
		double max_err = -FLT_MIN;
		int max_b = 0, max_c = 0, max_i = 0, max_j = 0;

		const double allowedEps = 4;
		for (int b = 0; b < batchs; ++b)
		{
			for (int c = 0; c < inputs; ++c)
			{
				for (int j = 0; j < bot_height; ++j)
				{
					for (int i = 0; i < bot_width; ++i)
					{
						aDType c_val = bot_df_v_ptr[b*bot_df_v_batch_stride + c*bot_df_v_channel_stride + j*bot_df_v_stride + i];
						aDType g_val = bot_df_ptr[b*bot_df_batch_stride + c*bot_df_channel_stride + j*bot_df_stride + i];

						sqr_accum += (c_val - g_val) * (c_val - g_val);
						if (abs(c_val - g_val) > max_err)
						{
							max_err = abs(c_val - g_val);
							max_b = b;
							max_c = c;
							max_i = i;
							max_j = j;
						}

					}
				}
			}
		}

		sqr_accum = sqrt(sqr_accum / ((double)batchs *inputs *bot_height *bot_width));

		int match = 1;

		if (sqr_accum > 0)
		{
			std::cout << "Error in conv back-propagation: " << getName() + " wrt B : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
				" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << std::endl;


			if (sqr_accum > (1. / 1000000000))
			{
				for (int b = 0; b < batchs && match; ++b)
				{
					for (int c = 0; c < inputs && match; ++c)
					{
						for (int j = 0; j < bot_height && match; ++j)
						{
							for (int i = 0; i < bot_width && match; ++i)
							{
								aDType c_val = bot_df_v_ptr[b*bot_df_v_batch_stride + c*bot_df_v_channel_stride + j*bot_df_v_stride + i];
								aDType g_val = bot_df_ptr[b*bot_df_batch_stride + c*bot_df_channel_stride + j*bot_df_stride + i];


								double err = CalculateErr(c_val, g_val);
								if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val)  /*&& abs(c_val - g_val) > 0.00000001*/)
								{
									std::cout << "Difference in conv back-propagation: " << getName() << " wrt B " << err << " too large at " << b << "," << c << ", " << i << "," << j <<
										" c_v = " << std::fixed << std::setw(11) << std::setprecision(9) << c_val <<
										" vs g_val = " << std::fixed << std::setw(11) << std::setprecision(9) << g_val << std::endl;
									match = 0;
									total_match &= match;
								}
							}
						}
					}
				}
			}
		}


		bot_df_v.commitTensor();
		bot_df.commitTensor();
	}


	if (total_match)
	{
		std::cout << "Passed varifier: layer: conv back-propagation: " << getName() << std::endl;
	}

	// TEST

//	exit(0);


	return(ret);
}


/************************************************************************************************************************
**
**				UPDATE WEIGHTS
**
************************************************************************************************************************/


int CDNN_Dnet_layer_conv::Update(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		int iter = getNTimingIter();


		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}

		for (int i = 0; i < iter; i++)
		{
			// rates changes
			aDType cur_weights_l_rate;
			aDType cur_weights_decay;
			aDType cur_bias_l_rate;
			aDType cur_bias_decay;
			UpdateRates(cur_weights_l_rate, cur_weights_decay,
				(aDType)getWeightsLr(), (aDType)getWeightsDecay());
			UpdateRates(cur_bias_l_rate, cur_bias_decay,
				(aDType)getBiasLr(), (aDType)getBiasDecay());

			// execute through specialized object
			ocl_args additional_args;
			additional_args[4] = std::make_pair(sizeof(aDType), &cur_weights_l_rate);
			additional_args[5] = std::make_pair(sizeof(aDType), &cur_weights_decay);
			additional_args[6] = std::make_pair(sizeof(aDType), &cur_bias_l_rate);
			additional_args[7] = std::make_pair(sizeof(aDType), &cur_bias_decay);


			ocl_bwd_execs_[3].ExecuteNoWait(&additional_args);

		}

		if (IsDoTiming())
		{
			clFinish(ocl_bwd_execs_[3].getOclQueue());
			e = mach_absolute_time();
		}
		// verify
#if CNN_VERIFY
		internalVerifyUpdate();
#endif
		{
			// APPROX for now
			CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));

			int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
			int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
			iter = (iter <= 0) ? 1 : iter;
			processing_time_ = subtractTimes(e, s);
			int ident = 4;
			double comp_compexity = (double)weights_width * weights_height * 6;  // multuiply by 2 due to 2 issues
			if (IsOutMessages())
			{
				printf("Passed layer: update convolution weights: \"%s\"\n", getName().c_str());
				printf("%*s" "Arguments: WxH: %dx%d\n", ident, " ", weights_width, weights_height);
				if (IsDoTiming())
				{
					printf("%*s" "Performance: %6.2f ms, %6.3f TFLOPs\n", ident, " ", processing_time_ / iter, (comp_compexity * iter) / (processing_time_ * 1000000000));
				}
			}
		}

//		printf("Passed layer: update convolution weights: \"%s\"\n", getName().c_str());

		ret = 0;
	}

	return(ret);
}


int CDNN_Dnet_layer_conv::UpdateHost(void)
{
	int ret = 0;

	// rates changes
	aDType cur_weights_l_rate;
	aDType cur_weights_decay;
	aDType cur_bias_l_rate;
	aDType cur_bias_decay;
	UpdateRates(cur_weights_l_rate, cur_weights_decay,
		(aDType)getWeightsLr(), (aDType)getWeightsDecay());
	UpdateRates(cur_bias_l_rate, cur_bias_decay,
		(aDType)getBiasLr(), (aDType)getBiasDecay());
	aDType momentum = (aDType)getNet().getNetParams().get_momentum();

	//weights
	{

		CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
		CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
		CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history.verify"));

		int weights_width = (int)weights_v.getDim(ANN_TENSOR_WIDTH);
		int weights_height = (int)weights_v.getDim(ANN_TENSOR_HEIGHT);
		int weights_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);
		int weights_df_stride = (int)weights_df.getStride(ANN_TENSOR_WIDTH);
		int weights_df_hist_stride = (int)weights_df_hist_v.getStride(ANN_TENSOR_WIDTH);

		aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		aDType * weights_df_ptr = weights_df.accessTensor(CL_MAP_READ);
		aDType * weights_df_hist_v_ptr = weights_df_hist_v.accessTensor(CL_MAP_READ | CL_MAP_WRITE);
		for (int j = 0; j < weights_height; ++j)
		{
			for (int i = 0; i < weights_width; ++i)
			{
				aDType we_out;
				aDType we_hist_out;
				aDType we_hist = weights_df_hist_v_ptr[j * weights_df_hist_stride + i];
				aDType we_df = weights_df_ptr[j * weights_df_stride + i];
				aDType we = weights_v_ptr[j * weights_stride + i];
				aDType rate, decay;
				rate = (i == weights_width - 1) ? cur_bias_l_rate : cur_weights_l_rate;
				decay = (i == weights_width - 1) ? cur_bias_decay : cur_weights_decay;
				annCalculateWeightsUpdate(&we_hist_out, &we_out,
					we_hist, we_df, we,
					momentum,
					rate, decay
					);
#if 0
				if (j == 0 && i == 0)
				{
					printf("C:wet: %13.11f %13.11f %13.11f %13.11f %13.11f %13.11f %13.11f %13.11f\n",
						we_hist_out, we_out,
						we_hist, we_df, we,
						momentum,
						rate, decay
						);
				}

#endif
				weights_df_hist_v_ptr[j * weights_df_hist_stride + i] = we_hist_out;
				weights_v_ptr[j * weights_stride + i] = we_out;
			}
		}

		weights_v.commitTensor();
		weights_df.commitTensor();
		weights_df_hist_v.commitTensor();

	}
	return(ret);
}

int CDNN_Dnet_layer_conv::internalVerifyUpdate(void)
{
	int ret = 0;

	UpdateHost();
	//weights


	CDNN_Tensor<aDType> & weights_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.verify"));
	CDNN_Tensor<aDType> & weights = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights"));
	CDNN_Tensor<aDType> & weights_df = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff"));
	CDNN_Tensor<aDType> & weights_df_hist_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("weights.diff.history.verify"));

	int weights_width = (int)weights.getDim(ANN_TENSOR_WIDTH);
	int weights_height = (int)weights.getDim(ANN_TENSOR_HEIGHT);
	int weights_stride = (int)weights.getStride(ANN_TENSOR_WIDTH);
	int weights_v_stride = (int)weights_v.getStride(ANN_TENSOR_WIDTH);

	aDType * weights_ptr = weights.accessTensor(CL_MAP_READ);
	aDType * weights_v_ptr = weights_v.accessTensor(CL_MAP_READ);

	double sqr_accum = 0;
	double max_err = -FLT_MIN;
	int max_b = 0, max_c = 0, max_i = 0, max_j = 0;

	double allowedEps = 2;
	for (int j = 0; j < weights_height; ++j)
	{
		for (int i = 0; i < weights_width; ++i)
		{
			aDType c_val = weights_v_ptr[j * weights_v_stride + i];
			aDType g_val = weights_ptr[j * weights_stride + i];

			sqr_accum += (c_val - g_val) * (c_val - g_val);
			if (abs(c_val - g_val) > max_err)
			{
				max_err = abs(c_val - g_val);
				//					max_b = b;
				//					max_c = c;
				max_i = i;
				max_j = j;
			}

		}
	}

	sqr_accum = sqrt(sqr_accum / ((double)weights_height * weights_width));

	int match = 1;

	if (sqr_accum > 0)
	{
		std::cout << "Error in n conv weights update: " << getName() + " : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
			" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << std::endl;

		if (sqr_accum > (1. / 1000000000))
		{

			for (int j = 0; j < weights_height && match; ++j)
			{
				for (int i = 0; i < weights_width && match; ++i)
				{
					aDType c_val = weights_v_ptr[j * weights_v_stride + i];
					aDType g_val = weights_ptr[j * weights_stride + i];

					double err = CalculateErr(c_val, g_val);
					if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
					{
						std::cout << "Difference in conv weights update: " << getName() + " : " << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
						match = 0;
					}

				}
			}
		}
	}

	weights_v.commitTensor();
	weights.commitTensor();



	if (match)
	{
		std::cout << "Passed varifier: layer: convolutional weights update: " << getName() << std::endl;
	}
	return(ret);
}


////////////////////////////////////////////////////////
//
//  POOLING
//
///////////////////////////////////////////////////////
CDNN_Dnet_layer_pooling :: CDNN_Dnet_layer_pooling()
{
}

CDNN_Dnet_layer_pooling::CDNN_Dnet_layer_pooling(CDNN_OVX * parent, void * wrapper, std::string name, const void * parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Dnet_layer(parent, wrapper, name,	parameters, ref_layer)
{
}

/************************************************************************************************************************
**
**				FORWARD
**
************************************************************************************************************************/

int CDNN_Dnet_layer_pooling :: Execute(void)
{
	int ret = -1;
	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top = getTopFwd();;



		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{
			cl_mem bot_mem = bot.getCLMem();
			cl_mem top_mem = top.getCLMem();
			// execute through specialized object
			ocl_args additional_args;
			additional_args[0] = std::make_pair(sizeof(cl_mem), &bot_mem);
			additional_args[1] = std::make_pair(sizeof(cl_mem), &top_mem);

			ocl_fwd_execs_[0].ExecuteNoWait(&additional_args);
		}

		if (IsDoTiming())
		{
			clFinish(ocl_bwd_execs_[0].getOclQueue());
			e = mach_absolute_time();
		}


#if CNN_VERIFY
		internalVerify();
#endif
		int out_width = (int)top.getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)top.getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)top.getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;

		if (IsOutMessages())
		{
			printf("Passed layer: pooling: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxIN_WxIN_HxOUT_Wx_OUT_HxOxB: %dx%dx%dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, out_width, out_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;

	
	return(ret);
}


int CDNN_Dnet_layer_pooling :: internalSetup(void)
{
	int ret = 0;

	size_t outputs = 0;
	size_t batch_sz = 0;
	size_t inputs = 0;
	size_t bot_width = 0;
	size_t bot_height = 0;
	size_t top_width = 0;
	size_t top_height = 0;
	CDNN_Tensor<aDType> * bot = NULL;
	CDNN_Tensor<aDType> * top = NULL;

	if (bottom_lyr_)
	{

		bot = &bottom_lyr_->getTopFwd();
		bot_width = bot->getDim(ANN_TENSOR_WIDTH);
		bot_height = bot->getDim(ANN_TENSOR_HEIGHT);
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);
		outputs = inputs = bot->getDim(ANN_TENSOR_DEPTH);

	}

	int kernel_size = getKernelSz( );
	int pad = getPad();
	int stride = getStep();
	assert(stride > 0 );
	size_t top_dims[4];

	if (top_lyr_)
	{
		top =&getTopFwd();
		CDNN_Tensor<aDType> * bot = &top_lyr_->getBotFwd();
		if (top==bot && !top->IsInited())
		{
// pooling width/height
			top_width = (size_t)ceil(((float)bot_width + 2.f * pad - (float)kernel_size) / (float)stride) + 1;
			top_height = (size_t)ceil(((float)bot_height + 2.f * pad - (float)kernel_size) / (float)stride) + 1;
			top->setName(getTopName());
			top->setParent(parent_);
			top_dims[ANN_TENSOR_WIDTH] = top_width;
			top_dims[ANN_TENSOR_HEIGHT] = top_height;
			top_dims[ANN_TENSOR_DEPTH] = outputs;
			top_dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, top_dims);
		}
	}

	int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top->getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top->getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);

	int ocl_group_sz0 = 8;
	int ocl_group_sz1 = 8;
	int ocl_group_lg2sz0 = (int)ceil(log((double)ocl_group_sz0)/log(2.));
	int ocl_group_lg2sz1 = (int)ceil(log((double)ocl_group_sz1)/log(2.));;
	int n_out_pix_horiz = 1;
	int n_out_pix_vert = 1;


	if (top_width < ocl_group_sz0 * 4 || top_height < ocl_group_sz1 * 4)
	{
		n_out_pix_horiz = 1;
		n_out_pix_vert = 1;
	}
	else
	{
		n_out_pix_horiz = 2;
		n_out_pix_vert = 2;
	}

	int pooling_method = getPoolingMethod();
	int op_id = (pooling_method == dnet_pooling_method_MAX) ? _DNN_POOLING_OP_MAX : _DNN_POOLING_OP_AVE;

	int m_indxs_stride = 1;
	int m_indxs_channel_stride = 1;
	int m_indxs_batch_stride = 1;


	if (pooling_method == dnet_pooling_method_MAX)
	{
		CDNN_Tensor<unsigned char> *m_indxs = new CDNN_Tensor<unsigned char>(&getParent(), NULL, std::string("max_indxs"));
		m_indxs->initTensor(top->getNDim(), top->getDims());
		addInternal(m_indxs->getName(), m_indxs);
		m_indxs->allocTensor();

		m_indxs_stride = (int)m_indxs->getStride(ANN_TENSOR_WIDTH);
		m_indxs_channel_stride = (int)m_indxs->getStride(ANN_TENSOR_HEIGHT);
		m_indxs_batch_stride = (int)m_indxs->getStride(ANN_TENSOR_DEPTH);
	}

	std::string comp_options = 
	std::string(" -D _DNN_POOLING_KERNEL_SZ=") + std::to_string((long long)kernel_size)
	+ std::string(" -D _DNN_POOLING_OP_ID=") + std::to_string((long long)op_id)
	+ std::string(" -D _DNN_POOLING_N_OUTPUTS=") + std::to_string((long long)outputs)
	+ std::string(" -D _DNN_POOLING_N_CHANNELS=") + std::to_string((long long)inputs)
	+ std::string(" -D _DNN_POOLING_PAD=") + std::to_string((long long)pad)
	+ std::string(" -D _DNN_POOLING_STRIDE=") + std::to_string((long long)stride)
	+ std::string(" -D _DNN_POOLING_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz)
	+ std::string(" -D _DNN_POOLING_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert)
	+ std::string(" -D _DNN_POOLING_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0)
	+ std::string(" -D _DNN_POOLING_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1)
	+ std::string(" -D _DNN_POOLING_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0)
	+ std::string(" -D _DNN_POOLING_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1)
	+ std::string(" -D _DNN_POOLING_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
	+ std::string(" -D _DNN_POOLING_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
	+ std::string(" -D _DNN_POOLING_BOT_STRIDE=") + std::to_string((long long)bot_stride)
	+ std::string(" -D _DNN_POOLING_TOP_BATCH_STRIDE=") + std::to_string((long long)top_batch_stride)
	+ std::string(" -D _DNN_POOLING_TOP_CHANNEL_STRIDE=") + std::to_string((long long)top_channel_stride)
	+ std::string(" -D _DNN_POOLING_TOP_STRIDE=") + std::to_string((long long)top_stride)
	+ std::string(" -D _DNN_POOLING_BOT_WIDTH=") + std::to_string((long long)bot_width)
	+ std::string(" -D _DNN_POOLING_BOT_HEIGHT=") + std::to_string((long long)bot_height)
	+ std::string(" -D _DNN_POOLING_TOP_WIDTH=") + std::to_string((long long)top_width)
	+ std::string(" -D _DNN_POOLING_TOP_HEIGHT=") + std::to_string((long long)top_height)
	+ std::string(" -D _DNN_POOLING_MINDX_BATCH_STRIDE=") + std::to_string((long long)m_indxs_batch_stride)
	+ std::string(" -D _DNN_POOLING_MINDX_CHANNEL_STRIDE=") + std::to_string((long long)m_indxs_channel_stride)
	+ std::string(" -D _DNN_POOLING_MINDX_STRIDE=") + std::to_string((long long)m_indxs_stride)
	
	

	+ parent_->getGenericCompOptions()
	;


//	int pooling_method = getPoolingMethod();
	if (pooling_method == dnet_pooling_method_MAX)
	{
		CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNPoolingMax", "aDNNPooling.cl");
// compile 
		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);

		kern_exe.setOclKern(ocl_kernel);
		kern_exe.setKernBuildOptions(comp_options);
		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top = getTopFwd();
		CDNN_Tensor<unsigned char> &m_indxs = *(CDNN_Tensor<unsigned char> *)findInternal(std::string("max_indxs"));


		std::vector<size_t> l_wk;
		l_wk.push_back(ocl_group_sz0);
		l_wk.push_back(ocl_group_sz1);
		l_wk.push_back(1);
		kern_exe.setLclSize(l_wk);


		int g_wk_width = (int)((top.getDim(ANN_TENSOR_WIDTH) + ocl_group_sz0 * n_out_pix_horiz - 1) / (ocl_group_sz0 * n_out_pix_horiz));
		int g_wk_height = (int)((top.getDim(ANN_TENSOR_HEIGHT) + ocl_group_sz1 * n_out_pix_vert - 1) / (ocl_group_sz1 * n_out_pix_vert));

		// n groups, n outputs, batch size

		std::vector<size_t> g_wk;
		g_wk.push_back(g_wk_width * ocl_group_sz0);
		g_wk.push_back(g_wk_height * ocl_group_sz1);
		g_wk.push_back(top.getDim(ANN_TENSOR_DEPTH) * top.getDim(ANN_TENSOR_4THDIM));
		kern_exe.setGblSize(g_wk);

//		cl_mem bot_mem = bot.getCLMem();
//		cl_mem top_mem = top.getCLMem();
		cl_mem m_indxs_mem = m_indxs.getCLMem();

		int n_arg = 0;
		ocl_args kern_args;
		n_arg++;
		//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_mem);
		n_arg++;
		//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_mem);
		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &m_indxs_mem);


		ret = CL_SUCCESS;

		ocl_args::iterator ai;
		for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
		{
			int i = (*ai).first;
			ocl_arg arg = (*ai).second;
			ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

		}

		CHECK_OPENCL_ERROR(ret, "parmeters failed.");

		kern_exe.setKernArgs(kern_args);

		// the same queue
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		kern_exe.setOclQueue(convQ);

		ocl_fwd_execs_.push_back(kern_exe);

	}
	else if (pooling_method == dnet_pooling_method_AVE)
	{
		CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNPoolingAve", "aDNNPooling.cl");
		// compile 
		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);

		kern_exe.setOclKern(ocl_kernel);
		kern_exe.setKernBuildOptions(comp_options);
		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top = getTopFwd();


		std::vector<size_t> l_wk;
		l_wk.push_back(ocl_group_sz0);
		l_wk.push_back(ocl_group_sz1);
		l_wk.push_back(1);
		kern_exe.setLclSize(l_wk);


		int g_wk_width = (int)((top.getDim(ANN_TENSOR_WIDTH) + ocl_group_sz0 * n_out_pix_horiz - 1) / (ocl_group_sz0 * n_out_pix_horiz));
		int g_wk_height = (int)((top.getDim(ANN_TENSOR_HEIGHT) + ocl_group_sz1 * n_out_pix_vert - 1) / (ocl_group_sz1 * n_out_pix_vert));

		// n groups, n outputs, batch size

		std::vector<size_t> g_wk;
		g_wk.push_back(g_wk_width * ocl_group_sz0);
		g_wk.push_back(g_wk_height * ocl_group_sz1);
		g_wk.push_back(top.getDim(ANN_TENSOR_DEPTH) * top.getDim(ANN_TENSOR_4THDIM));
		kern_exe.setGblSize(g_wk);

		//		cl_mem bot_mem = bot.getCLMem();
		//		cl_mem top_mem = top.getCLMem();

		int n_arg = 0;
		ocl_args kern_args;
		n_arg++;
		//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_mem);
		n_arg++;
		//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_mem);


		ret = CL_SUCCESS;

		ocl_args::iterator ai;
		for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
		{
			int i = (*ai).first;
			ocl_arg arg = (*ai).second;
			ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

		}

		CHECK_OPENCL_ERROR(ret, "parmeters failed.");

		kern_exe.setKernArgs(kern_args);

		// the same queue
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		kern_exe.setOclQueue(convQ);

		ocl_fwd_execs_.push_back(kern_exe);

	}
	else
	{
		printf("Layer: %s. Error: unknowm method\n", getName().c_str());
		ret = -1;
	}


	return(ret);
}

int CDNN_Dnet_layer_pooling::ExecuteHost(void)
{
	int ret = 0;


	return(ret);
}


int CDNN_Dnet_layer_pooling::internalVerify(void)
{
	int ret = 0;

	ret = ExecuteHost();

	CDNN_Tensor<aDType> * bot = &getBotFwd();
	CDNN_Tensor<aDType> * top = &getTopFwd();
	assert(bot && top);
	aDType * bot_ptr = bot->accessTensor(CL_MAP_READ);
	aDType * top_ptr = top->accessTensor(CL_MAP_READ);
	int kernel_size = getKernelSz();
	int pad = getPad();
	int stride = getStep();
	int n_outputs = (int)top->getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top->getDim(ANN_TENSOR_4THDIM);
	int bot_width = (int)bot->getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot->getDim(ANN_TENSOR_HEIGHT);
	int top_width = (int)top->getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top->getDim(ANN_TENSOR_HEIGHT);
	int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top->getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top->getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);


	int match = 1;
	double allowedEps = 3;
	int pooling_method = getPoolingMethod();

	for (int b = 0; b < n_batchs && match; b++)
	{
		for (int o = 0; o < n_outputs && match; o++)
		{
			for (int j = 0; j < top_height && match; j++)
			{
				for (int i = 0; i < top_width && match; i++)
				{
					// c-emulator
					aDType res = 0;
					if (pooling_method == dnet_pooling_method_MAX)
					{
						res = -FLT_MAX;
						int hstart = j * stride;
						int wstart = i * stride;
						int hend = min(hstart + kernel_size, bot_height);
						int wend = min(wstart + kernel_size, bot_width);
						for (int h = hstart; h < hend; ++h)
						{
							for (int w = wstart; w < wend; ++w)
							{
#if 0
								if (b==0 && o==0 && i == 2 && j == 0)
								{
									printf("c:%d %d  %f %f\n", w, h, res, bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w]);
								}
#endif
								res = max(res, bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w]);
							}
						}
					}
					else if (pooling_method == dnet_pooling_method_AVE)
					{
						//						allowedEps = 4;
						res = 0;
						int hstart = j * stride - pad;
						int wstart = i * stride - pad;
						int hend = min(hstart + kernel_size, bot_height + pad);
						int wend = min(wstart + kernel_size, bot_width + pad);
						int pool_size = (hend - hstart) * (wend - wstart);
						hstart = max(hstart, 0);
						wstart = max(wstart, 0);
						hend = min(hend, bot_height);
						wend = min(wend, bot_width);
						for (int h = hstart; h < hend; ++h) {
							for (int w = wstart; w < wend; ++w) {
#if 0
								if (b == 0 && o == 0 && i == 0 && j == 0)
								{
									printf("c:%d %d  %f %f\n", w, h, res, bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w]);
								}
#endif
								res +=
									bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w];
							}
						}
						res /= pool_size;
					}
					else
					{
						std::cout << "ERROR: unknown operator : layer: pooling: " << getName() << std::endl;
						match = 0;
						continue;
					}
					aDType c_val = res;
					aDType g_val = top_ptr[b*top_batch_stride + o * top_channel_stride + j * top_stride + i];
					double err = CalculateErr(c_val, g_val);
					if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
					{
						std::cout << "Difference " << err << " too large at " << i << ", " << j << ", " << o << ", " << b << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
						match = 0;
					}
				}
			}
		}
	}


	if (match)
	{
		std::cout << "Passed varifier: layer: pooling: " << getName() << std::endl;
	}



	top->commitTensor();
	bot->commitTensor();

	return(ret);
}

/************************************************************************************************************************
**
**				BACKWARD
**
************************************************************************************************************************/


int CDNN_Dnet_layer_pooling::internalSetupBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{
		makeBwdDiff();

		int kernel_size = getKernelSz();
		int pad = getPad();
		int stride = getStep();


		CDNN_Tensor<aDType> & bot_df = getBotDiff();
		CDNN_Tensor<aDType> & top_df = getTopDiff();
		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top = getTopFwd();

		int outputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
		int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);
		int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
		int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
		int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
		int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);

		int bot_df_batch_stride = (int)bot_df.getStride(ANN_TENSOR_DEPTH);
		int bot_df_channel_stride = (int)bot_df.getStride(ANN_TENSOR_HEIGHT);
		int bot_df_stride = (int)bot_df.getStride(ANN_TENSOR_WIDTH);

		int n_out_pix_horiz = stride;
		int n_out_pix_vert = stride;
		int ocl_group_sz0 = 8;
		int ocl_group_sz1 = 8;
		int ocl_group_lg2sz0 = (int)ceil(log((double)ocl_group_sz0) / log(2.));
		int ocl_group_lg2sz1 = (int)ceil(log((double)ocl_group_sz1) / log(2.));


		int top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);
		int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
		int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);
		int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
		int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);

		int top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);
		int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
		int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);


#if CNN_VERIFY
		{
			// add system only for verification
			CDNN_Tensor<aDType> * bot_df_v = new CDNN_Tensor<aDType>(&getParent(), NULL, 
				std::string("bot.diff.verify"), bot_df.getDim(ANN_TENSOR_WIDTH), bot_df.getDim(ANN_TENSOR_HEIGHT), bot_df.getDim(ANN_TENSOR_DEPTH), bot_df.getDim(ANN_TENSOR_4THDIM));
			addInternal(bot_df_v->getName(), bot_df_v);
			bot_df_v->allocTensor(_CBUF_MEM_SYS_ONLY);
		}
#endif

		int pooling_method = getPoolingMethod();
		if (pooling_method == dnet_pooling_method_MAX)
		{
			pad = 0;
		}

		std::string comp_options =
			std::string(" -D _DNN_POOLING_KERNEL_SZ=") + std::to_string((long long)kernel_size)
			+ std::string(" -D _DNN_POOLING_N_OUTPUTS=") + std::to_string((long long)outputs)
			+ std::string(" -D _DNN_POOLING_PAD=") + std::to_string((long long)pad)
			+ std::string(" -D _DNN_POOLING_STRIDE=") + std::to_string((long long)stride)
			+ std::string(" -D _DNN_POOLBWD_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz)
			+ std::string(" -D _DNN_POOLBWD_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert)
			+ std::string(" -D _DNN_POOLBWD_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0)
			+ std::string(" -D _DNN_POOLBWD_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1)
			+ std::string(" -D _DNN_POOLBWD_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0)
			+ std::string(" -D _DNN_POOLBWD_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1)
			+ std::string(" -D _DNN_POOLBWD_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
			+ std::string(" -D _DNN_POOLBWD_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
			+ std::string(" -D _DNN_POOLBWD_BOT_STRIDE=") + std::to_string((long long)bot_stride)
			+ std::string(" -D _DNN_POOLBWD_TOP_BATCH_STRIDE=") + std::to_string((long long)top_batch_stride)
			+ std::string(" -D _DNN_POOLBWD_TOP_CHANNEL_STRIDE=") + std::to_string((long long)top_channel_stride)
			+ std::string(" -D _DNN_POOLBWD_TOP_STRIDE=") + std::to_string((long long)top_stride)
			+ std::string(" -D _DNN_POOLBWD_BOT_WIDTH=") + std::to_string((long long)bot_width)
			+ std::string(" -D _DNN_POOLBWD_BOT_HEIGHT=") + std::to_string((long long)bot_height)
			+ std::string(" -D _DNN_POOLBWD_TOP_WIDTH=") + std::to_string((long long)top_width)
			+ std::string(" -D _DNN_POOLBWD_TOP_HEIGHT=") + std::to_string((long long)top_height)
			+ std::string(" -D _DNN_POOLBWD_BOTDF_BATCH_STRIDE=") + std::to_string((long long)bot_df_batch_stride)
			+ std::string(" -D _DNN_POOLBWD_BOTDF_CHANNEL_STRIDE=") + std::to_string((long long)bot_df_channel_stride)
			+ std::string(" -D _DNN_POOLBWD_BOTDF_STRIDE=") + std::to_string((long long)bot_df_stride)
			+ std::string(" -D _DNN_POOLBWD_TOPDF_BATCH_STRIDE=") + std::to_string((long long)top_df_batch_stride)
			+ std::string(" -D _DNN_POOLBWD_TOPDF_CHANNEL_STRIDE=") + std::to_string((long long)top_df_channel_stride)
			+ std::string(" -D _DNN_POOLBWD_TOPDF_STRIDE=") + std::to_string((long long)top_df_stride)

			+ parent_->getGenericCompOptions()
			;

	


		int g_wk_width = (int)((bot_df.getDim(ANN_TENSOR_WIDTH) + ocl_group_sz0 * n_out_pix_horiz - 1) / (ocl_group_sz0 * n_out_pix_horiz));
		int g_wk_height = (int)((bot_df.getDim(ANN_TENSOR_HEIGHT) + ocl_group_sz1 * n_out_pix_vert - 1) / (ocl_group_sz1 * n_out_pix_vert));


		if (pooling_method == dnet_pooling_method_MAX)
		{
			CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNPoolingMaxBwd", "aDNNPoolingBwd.cl");
			// compile 
			cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);

			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);
			CDNN_Tensor<unsigned char> &m_indxs = *(CDNN_Tensor<unsigned char> *)findInternal(std::string("max_indxs"));


			std::vector<size_t> l_wk;
			l_wk.push_back(ocl_group_sz0);
			l_wk.push_back(ocl_group_sz1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);



			// n groups, n outputs, batch size

			std::vector<size_t> g_wk;
			g_wk.push_back(g_wk_width * ocl_group_sz0);
			g_wk.push_back(g_wk_height * ocl_group_sz1);
			g_wk.push_back(bot_df.getDim(ANN_TENSOR_DEPTH) * bot_df.getDim(ANN_TENSOR_4THDIM));
			kern_exe.setGblSize(g_wk);

			//		cl_mem top_df_mem = top_df.getCLMem();
			//		cl_mem bot_df_mem = bot_df.getCLMem();


			int n_arg = 0;
			ocl_args kern_args;
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_df_mem);
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_mem);
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_mem);
			n_arg++;
			//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_df_mem);



			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// the same queue
			cl_command_queue convQ = getaDNNOCL().getClQueue(0);

			kern_exe.setOclQueue(convQ);

			ocl_bwd_execs_.push_back(kern_exe);

		}
		else if (pooling_method == dnet_pooling_method_AVE)
		{
			CDNN_OCL_kern_exe kern_exe(&getParent(), "aDNNPoolingAveBwd", "aDNNPoolingBwd.cl");
			// compile 
			cl_kernel ocl_kernel = getaDNNOCL().getKernel(kern_exe.getKernFileNm(), kern_exe.getKernNm(), comp_options);

			kern_exe.setOclKern(ocl_kernel);
			kern_exe.setKernBuildOptions(comp_options);


			std::vector<size_t> l_wk;
			l_wk.push_back(ocl_group_sz0);
			l_wk.push_back(ocl_group_sz1);
			l_wk.push_back(1);
			kern_exe.setLclSize(l_wk);


			// n groups, n outputs, batch size

			std::vector<size_t> g_wk;
			g_wk.push_back(g_wk_width * ocl_group_sz0);
			g_wk.push_back(g_wk_height * ocl_group_sz1);
			g_wk.push_back(bot_df.getDim(ANN_TENSOR_DEPTH) * bot_df.getDim(ANN_TENSOR_4THDIM));
			kern_exe.setGblSize(g_wk);

			//		cl_mem top_df_mem = top_df.getCLMem();
			//		cl_mem bot_df_mem = bot_df.getCLMem();

			int n_arg = 0;
			ocl_args kern_args;
			n_arg++;
			//		kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &top_df_mem);
			n_arg++;
			//	kern_args[n_arg++] = std::make_pair(sizeof(cl_mem), &bot_df_mem);


			ret = CL_SUCCESS;

			ocl_args::iterator ai;
			for (ai = kern_args.begin(); ai != kern_args.end(); ++ai)
			{
				int i = (*ai).first;
				ocl_arg arg = (*ai).second;
				ret |= clSetKernelArg(ocl_kernel, i, arg.first, arg.second);

			}

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");

			kern_exe.setKernArgs(kern_args);

			// the same queue
			cl_command_queue convQ = getaDNNOCL().getClQueue(0);

			kern_exe.setOclQueue(convQ);

			ocl_bwd_execs_.push_back(kern_exe);

		}
		else
		{
			printf("Layer: %s. Error: unknowm method\n", getName().c_str());
			ret = -1;
		}


	}

	return(ret);
}


int CDNN_Dnet_layer_pooling::ExecuteBwd(void)
{
	int ret = -1;
	if (doNeedBackProp())
	{

		CDNN_Tensor<aDType> &bot_df = getBotDiff();
		CDNN_Tensor<aDType> &top_df = getTopDiff();
		CDNN_Tensor<aDType> &bot = getBotFwd();
		CDNN_Tensor<aDType> &top = getTopFwd();



		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{

			cl_mem bot_df_mem = bot_df.getCLMem();
			cl_mem top_df_mem = top_df.getCLMem();
			cl_mem bot_mem = bot.getCLMem();
			cl_mem top_mem = top.getCLMem();

			// execute through specialized object
			ocl_args additional_args;
			int pooling_method = getPoolingMethod();
			if (pooling_method == dnet_pooling_method_MAX)
			{
				additional_args[0] = std::make_pair(sizeof(cl_mem), &top_df_mem);
				additional_args[1] = std::make_pair(sizeof(cl_mem), &top_mem);
				additional_args[2] = std::make_pair(sizeof(cl_mem), &bot_mem);
				additional_args[3] = std::make_pair(sizeof(cl_mem), &bot_df_mem);
			}
			else
			{
				additional_args[0] = std::make_pair(sizeof(cl_mem), &top_df_mem);
				additional_args[1] = std::make_pair(sizeof(cl_mem), &bot_df_mem);
			}
			ocl_bwd_execs_[0].ExecuteNoWait(&additional_args);

		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


#if CNN_VERIFY
		internalVerifyBwd();
#endif
		int out_width = (int)bot_df.getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)bot_df.getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)top_df.getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)top_df.getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)bot_df.getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;

		if (IsOutMessages())
		{
			printf("Passed layer: pooling back-propagation: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxIN_WxIN_HxOUT_Wx_OUT_HxOxB: %dx%dx%dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, out_width, out_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;



	}

	return(ret);
}

int CDNN_Dnet_layer_pooling::ExecuteBwdHost(void)
{
	int ret = 0;

	CDNN_Tensor<aDType> & bot_df_v = *(CDNN_Tensor<aDType>*)findInternal("bot.diff.verify");
	CDNN_Tensor<aDType> & top_df = getTopDiff();
	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top = getTopFwd();

	aDType * bot_df_v_ptr = bot_df_v.accessTensor(CL_MAP_WRITE);
	aDType * top_df_ptr = top_df.accessTensor(CL_MAP_READ);
	aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	int kernel_size = getKernelSz();
	int pad = getPad();
	int stride = getStep();


	int bot_df_v_batch_stride = (int)bot_df_v.getStride(ANN_TENSOR_DEPTH);
	int bot_df_v_channel_stride = (int)bot_df_v.getStride(ANN_TENSOR_HEIGHT);
	int bot_df_v_stride = (int)bot_df_v.getStride(ANN_TENSOR_WIDTH);
	int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);
	int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
	int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
	int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);
	int n_outputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)bot.getDim(ANN_TENSOR_4THDIM);

	int top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);
	int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
	int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);
	int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);
	int top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);
	int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
	int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);

	int pooling_method = getPoolingMethod();
	memset(bot_df_v_ptr, 0, bot_df_v.getSizeInBytes());
	for (int b = 0; b < n_batchs; b++)
	{
		for (int o = 0; o < n_outputs; o++)
		{
			int  bot_off = b * bot_batch_stride + o * bot_channel_stride;
			int  bot_df_v_off = b * bot_df_v_batch_stride + o * bot_df_v_channel_stride;
			int  top_df_off = b * top_df_batch_stride + o * top_df_channel_stride;
			int  top_off = b * top_batch_stride + o * top_channel_stride;

			if (pooling_method == dnet_pooling_method_MAX)
			{

				for (int j = 0; j < top_height; j++)
				{
					for (int i = 0; i < top_width; i++)
					{

						int hstart = j * stride;
						int wstart = i * stride;
						int hend = min(hstart + kernel_size, bot_height);
						int wend = min(wstart + kernel_size, bot_width);
						for (int h = hstart; h < hend; ++h) {
							for (int w = wstart; w < wend; ++w) {
								bot_df_v_ptr[bot_df_v_off + h * bot_df_v_stride + w] +=
									top_df_ptr[top_df_off + j * top_df_stride + i] *
									(bot_ptr[bot_off + h * bot_stride + w] ==
									top_ptr[top_off + j * top_stride + i]);
#if 0
								if (b==0 && o ==5 && w == 17 && h == 0)
								{
									printf("C:max: %d %d   %13.11f  %13.11f  %13.11f %13.11f\n",
										i, j,
										bot_df_v_ptr[bot_df_v_off + h * bot_df_v_stride + w],
										top_df_ptr[top_df_off + j * top_df_stride + i],
										bot_ptr[bot_off + h * bot_stride + w],
										top_ptr[top_off + j * top_stride + i]
										);
								}
#endif
							}
						}

					}
				}

			}
			else if (pooling_method == dnet_pooling_method_AVE)
			{

				for (int j = 0; j < bot_height; j++)
				{
					for (int i = 0; i < bot_width; i++)
					{
						// c-emulator
						aDType res = 0;

						res = 0;
						bot_df_v_ptr[bot_df_v_off + j * bot_df_v_stride + i] = 0;
						int w = i + pad;
						int h = j + pad;
						int phstart = (h < kernel_size) ? 0 : (h - kernel_size) / stride + 1;
						int phend = min(h / stride + 1, top_height);
						int pwstart = (w < kernel_size) ? 0 : (w - kernel_size) / stride + 1;
						int pwend = min(w / stride + 1, top_width);
						aDType gradient = 0;
						for (int ph = phstart; ph < phend; ++ph) {
							for (int pw = pwstart; pw < pwend; ++pw) {
								// figure out the pooling size
								int hstart = ph * stride - pad;
								int wstart = pw * stride - pad;
								int hend = min(hstart + kernel_size, bot_height + pad);
								int wend = min(wstart + kernel_size, bot_width + pad);
								int pool_size = (hend - hstart) * (wend - wstart);
								gradient += top_df_ptr[top_df_off + ph * top_df_stride + pw] / pool_size;

#if 0
								if (b==0 && o==3 && i == 6 && j == 0)
								{
									printf("C:com: %10.8f %10.8f %10.8f %d\n", gradient, top_ptr[top_off + ph * top_stride + pw] / pool_size, top_ptr[top_off + ph * top_stride + pw], pool_size);
								}

#endif
							}
						}
						bot_df_v_ptr[bot_df_v_off + j * bot_df_v_stride + i] = gradient;
					}
				}
			}
			else
			{
				std::cout << "ERROR: unknown operator : layer: pooling back-propagation: " << getName() << std::endl;
				continue;
			}
#if 0
			aDType c_val = res;
			aDType g_val = top_ptr[b*top_batch_stride + o * top_channel_stride + j * top_stride + i];
			double err = CalculateErr(c_val, g_val);
			if (err > allowedEps)
			{
				std::cout << "Difference " << err << " too large at " << i << ", " << j << ", " << o << ", " << b << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
				match = 0;
			}
#endif


				}
	}


	top.commitTensor();
	bot.commitTensor();
	bot_df_v.commitTensor();
	top_df.commitTensor();


	return(ret);
}

int CDNN_Dnet_layer_pooling::internalVerifyBwd(void)
{
	int ret = 0;
	ExecuteBwdHost();
	CDNN_Tensor<aDType> & bot_diff_v = *(CDNN_Tensor<aDType>*)findInternal("bot.diff.verify");
	CDNN_Tensor<aDType> & bot_diff = getBotDiff();
	int bot_batch_stride = (int)bot_diff.getStride(ANN_TENSOR_DEPTH);
	int bot_channel_stride = (int)bot_diff.getStride(ANN_TENSOR_HEIGHT);
	int bot_stride = (int)bot_diff.getStride(ANN_TENSOR_WIDTH);
	int bot_width = (int)bot_diff.getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot_diff.getDim(ANN_TENSOR_HEIGHT);
	int n_outputs = (int)bot_diff.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)bot_diff.getDim(ANN_TENSOR_4THDIM);
	int bot_v_batch_stride = (int)bot_diff_v.getStride(ANN_TENSOR_DEPTH);
	int bot_v_channel_stride = (int)bot_diff_v.getStride(ANN_TENSOR_HEIGHT);
	int bot_v_stride = (int)bot_diff_v.getStride(ANN_TENSOR_WIDTH);
	aDType * bot_ptr = bot_diff.accessTensor(CL_MAP_READ);
	aDType * bot_v_ptr = bot_diff_v.accessTensor(CL_MAP_READ);


	double sqr_accum = 0;
	double max_err = -FLT_MIN;
	int max_b = 0, max_o = 0, max_i = 0, max_j = 0;

	for (int b = 0; b < n_batchs; b++)
	{
		for (int o = 0; o < n_outputs; o++)
		{
			for (int j = 0; j < bot_height; j++)
			{
				for (int i = 0; i < bot_width; i++)
				{
					aDType c_val = bot_v_ptr[b*bot_v_batch_stride + o * bot_v_channel_stride + j * bot_v_stride + i];
					aDType g_val = bot_ptr[b*bot_batch_stride + o * bot_channel_stride + j * bot_stride + i];
					sqr_accum += (c_val - g_val) * (c_val - g_val);
					if (abs(c_val - g_val) > max_err)
					{
						max_err = abs(c_val - g_val);
						max_b = b;
						max_o = o;
						max_i = i;
						max_j = j;
					}

				}
			}
		}
	}

	sqr_accum = sqrt(sqr_accum / ((double)n_batchs *n_outputs * bot_height *bot_width));
	int match = 1;

	if (sqr_accum > 0)
	{
		std::cout << "Error in pooling back-propagation " << getName() + " : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
			" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << std::endl;

		double allowedEps = 4;
		if (sqr_accum > (1. / 1000000000))
		{
			for (int b = 0; b < n_batchs && match; b++)
			{
				for (int o = 0; o < n_outputs && match; o++)
				{
					for (int j = 0; j < bot_height && match; j++)
					{
						for (int i = 0; i < bot_width && match; i++)
						{
							aDType c_val = bot_v_ptr[b*bot_v_batch_stride + o * bot_v_channel_stride + j * bot_v_stride + i];
							aDType g_val = bot_ptr[b*bot_batch_stride + o * bot_channel_stride + j * bot_stride + i];


							double err = CalculateErr(c_val, g_val);
							if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
							{
								std::cout << "Difference in pooling back-propagation " << getName() + " " << err << " is too large at " << b << ", " << o << ", " << i << ", " << j <<
									" c_v = " << std::fixed << std::setw(13) << std::setprecision(11) << c_val <<
									" vs g_val = " << std::fixed << std::setw(13) << std::setprecision(11) << g_val << std::endl;
								match = 0;
							}

						}
					}
				}
			}
		}

	}

	bot_diff.commitTensor();
	bot_diff_v.commitTensor();

	if (match)
	{
		std::cout << "Passed varifier: layer: pooling back-propagation: " << getName() << std::endl;
	}


	return(ret);
}


///////////////////////////////////////////////////////////////////////////////////
//
// Neuron
//
///////////////////////////////////////////////////////////////////////////////////


CDNN_Dnet_layer_neuron :: CDNN_Dnet_layer_neuron()
{

}

CDNN_Dnet_layer_neuron::CDNN_Dnet_layer_neuron(CDNN_OVX * parent, void * wrapper, std::string name, const void * parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Dnet_layer(parent, wrapper, name,	parameters, ref_layer)
{
}

/************************************************************************************************************************
**
**				FORWARD
**
************************************************************************************************************************/

int CDNN_Dnet_layer_neuron :: Execute(void)
{
	int ret = -1;
	CDNN_Tensor<aDType> * bot = &getBotFwd();
	CDNN_Tensor<aDType> * top = &getTopFwd();


	if (bot && top )
	{
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{
			cl_mem bot_mem = bot->getCLMem();
			cl_mem top_mem = top->getCLMem();

			size_t bot_size = bot->getSize();
			size_t top_size = top->getSize();

			size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };

			size_t g_wk[3] = { (bot_size / n_out_pix_horiz_), 1, 1 };

			aDType negative_slope = (aDType)getNeuronNegSlope();
			aDType power = 0;
			aDType scale = 0;
			aDType shift = 0;
			double dpower, dscale, dshift;
			getNeuronPowerArgs(&dpower, &dscale, &dshift);
			power = (aDType)dpower;
			scale = (aDType)dscale;
			shift = (aDType)dshift;

			std::string kernel_name = "aDNNNeuron4";

			cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];

			int n_arg = 0;

			ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &negative_slope);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &power);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &scale);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &shift);

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");
			int iter = getNTimingIter();
			ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
			CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");

		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}

		// verify
#if CNN_VERIFY
		internalVerify();
#endif


		int out_width = (int)top->getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)top->getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)bot->getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)bot->getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)bot->getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)bot->getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)top->getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: neuron: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxWxHxOxB: %dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;



	}

	return(ret);
}

int CDNN_Dnet_layer_neuron :: internalSetup(void)
{
	int ret = 0;

	size_t outputs = 0;
	size_t batch_sz = 0;
	size_t inputs = 0;
	size_t bot_width = 0;
	size_t bot_height = 0;
	size_t top_width = 0;
	size_t top_height = 0;
	size_t size = 0;

	CDNN_Tensor<aDType> * bot = NULL;
	if (bottom_lyr_)
	{
		bot = &bottom_lyr_->getTopFwd();
		bot_width = bot->getDim(ANN_TENSOR_WIDTH);
		bot_height = bot->getDim(ANN_TENSOR_HEIGHT);
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);
		outputs = inputs = bot->getDim(ANN_TENSOR_DEPTH);
		size = bot->getSize();

	}


	if (top_lyr_)
	{
		CDNN_Tensor<aDType> * top = &getTopFwd();
		bot = &top_lyr_->getBotFwd();
		if (top==bot && !top->IsInited())
		{
// pooling width/height

			top->setName(getTopName());
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = bot_width;
			dims[ANN_TENSOR_HEIGHT] = bot_height;
			dims[ANN_TENSOR_DEPTH] = outputs;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);
		}
	}

	bot = &getBotFwd();

	n_out_pix_horiz_ = 1;
	n_out_pix_vert_ = 1;

	if (((size / 4) * 4) == size && size >= 256)
	{
			ocl_group_sz0_ = 256;
			ocl_group_sz1_ = 1;
			n_out_pix_horiz_ = 4;
			n_out_pix_vert_ = 1;

	}
	
	int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);

	ocl_group_lg2sz0_ = (int)ceil(log((double)ocl_group_sz0_)/log(2.));
	ocl_group_lg2sz1_ = (int)ceil(log((double)ocl_group_sz1_)/log(2.));

	std::string comp_options = 
//#define _DNN_POOLING_KERNEL_SZ 3
//#define _DNN_POOLING_N_HORIZ_OUT_PIX 4
	std::string(" -D _DNN_NRN_N_CHANNELS=") + std::to_string((long long)outputs)
	+ std::string(" -D _DNN_NRN_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0_)
	+ std::string(" -D _DNN_NRN_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1_)
	+ std::string(" -D _DNN_NRN_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz_)
	+ std::string(" -D _DNN_NRN_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert_)
	+ std::string(" -D _DNN_NRN_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
	+ std::string(" -D _DNN_NRN_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
	+ std::string(" -D _DNN_NRN_BOT_STRIDE=") + std::to_string((long long)bot_batch_stride)
	+ std::string(" -D _DNN_NRN_OP_ID=") + std::to_string((long long)getNeuronType())
	+ std::string(" -D _ACCEL_GPU=") + std::to_string((long long)_DNN_ACCEL_GPU)
	+ parent_->getGenericCompOptions()
	;
	std::string kernel_file = "aDNNNeuron.cl";

	{
		std::string kernel_name = "aDNNNeuron4";

		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;
	}

	{
		std::string kernel_name = "aDNNNeuron4_Bwd";

		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;
	}

	return(ret);
}

int CDNN_Dnet_layer_neuron::ExecuteHost(void)
{
	int ret = 0;


	return(ret);
}

int CDNN_Dnet_layer_neuron::internalVerify(void )
{
	int ret = 0;

	ret = ExecuteHost();

	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top = getTopFwd();

	aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	size_t size = bot.getSize();
	AF activ_func = NULL;
	AF activ_func_list[dnet_neuron_COUNT] =
	{
		NULL,
		ActivationFunction_ReLU,
		ActivationFunction_Sigmoid,
		ActivationFunction_TanH,
		ActivationFunction_Abs,
		ActivationFunction_Power,
		ActivationFunction_BNLL

	};

	aDType negative_slope = (aDType)getNeuronNegSlope();
	aDType power = 0;
	aDType scale = 0;
	aDType shift = 0;
	double dpower, dscale, dshift;
	getNeuronPowerArgs(&dpower, &dscale, &dshift);
	power = (aDType)dpower;
	scale = (aDType)dscale;
	shift = (aDType)dshift;

	activ_func = activ_func_list[getNeuronType()];

	if (activ_func && bot_ptr && top_ptr)
	{
		int match = 1;
		const double allowedEps = 3;

		for (size_t i = 0; i < size / 4 && match; i++)
		{
			aDType c_res[4];
			const aDType * data = &bot_ptr[i * 4];
			activ_func((aDType*)c_res, data, negative_slope, power, scale, shift);
			const aDType * g_res = &top_ptr[i * 4];
			for (int k = 0; k < 4; k++)
			{
				aDType c_val = c_res[k];
				aDType g_val = g_res[k];
				double err = CalculateErr(c_val, g_val);

				if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
				{
					std::cout << "Difference in neuron layer: " << getName()  + " " << err << " too large at " << i * 4 + k << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}

		}

		if (match)
		{
			std::cout << "Passed varifier: layer: neuron: " << getName() << std::endl;
		}

	}

	top.commitTensor();
	bot.commitTensor();

	return(ret);
}



/************************************************************************************************************************
**
**				BACKWARD
**
************************************************************************************************************************/


int CDNN_Dnet_layer_neuron::internalSetupBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{
		makeBwdDiff();
	}

	return(ret);
}


int CDNN_Dnet_layer_neuron::ExecuteBwd(void)
{
	int ret = -1;
	if (doNeedBackProp())
	{

		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top = getTopFwd();
		CDNN_Tensor<aDType> & bot_diff = getBotDiff();
		CDNN_Tensor<aDType> & top_diff = getTopDiff();


		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{
			cl_mem bot_mem = bot.getCLMem();
			cl_mem top_mem = top.getCLMem();
			cl_mem bot_diff_mem = bot_diff.getCLMem();
			cl_mem top_diff_mem = top_diff.getCLMem();

			size_t bot_size = bot_diff.getSize();

			size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };

			size_t g_wk[3] = { (bot_size / n_out_pix_horiz_), 1, 1 };

			aDType negative_slope = (aDType)getNeuronNegSlope();
			aDType power = 0;
			aDType scale = 0;
			aDType shift = 0;
			double dpower, dscale, dshift;
			getNeuronPowerArgs(&dpower, &dscale, &dshift);
			power = (aDType)dpower;
			scale = (aDType)dscale;
			shift = (aDType)dshift;
			aDType diff_scale = scale * power;

			std::string kernel_name = "aDNNNeuron4_Bwd";

			cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];

			int n_arg = 0;

			ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_diff_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_diff_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &diff_scale);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &negative_slope);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &power);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &scale);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &shift);

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");
			int iter = getNTimingIter();
			ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
			CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");

		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}

		// verify
#if CNN_VERIFY
		internalVerifyBwd();
#endif


		int out_width = (int)bot_diff.getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)bot_diff.getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)top_diff.getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)top_diff.getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)top_diff.getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)bot_diff.getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)bot_diff.getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: neuron back-propagation: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxWxHxOxB: %dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;





//		printf("Passed layer: neuron back-propogation: \"%s\"\n", getName().c_str());
	}

	return(ret);
}

int CDNN_Dnet_layer_neuron::ExecuteBwdHost(void)
{
	int ret = 0;

	return(ret);
}

int CDNN_Dnet_layer_neuron::internalVerifyBwd(void)
{
	int ret = 0;

	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top = getTopFwd();
	CDNN_Tensor<aDType> & bot_df = getBotDiff();
	CDNN_Tensor<aDType> & top_df = getTopDiff();

	aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	aDType * bot_df_ptr = bot_df.accessTensor(CL_MAP_READ);
	aDType * top_df_ptr = top_df.accessTensor(CL_MAP_READ);

	size_t size = bot.getSize();
	aDType negative_slope = (aDType)getNeuronNegSlope();
	aDType power = 0;
	aDType scale = 0;
	aDType shift = 0;
	double dpower, dscale, dshift;
	getNeuronPowerArgs(&dpower, &dscale, &dshift);
	power = (aDType)dpower;
	scale = (aDType)dscale;
	shift = (aDType)dshift;

	int neuron_type = getNeuronType();

	int match = 1;
	if (neuron_type == _DNN_NRN_OP_ID_ReLU)
	{
		const double allowedEps = 3;

		for (size_t i = 0; i < size / 4 && match; i++)
		{
			aDType bot_df_v_p[4];
			ActivationFunction_ReLU_Diff(bot_df_v_p, &top_df_ptr[i * 4], &bot_ptr[i * 4], negative_slope);
			const aDType * bot_df_p = &bot_df_ptr[i * 4];
			for (int k = 0; k < 4; k++)
			{
				aDType c_val = bot_df_v_p[k];
				aDType g_val = bot_df_p[k];
				double err = CalculateErr(c_val, g_val);

				if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
				{
					std::cout << "Difference in neuron back-propagation: " << getName() + " " << err << " too large at " << i * 4 + k << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
					match = 0;
				}
			}



		}
	}
	else if (neuron_type == _DNN_NRN_OP_ID_Sigmoid)
	{
		// 1/(1 + exp(-x))  
		printf("Neuron: ERROR: bwd func %d has not been implemented yet\n", neuron_type);
	}
	else if (neuron_type == _DNN_NRN_OP_ID_TanH)
	{
		// (exp(2x) -1) / (exp(2x) + 1)
		printf("Neuron: ERROR: bwd func %d has not been implemented yet\n", neuron_type);
	}
	else if (neuron_type == _DNN_NRN_OP_ID_AbsVal)
	{
		printf("Neuron: ERROR: bwd func %d has not been implemented yet\n", neuron_type);
	}
	else if (neuron_type == _DNN_NRN_OP_ID_Power)
	{
		// (shift + scale * x ) ^power
		printf("Neuron: ERROR: bwd func %d has not been implemented yet\n", neuron_type);
	}
	else if (neuron_type == _DNN_NRN_OP_ID_BNLL)
	{
		//	log(1 + exp(x))
		printf("Neuron: ERROR: bwd func %d has not been implemented yet\n", neuron_type);
	}
	else
	{
		printf("Neuron: ERROR: unknown bwd func %d\n", neuron_type);
	}

	bot.commitTensor();
	top.commitTensor();
	bot_df.commitTensor();
	top_df.commitTensor();

	if (match)
	{
		std::cout << "Passed varifier: layer: neuron back-propagation: " << getName() << std::endl;
	}


	return(ret);
}


///////////////////////////////////////////////////////////////////////////////////
//
// LRN
//
///////////////////////////////////////////////////////////////////////////////////


CDNN_Dnet_layer_LRN :: CDNN_Dnet_layer_LRN()
{
}

CDNN_Dnet_layer_LRN::CDNN_Dnet_layer_LRN(CDNN_OVX * parent, void * wrapper, std::string name, const void * parameters, CDNN_Dnet_layer * ref_layer) :
			CDNN_Dnet_layer(parent, wrapper, name,	parameters, ref_layer)
{
}


/************************************************************************************************************************
**
**				FORWARD
**
************************************************************************************************************************/


int CDNN_Dnet_layer_LRN :: Execute(void)
{
	int ret = -1;
	CDNN_Tensor<aDType> * bot = (CDNN_Tensor<aDType> * )&getBotFwd();
	CDNN_Tensor<aDType> * top = (CDNN_Tensor<aDType> * )&getTopFwd();
	CDNN_Tensor<aDType> * scale = (CDNN_Tensor<aDType> *)findInternal(std::string("scale"));


	if (bot && top && scale)
	{
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{

			cl_mem bot_mem = bot->getCLMem();
			cl_mem top_mem = top->getCLMem();
			cl_mem scale_mem = scale->getCLMem();
			int local_area = getLocalArea();
			aDType alpha = (aDType)getAlphaLRN();
// whithin channel alphaoverarea is going to be culculate based on actual areal size (cut by borders).
			aDType alphaoverarea = (aDType)((getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS) ? alpha / local_area : alpha / (local_area * local_area));
			aDType beta = (aDType)getBetaLRN();
			
			std::string kernel_name;


			size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };
			size_t g_wk[3] ;
			if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
			{
				g_wk[0] = top->getDim(ANN_TENSOR_WIDTH);
				g_wk[1] = top->getDim(ANN_TENSOR_HEIGHT);
				g_wk[2] = top->getDim(ANN_TENSOR_4THDIM);
				kernel_name = "aDNNLRNAcrossChannels1";
			}
			else
			{ 
				int g_wk_width = (int)((top->getDim(ANN_TENSOR_WIDTH) + ocl_group_sz0_ * n_out_pix_horiz_ - 1) / (ocl_group_sz0_ * n_out_pix_horiz_));
				int g_wk_height = (int)((top->getDim(ANN_TENSOR_HEIGHT) + ocl_group_sz1_ * n_out_pix_vert_ - 1) / (ocl_group_sz1_ * n_out_pix_vert_));

				g_wk[0] = g_wk_width * ocl_group_sz0_;
				g_wk[1] = g_wk_height * ocl_group_sz1_;
				g_wk[2] = top->getDim(ANN_TENSOR_DEPTH) * top->getDim(ANN_TENSOR_4THDIM);
				kernel_name = "aDNNLRNWithinChannel";

			}


			cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];


			int n_arg = 0;

			ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &scale_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &alphaoverarea);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &alpha);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &beta);

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");
			int iter = getNTimingIter();
			ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
			CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");
		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


		// verify
#if CNN_VERIFY
		internalVerify();
#endif

		int out_width = (int)top->getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)top->getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)bot->getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)bot->getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)bot->getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)bot->getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)top->getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: LRN: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxWxHxOxB: %dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;

	}

	return(ret);
}

int CDNN_Dnet_layer_LRN :: internalSetup(void)
{
	int ret = 0;

	size_t outputs = 0;
	size_t batch_sz = 0;
	size_t inputs = 0;
	size_t bot_width = 0;
	size_t bot_height = 0;
	size_t top_width = 0;
	size_t top_height = 0;
	CDNN_Tensor<aDType> * bot = NULL;
	CDNN_Tensor<aDType> * top = NULL;

	if (bottom_lyr_)
	{
		bot = (CDNN_Tensor<aDType> *)&bottom_lyr_->getTopFwd();
		bot_width = bot->getDim(ANN_TENSOR_WIDTH);
		bot_height = bot->getDim(ANN_TENSOR_HEIGHT);
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);
		outputs = inputs = bot->getDim(ANN_TENSOR_DEPTH);

	}


	if (top_lyr_)
	{
		top = (CDNN_Tensor<aDType> *)&getTopFwd();
		CDNN_Tensor<aDType> * bot = (CDNN_Tensor<aDType> *)&top_lyr_->getBotFwd();
		if (top==bot && !top->IsInited())
		{
// pooling width/height

			top->setName(getTopName());
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = bot_width;
			dims[ANN_TENSOR_HEIGHT] = bot_height;
			dims[ANN_TENSOR_DEPTH] = outputs;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);
		}
	}
	{
		CDNN_Tensor<aDType> * scale = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("scale"));
		CDNN_Tensor<aDType> & top = getTopFwd();
		assert(scale);
		scale->initTensor(top.getNDim(), top.getDims());
		addInternal(scale->getName(), scale);
		scale->allocTensor();
	}

#if CNN_VERIFY
	{
		// add system only for verification
		CDNN_Tensor<aDType> * top_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.verify"));
		CDNN_Tensor<aDType> & top = getTopFwd();
		assert(top_verify);
		top_verify->initTensor(top.getNDim(), top.getDims());
		addInternal(top_verify->getName(), top_verify);
		top_verify->allocTensor(_CBUF_MEM_SYS_ONLY);


		CDNN_Tensor<aDType> * scale_v = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("scale.verify"));
		assert(scale_v);
		scale_v->initTensor(top.getNDim(), top.getDims());
		addInternal(scale_v->getName(), scale_v);
		scale_v->allocTensor(_CBUF_MEM_SYS_ONLY);

	}
#endif

	int local_area = getLocalArea();
	int pre_pad = (local_area - 1) / 2;
	int pad = local_area - pre_pad - 1;
//	aDType alphaoverarea = (aDType)((getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS) ? getAlphaLRN() / local_area : getAlphaLRN() / (local_area * local_area));
//	aDType beta = (aDType)getBetaLRN();

	CDNN_Tensor<aDType> * scale = (CDNN_Tensor<aDType> *)findInternal(std::string("scale"));

	outputs = top->getDim(ANN_TENSOR_DEPTH);
	inputs = bot->getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top->getDim(ANN_TENSOR_4THDIM);
	bot_width = (int)bot->getDim(ANN_TENSOR_WIDTH);
	bot_height = (int)bot->getDim(ANN_TENSOR_HEIGHT);
	top_width = (int)top->getDim(ANN_TENSOR_WIDTH);
	top_height = (int)top->getDim(ANN_TENSOR_HEIGHT);
	int bot_stride = (int)bot->getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot->getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top->getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top->getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top->getStride(ANN_TENSOR_DEPTH);

	int scale_stride = (int)scale->getStride(ANN_TENSOR_WIDTH);
	int scale_channel_stride = (int)scale->getStride(ANN_TENSOR_HEIGHT);
	int	scale_batch_stride = (int)scale->getStride(ANN_TENSOR_DEPTH);

	int top_df_stride = 1;
	int top_df_channel_stride = 1;
	int	top_df_batch_stride = 1;

	int bot_df_stride = 1;
	int bot_df_channel_stride = 1;
	int	bot_df_batch_stride = 1;


	if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
	{
		n_out_pix_horiz_ = 1;
		n_out_pix_vert_ = 1;
		ocl_group_sz0_ = (top_width <= 8) ? 8 : 16;
		ocl_group_sz1_ = (top_height <= 8) ? 8 : 16;

	}
	else
	{
		ocl_group_sz0_ = 8;
		ocl_group_sz1_ = 8;

		n_out_pix_horiz_ = (top_width <= 8) ? 1 : (top_width <= 16) ? 2 : 4;
		n_out_pix_vert_ = (top_height <= 8) ? 1 : (top_height <= 16) ? 2 : 4;;
	}
	ocl_group_lg2sz0_ = (int)ceil(log((double)ocl_group_sz0_) / log(2.));
	ocl_group_lg2sz1_ = (int)ceil(log((double)ocl_group_sz1_) / log(2.));


	std::string comp_options = 
	std::string(" -D _DNN_LRN_KERNEL_SZ=") + std::to_string((long long)local_area)
	+ std::string(" -D _DNN_LRN_N_OUTPUTS=") + std::to_string((long long)outputs)
	+ std::string(" -D _DNN_LRN_N_CHANNELS=") + std::to_string((long long)inputs)
	+ std::string(" -D _DNN_LRN_PAD=") + std::to_string((long long)pad)
	+ std::string(" -D _DNN_LRN_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz_)
	+ std::string(" -D _DNN_LRN_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert_)
	+ std::string(" -D _DNN_LRN_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0_)
	+ std::string(" -D _DNN_LRN_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1_)
	+ std::string(" -D _DNN_LRN_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0_)
	+ std::string(" -D _DNN_LRN_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1_)
	+ std::string(" -D _DNN_LRN_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
	+ std::string(" -D _DNN_LRN_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
	+ std::string(" -D _DNN_LRN_BOT_STRIDE=") + std::to_string((long long)bot_stride)
	+ std::string(" -D _DNN_LRN_TOP_BATCH_STRIDE=") + std::to_string((long long)top_batch_stride)
	+ std::string(" -D _DNN_LRN_TOP_CHANNEL_STRIDE=") + std::to_string((long long)top_channel_stride)
	+ std::string(" -D _DNN_LRN_TOP_STRIDE=") + std::to_string((long long)top_stride)
	+ std::string(" -D _DNN_LRN_BOT_WIDTH=") + std::to_string((long long)bot_width)
	+ std::string(" -D _DNN_LRN_BOT_HEIGHT=") + std::to_string((long long)bot_height)
	+ std::string(" -D _DNN_LRN_TOP_WIDTH=") + std::to_string((long long)top_width)
	+ std::string(" -D _DNN_LRN_TOP_HEIGHT=") + std::to_string((long long)top_height)
	+ std::string(" -D _DNN_LRN_SCALE_BATCH_STRIDE=") + std::to_string((long long)scale_batch_stride)
	+ std::string(" -D _DNN_LRN_SCALE_CHANNEL_STRIDE=") + std::to_string((long long)scale_channel_stride)
	+ std::string(" -D _DNN_LRN_SCALE_STRIDE=") + std::to_string((long long)scale_stride)
	+ std::string(" -D _DNN_LRN_TOPDF_BATCH_STRIDE=") + std::to_string((long long)top_df_batch_stride)
	+ std::string(" -D _DNN_LRN_TOPDF_CHANNEL_STRIDE=") + std::to_string((long long)top_df_channel_stride)
	+ std::string(" -D _DNN_LRN_TOPDF_STRIDE=") + std::to_string((long long)top_df_stride)
	+ std::string(" -D _DNN_LRN_BOTDF_BATCH_STRIDE=") + std::to_string((long long)bot_df_batch_stride)
	+ std::string(" -D _DNN_LRN_BOTDF_CHANNEL_STRIDE=") + std::to_string((long long)bot_df_channel_stride)
	+ std::string(" -D _DNN_LRN_BOTDF_STRIDE=") + std::to_string((long long)bot_df_stride)
	+ std::string(" -D _DNN_LRN_BATCH_SZ=") + std::to_string((long long)n_batchs)



	+ parent_->getGenericCompOptions()
	;
	std::string kernel_file = "aDNNLRN.cl";
	std::string kernel_name = (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS) ? "aDNNLRNAcrossChannels1" : "aDNNLRNWithinChannel";;

	cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

	cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;

	return(ret);
}


int CDNN_Dnet_layer_LRN::ExecuteHost(void)
{
	int ret = 0;

	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & scale_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("scale.verify"));
	CDNN_Tensor<aDType> & top_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	CDNN_Tensor<aDType> & top = getTopFwd();


	aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	aDType * scale_v_ptr = scale_v.accessTensor(CL_MAP_WRITE);
	aDType * top_v_ptr = top_v.accessTensor(CL_MAP_WRITE);

	int local_area = getLocalArea();
	int pre_pad = (local_area - 1) / 2;
	int pad = local_area - pre_pad - 1;

	aDType alphaoverarea = (aDType)((getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS) ? getAlphaLRN() / local_area : getAlphaLRN() / (local_area * local_area));
	aDType beta = (aDType)getBetaLRN();

	int n_outputs = (int)top.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top.getDim(ANN_TENSOR_4THDIM);
	int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);
	int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);
	int n_inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
	int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);

	int scale_v_stride = (int)scale_v.getStride(ANN_TENSOR_WIDTH);
	int scale_v_channel_stride = (int)scale_v.getStride(ANN_TENSOR_HEIGHT);
	int scale_v_batch_stride = (int)scale_v.getStride(ANN_TENSOR_DEPTH);

	int top_v_stride = (int)top_v.getStride(ANN_TENSOR_WIDTH);
	int top_v_channel_stride = (int)top_v.getStride(ANN_TENSOR_HEIGHT);
	int top_v_batch_stride = (int)top_v.getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);


	if (bot_ptr && top_ptr && scale_v_ptr && top_v_ptr)
	{
	
		if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
		{
	

			for (int b = 0; b < n_batchs; b++)
			{
				for (int j = 0; j < top_height; j++)
				{
					for (int i = 0; i < top_width; i++)
					{
						// c-emulator
						aDType res = 0;
						aDType accum_scale = 0;
						int head = 0;
						aDType bot_val;
						while (head < pad) {
							bot_val = bot_ptr[b*bot_batch_stride + head * bot_channel_stride + j * bot_stride + i];
							accum_scale += bot_val  * bot_val;
							++head;
						}
						// until we reach size, nothing needs to be subtracted
						while (head < local_area) {
							bot_val = bot_ptr[b*bot_batch_stride + head * bot_channel_stride + j * bot_stride + i];
							accum_scale += bot_val  * bot_val;
							aDType scale = (aDType)1. + accum_scale * alphaoverarea;
							scale_v_ptr[b*scale_v_batch_stride + (head - pad) * scale_v_channel_stride + j * scale_v_stride + i] = scale;
							bot_val = bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i];
							aDType s = pow(scale, -beta);
							aDType c_val = bot_val * s;
							top_v_ptr[b*top_v_batch_stride + (head - pad) * top_v_channel_stride + j * top_v_stride + i] = c_val;
							++head;
						}
						// both add and subtract
						while (head < n_inputs) {
							bot_val = bot_ptr[b*bot_batch_stride + head * bot_channel_stride + j * bot_stride + i];
							accum_scale += bot_val  * bot_val;
							bot_val = bot_ptr[b*bot_batch_stride + (head - local_area) * bot_channel_stride + j * bot_stride + i];
							accum_scale -= bot_val  * bot_val;
							aDType scale = (aDType)1. + accum_scale * alphaoverarea;
							scale_v_ptr[b*scale_v_batch_stride + (head - pad) * scale_v_channel_stride + j * scale_v_stride + i] = scale;
							aDType s = pow(scale, -beta);
							bot_val = bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i];
							aDType c_val = bot_val * s;
							top_v_ptr[b*top_v_batch_stride + (head - pad) * top_v_channel_stride + j * top_v_stride + i] = c_val;
							++head;
						}
						// subtract only
						while (head < n_inputs + pad) {
							bot_val = bot_ptr[b*bot_batch_stride + (head - local_area) * bot_channel_stride + j * bot_stride + i];
							accum_scale -= bot_val  * bot_val;
							aDType scale = (aDType)1. + accum_scale * alphaoverarea;
							scale_v_ptr[b*scale_v_batch_stride + (head - pad) * scale_v_channel_stride + j * scale_v_stride + i] = scale;
							bot_val = bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i];
							aDType s = pow(scale, -beta);
							aDType c_val = bot_val * s;
							top_v_ptr[b*top_v_batch_stride + (head - pad) * top_v_channel_stride + j * top_v_stride + i] = c_val;
							++head;
						}

					}
				}
			}
		}
		else
		{
	

			for (int b = 0; b < n_batchs; b++)
			{
				for (int o = 0; o < n_outputs; o++)
				{
					for (int j = 0; j < top_height; j++)
					{
						for (int i = 0; i < top_width; i++)
						{
							// c-emulator
							aDType scale = 0;
							int hstart = j - pad;
							int wstart = i - pad;
							int hend = min(hstart + local_area, bot_height + pad);
							int wend = min(wstart + local_area, bot_width + pad);
							int adj_area_size = (hend - hstart) * (wend - wstart);
							hstart = max(hstart, 0);
							wstart = max(wstart, 0);
							hend = min(hend, bot_height);
							wend = min(wend, bot_width);
							aDType accum = 0;
							for (int h = hstart; h < hend; ++h)
							{
								for (int w = wstart; w < wend; ++w)
								{
#if 0
									if (b == 0 && o == 0 && j == 0 && i == 0)
									{

										printf("c:%d %d   %f %f\n", i, j, res, bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w]);
									}
#endif
									aDType bot_val = bot_ptr[b*bot_batch_stride + o * bot_channel_stride + h * bot_stride + w];
									accum += bot_val * bot_val;

								}
							}

							alphaoverarea = (aDType)getAlphaLRN() / adj_area_size;
							scale = (aDType)1. + accum* alphaoverarea;

							aDType s = pow(scale, -beta);
							aDType bot_val = bot_ptr[b*bot_batch_stride + o * bot_channel_stride + j * bot_stride + i];
							aDType c_val = bot_val * s;
#if 0
							if (i == 9 && j == 4 && o == 0)
							{
								printf("C:lrn: %13.11f %13.11f %13.11f %13.11f\n", c_val, bot_val, s, scale);
							}
#endif

							top_v_ptr[b*top_batch_stride + o * top_channel_stride + j * top_stride + i] = c_val;

						}
					}
				}
			} // (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)

		}



	}

	top.commitTensor();
	bot.commitTensor();
	scale_v.commitTensor();
	top_v.commitTensor();
	return(ret);
}


int CDNN_Dnet_layer_LRN::internalVerify(void)
{
	int ret = 0;
	ret = ExecuteHost();


	CDNN_Tensor<aDType> & top_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	CDNN_Tensor<aDType> & top = getTopFwd();

	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	aDType * top_v_ptr = top_v.accessTensor(CL_MAP_READ);

	int n_outputs = (int)top.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top.getDim(ANN_TENSOR_4THDIM);

	int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);



	int top_v_stride = (int)top_v.getStride(ANN_TENSOR_WIDTH);
	int top_v_channel_stride = (int)top_v.getStride(ANN_TENSOR_HEIGHT);
	int top_v_batch_stride = (int)top_v.getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);

	double sqr_accum = 0;
	double max_err = -FLT_MIN;
	int max_b = 0, max_o = 0, max_i = 0, max_j = 0;

	for (int b = 0; b < n_batchs; b++)
	{
		for (int o = 0; o < n_outputs; o++)
		{
			for (int j = 0; j < top_height; j++)
			{
				for (int i = 0; i < top_width; i++)
				{

					aDType c_val = top_v_ptr[b*top_v_batch_stride + o * top_v_channel_stride + j * top_v_stride + i];
					aDType g_val = top_ptr[b*top_batch_stride + o * top_channel_stride + j * top_stride + i];
					sqr_accum += (c_val - g_val) * (c_val - g_val);
					if (abs(c_val - g_val) > max_err)
					{
						max_err = abs(c_val - g_val);
						max_b = b;
						max_o = o;
						max_i = i;
						max_j = j;
					}

				}
			}
		}
	}

	sqr_accum = sqrt(sqr_accum / ((double)n_batchs *n_outputs * top_height *top_width));

	int match = 1;

	if (sqr_accum > 0)
	{
		std::cout << "Error in LRN : " << getName() + " : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
			" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << " at " << max_b << ", " << max_o << ", " << max_i << ", " << max_j << std::endl;

		if (sqr_accum > 1. / 1000000000)
		{

			double allowedEps = 4;

			for (int b = 0; b < n_batchs && match; b++)
			{
				for (int o = 0; o < n_outputs && match; o++)
				{
					for (int j = 0; j < top_height && match; j++)
					{
						for (int i = 0; i < top_width && match; i++)
						{

							aDType c_val = top_v_ptr[b*top_v_batch_stride + o * top_v_channel_stride + j * top_v_stride + i];
							aDType g_val = top_ptr[b*top_batch_stride + o * top_channel_stride + j * top_stride + i];
							double err = CalculateErr(c_val, g_val);
							if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
							{
								std::cout << "Difference in LRN : " << getName() + " " << err << " too large at " << b << ", " << o << ", " << i << ", " << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
								match = 0;
							}



						}
					}
				}
			}

		}
	}
	if (match)
	{
		std::cout << "Passed varifier: layer: LRN: " << getName() << std::endl;
	}

	

	top.commitTensor();

	top_v.commitTensor();



	return(ret);
}


/************************************************************************************************************************
**
**				BACKWARD
**
************************************************************************************************************************/


int CDNN_Dnet_layer_LRN::internalSetupBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{
		makeBwdDiff();

#if CNN_VERIFY
		{
			// add system only for verification
			CDNN_Tensor<aDType> * bot_df_v = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("bot.diff.verify"));
			CDNN_Tensor<aDType> & bot_df = getBotDiff();
			assert(bot_df_v);
			bot_df_v->initTensor(bot_df.getNDim(), bot_df.getDims());
			addInternal(bot_df_v->getName(), bot_df_v);
			bot_df_v->allocTensor(_CBUF_MEM_SYS_ONLY);

// TEMP TEST FIIL TOP DIFF with something

			CDNN_Tensor<aDType> & top_df = getTopDiff();
			aDType * top_df_ptr = top_df.accessTensor(CL_MAP_WRITE);
			size_t sz = top_df.getSize();
			for (size_t i = 0; i < sz; i++)
			{
				top_df_ptr[i] = (aDType)((2.f * (aDType)rand() / (aDType)RAND_MAX - 1.0f));
			}
			top_df.commitTensor();

		}
#endif

		int local_area = getLocalArea();
		int pre_pad = local_area - (local_area + 1) / 2;
		int pad = local_area - pre_pad - 1;

		CDNN_Tensor<aDType> & scale = *(CDNN_Tensor<aDType> *)findInternal(std::string("scale"));
		CDNN_Tensor<aDType> & top = getTopFwd();
		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top_df = getTopDiff();
		CDNN_Tensor<aDType> & bot_df = getBotDiff();

		int outputs = (int)top.getDim(ANN_TENSOR_DEPTH);
		int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
		int n_batchs = (int)top.getDim(ANN_TENSOR_4THDIM);
		int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
		int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);
		int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
		int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);
		int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
		int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
		int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);

		int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);
		int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
		int	top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);

		int scale_stride = (int)scale.getStride(ANN_TENSOR_WIDTH);
		int scale_channel_stride = (int)scale.getStride(ANN_TENSOR_HEIGHT);
		int	scale_batch_stride = (int)scale.getStride(ANN_TENSOR_DEPTH);

		int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);
		int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
		int	top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);

		int bot_df_width = (int)bot_df.getDim(ANN_TENSOR_WIDTH);
		int bot_df_height = (int)bot_df.getDim(ANN_TENSOR_HEIGHT);
		int bot_df_stride = (int)bot_df.getStride(ANN_TENSOR_WIDTH);
		int bot_df_channel_stride = (int)bot_df.getStride(ANN_TENSOR_HEIGHT);
		int	bot_df_batch_stride = (int)bot_df.getStride(ANN_TENSOR_DEPTH);

		aDType alpha = (aDType)getAlphaLRN();
		aDType beta = (aDType)getBetaLRN();

		ratio_dta_bwd_ = (aDType) 2. * alpha * beta / local_area;

		if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
		{
			n_out_pix_horiz_ = 1;
			n_out_pix_vert_ = 1;
			ocl_group_sz0_ = (bot_df_width <= 8) ? 8 : 16;
			ocl_group_sz1_ = (bot_df_height <= 8) ? 8 : 16;

		}
		else
		{
			ocl_group_sz0_ = 8;
			ocl_group_sz1_ = 8;

			n_out_pix_horiz_ = (bot_df_width <= 8) ? 1 : (bot_df_width <= 16) ? 2 : 4;
			n_out_pix_vert_ = (bot_df_height <= 8) ? 1 : (bot_df_height <= 16) ? 2 : 4;;
		}
		ocl_group_lg2sz0_ = (int)ceil(log((double)ocl_group_sz0_) / log(2.));
		ocl_group_lg2sz1_ = (int)ceil(log((double)ocl_group_sz1_) / log(2.));


		std::string comp_options =
			std::string(" -D _DNN_LRN_KERNEL_SZ=") + std::to_string((long long)local_area)
			+ std::string(" -D _DNN_LRN_N_OUTPUTS=") + std::to_string((long long)outputs)
			+ std::string(" -D _DNN_LRN_N_CHANNELS=") + std::to_string((long long)inputs)
			+ std::string(" -D _DNN_LRN_PAD=") + std::to_string((long long)pad)
			+ std::string(" -D _DNN_LRN_N_HORIZ_OUT_PIX=") + std::to_string((long long)n_out_pix_horiz_)
			+ std::string(" -D _DNN_LRN_N_VERT_OUT_PIX=") + std::to_string((long long)n_out_pix_vert_)
			+ std::string(" -D _DNN_LRN_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0_)
			+ std::string(" -D _DNN_LRN_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1_)
			+ std::string(" -D _DNN_LRN_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0_)
			+ std::string(" -D _DNN_LRN_GROUP_LG2SZ1=") + std::to_string((long long)ocl_group_lg2sz1_)
			+ std::string(" -D _DNN_LRN_BOT_BATCH_STRIDE=") + std::to_string((long long)bot_batch_stride)
			+ std::string(" -D _DNN_LRN_BOT_CHANNEL_STRIDE=") + std::to_string((long long)bot_channel_stride)
			+ std::string(" -D _DNN_LRN_BOT_STRIDE=") + std::to_string((long long)bot_stride)
			+ std::string(" -D _DNN_LRN_TOP_BATCH_STRIDE=") + std::to_string((long long)top_batch_stride)
			+ std::string(" -D _DNN_LRN_TOP_CHANNEL_STRIDE=") + std::to_string((long long)top_channel_stride)
			+ std::string(" -D _DNN_LRN_TOP_STRIDE=") + std::to_string((long long)top_stride)
			+ std::string(" -D _DNN_LRN_BOT_WIDTH=") + std::to_string((long long)bot_width)
			+ std::string(" -D _DNN_LRN_BOT_HEIGHT=") + std::to_string((long long)bot_height)
			+ std::string(" -D _DNN_LRN_TOP_WIDTH=") + std::to_string((long long)top_width)
			+ std::string(" -D _DNN_LRN_TOP_HEIGHT=") + std::to_string((long long)top_height)
			+ std::string(" -D _DNN_LRN_SCALE_BATCH_STRIDE=") + std::to_string((long long)scale_batch_stride)
			+ std::string(" -D _DNN_LRN_SCALE_CHANNEL_STRIDE=") + std::to_string((long long)scale_channel_stride)
			+ std::string(" -D _DNN_LRN_SCALE_STRIDE=") + std::to_string((long long)scale_stride)
			+ std::string(" -D _DNN_LRN_TOPDF_BATCH_STRIDE=") + std::to_string((long long)top_df_batch_stride)
			+ std::string(" -D _DNN_LRN_TOPDF_CHANNEL_STRIDE=") + std::to_string((long long)top_df_channel_stride)
			+ std::string(" -D _DNN_LRN_TOPDF_STRIDE=") + std::to_string((long long)top_df_stride)
			+ std::string(" -D _DNN_LRN_BOTDF_BATCH_STRIDE=") + std::to_string((long long)bot_df_batch_stride)
			+ std::string(" -D _DNN_LRN_BOTDF_CHANNEL_STRIDE=") + std::to_string((long long)bot_df_channel_stride)
			+ std::string(" -D _DNN_LRN_BOTDF_STRIDE=") + std::to_string((long long)bot_df_stride)
			+ std::string(" -D _DNN_LRN_BATCH_SZ=") + std::to_string((long long)n_batchs)



			+parent_->getGenericCompOptions()
			;
		std::string kernel_file = "aDNNLRN.cl";
		std::string kernel_name = (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS) ? "aDNNLRNAcrossChannelsBwd1" : "aDNNLRNWithinChannelBwd";;

		cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

		cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;

	}

	return(ret);
}


int CDNN_Dnet_layer_LRN::ExecuteBwd(void)
{
	int ret = 0;
	if (doNeedBackProp())
	{

		CDNN_Tensor<aDType> & bot = getBotFwd();
		CDNN_Tensor<aDType> & top = getTopFwd();
		CDNN_Tensor<aDType> & scale = *(CDNN_Tensor<aDType> *)findInternal(std::string("scale"));
		CDNN_Tensor<aDType> & bot_df = getBotDiff();
		CDNN_Tensor<aDType> & top_df = getTopDiff();


		cl_command_queue convQ = getaDNNOCL().getClQueue(0);

		int iter = getNTimingIter();
		double s = 0, e = 0;
		aDType alpha = (aDType)getAlphaLRN();
		aDType beta = (aDType)getBetaLRN();

		cl_mem bot_mem = bot.getCLMem();
		cl_mem top_mem = top.getCLMem();
		cl_mem scale_mem = scale.getCLMem();
		cl_mem bot_df_mem = bot_df.getCLMem();
		cl_mem top_df_mem = top_df.getCLMem();


		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{


			std::string kernel_name;


			size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };
			size_t g_wk[3];
			if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
			{
				g_wk[0] = bot_df.getDim(ANN_TENSOR_WIDTH);
				g_wk[1] = bot_df.getDim(ANN_TENSOR_HEIGHT);
				g_wk[2] = bot_df.getDim(ANN_TENSOR_4THDIM);
				kernel_name = "aDNNLRNAcrossChannelsBwd1";
			}
			else
			{
				int g_wk_width = (int)((bot_df.getDim(ANN_TENSOR_WIDTH) + ocl_group_sz0_ * n_out_pix_horiz_ - 1) / (ocl_group_sz0_ * n_out_pix_horiz_));
				int g_wk_height = (int)((bot_df.getDim(ANN_TENSOR_HEIGHT) + ocl_group_sz1_ * n_out_pix_vert_ - 1) / (ocl_group_sz1_ * n_out_pix_vert_));

				g_wk[0] = g_wk_width * ocl_group_sz0_;
				g_wk[1] = g_wk_height * ocl_group_sz1_;
				g_wk[2] = bot_df.getDim(ANN_TENSOR_DEPTH) * bot_df.getDim(ANN_TENSOR_4THDIM);
				kernel_name = "aDNNLRNWithinChannelBwd";

			}


			cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];


			int n_arg = 0;

			ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &top_df_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &scale_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &bot_df_mem);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &ratio_dta_bwd_);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &alpha);
			ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(aDType), &beta);

			CHECK_OPENCL_ERROR(ret, "parmeters failed.");
			int iter = getNTimingIter();
			ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);
			CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");
		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


		// verify
#if CNN_VERIFY
		internalVerifyBwd();
#endif

		int out_width = (int)bot_df.getDim(ANN_TENSOR_WIDTH);
		int out_height = (int)bot_df.getDim(ANN_TENSOR_HEIGHT);

		int in_width = (int)top_df.getDim(ANN_TENSOR_WIDTH);
		int in_height = (int)top_df.getDim(ANN_TENSOR_HEIGHT);

		int inputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);
		int outputs = (int)top_df.getDim(ANN_TENSOR_DEPTH);
		int batch_sz = (int)bot_df.getDim(ANN_TENSOR_4THDIM);

		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: LRN back-propagation: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: CxWxHxOxB: %dx%dx%dx%dx%d\n", ident, " ", inputs, in_width, in_height, outputs, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;


	}

	return(ret);
}

int CDNN_Dnet_layer_LRN::ExecuteBwdHost(void)
{
	int ret = 0;


	int local_area = getLocalArea();

	int pre_pad = local_area - (local_area + 1) / 2;
	int pad = local_area - pre_pad - 1;

	CDNN_Tensor<aDType> & scale = *(CDNN_Tensor<aDType> *)findInternal(std::string("scale"));
	CDNN_Tensor<aDType> & top = getTopFwd();
	CDNN_Tensor<aDType> & bot = getBotFwd();
	CDNN_Tensor<aDType> & top_df = getTopDiff();
//	CDNN_Tensor<aDType> & bot_df = getBotDiff();
	CDNN_Tensor<aDType> & bot_df_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bot.diff.verify"));


	aDType * bot_ptr = bot.accessTensor(CL_MAP_READ);
	aDType * top_ptr = top.accessTensor(CL_MAP_READ);
	aDType * scale_ptr = scale.accessTensor(CL_MAP_READ);
	aDType * bot_df_v_ptr = bot_df_v.accessTensor(CL_MAP_WRITE);
	aDType * top_df_ptr = top_df.accessTensor(CL_MAP_READ);


	int outputs = (int)top.getDim(ANN_TENSOR_DEPTH);
	int inputs = (int)bot.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)top.getDim(ANN_TENSOR_4THDIM);
	int bot_width = (int)bot.getDim(ANN_TENSOR_WIDTH);
	int bot_height = (int)bot.getDim(ANN_TENSOR_HEIGHT);
	int top_width = (int)top.getDim(ANN_TENSOR_WIDTH);
	int top_height = (int)top.getDim(ANN_TENSOR_HEIGHT);
	int bot_stride = (int)bot.getStride(ANN_TENSOR_WIDTH);
	int bot_channel_stride = (int)bot.getStride(ANN_TENSOR_HEIGHT);
	int bot_batch_stride = (int)bot.getStride(ANN_TENSOR_DEPTH);

	int top_stride = (int)top.getStride(ANN_TENSOR_WIDTH);
	int top_channel_stride = (int)top.getStride(ANN_TENSOR_HEIGHT);
	int	top_batch_stride = (int)top.getStride(ANN_TENSOR_DEPTH);

	int scale_stride = (int)scale.getStride(ANN_TENSOR_WIDTH);
	int scale_channel_stride = (int)scale.getStride(ANN_TENSOR_HEIGHT);
	int	scale_batch_stride = (int)scale.getStride(ANN_TENSOR_DEPTH);

	int top_df_stride = (int)top_df.getStride(ANN_TENSOR_WIDTH);
	int top_df_channel_stride = (int)top_df.getStride(ANN_TENSOR_HEIGHT);
	int	top_df_batch_stride = (int)top_df.getStride(ANN_TENSOR_DEPTH);

	int bot_df_v_width = (int)bot_df_v.getDim(ANN_TENSOR_WIDTH);
	int bot_df_v_height = (int)bot_df_v.getDim(ANN_TENSOR_HEIGHT);
	int bot_df_v_stride = (int)bot_df_v.getStride(ANN_TENSOR_WIDTH);
	int bot_df_v_channel_stride = (int)bot_df_v.getStride(ANN_TENSOR_HEIGHT);
	int	bot_df_v_batch_stride = (int)bot_df_v.getStride(ANN_TENSOR_DEPTH);

	aDType alpha = (aDType)getAlphaLRN();
	aDType beta = (aDType)getBetaLRN();
	aDType negative_beta = -beta;

	if (getNormRegion() == dnet_LRN_norm_region_ACROSS_CHANNELS)
	{

		for (int b = 0; b < n_batchs; b++)
		{
			for (int j = 0; j < bot_height; j++)
			{
				for (int i = 0; i < bot_width; i++)
				{

					// c-emulator
					int head = 0;
					aDType accum_ratio = 0;
					// accumulate values
					while (head < pad) {

						aDType adder = (top_df_ptr[b*top_df_batch_stride + head * top_df_channel_stride + j * top_df_stride + i]
							* top_ptr[b*top_batch_stride + head * top_channel_stride + j * top_stride + i])
							/ scale_ptr[b*scale_batch_stride + head * scale_channel_stride + j * scale_stride + i];

#if 0
						if (i == 5 && j == 11/* && (head - pad) == 12 */ && b == 10)
						{
							printf("C:a %d %f %f\n",
								head,
								accum_ratio,
								adder
								);
						}
#endif


						accum_ratio += adder;


						++head;
					}
					// until we reach size, nothing needs to be subtracted
					while (head < local_area) {

						aDType adder = (top_df_ptr[b*top_df_batch_stride + head * top_df_channel_stride + j * top_df_stride + i]
							* top_ptr[b*top_batch_stride + head * top_channel_stride + j * top_stride + i])
							/ scale_ptr[b*scale_batch_stride + head * scale_channel_stride + j * scale_stride + i];

#if 0
						if (i == 5 && j == 11/* && (head - pad) == 12 */ && b == 10)
						{
							printf("C:a %d %f %f\n",
								head,
								accum_ratio,
								adder
								);
						}
#endif


						accum_ratio += adder;


						bot_df_v_ptr[b*bot_df_v_batch_stride + (head - pad) * bot_df_v_channel_stride + j * bot_df_v_stride + i] =
							top_df_ptr[b*top_df_batch_stride + (head - pad) * top_df_channel_stride + j * top_df_stride + i]
							* pow(scale_ptr[b*scale_batch_stride + (head - pad) * scale_channel_stride + j * scale_stride + i], negative_beta)
							- ratio_dta_bwd_ * bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i] * accum_ratio;

						++head;
					}
					// both add and subtract
					while (head < inputs) {

						aDType adder = top_df_ptr[b*top_df_batch_stride + head * top_df_channel_stride + j * top_df_stride + i]
							* top_ptr[b*top_batch_stride + head * top_channel_stride + j * top_stride + i]
							/ scale_ptr[b*scale_batch_stride + head * scale_channel_stride + j * scale_stride + i];

#if 0
						if (i == 5 && j == 11/* && (head - pad) == 12 */ && b == 10)
						{
							printf("C:a %d %f %f\n",
								head,
								accum_ratio,
								adder
								);
						}
#endif

						accum_ratio += adder;

						aDType subs = (top_df_ptr[b*top_df_batch_stride + (head - local_area) * top_df_channel_stride + j * top_df_stride + i]
							* top_ptr[b*top_batch_stride + (head - local_area) * top_channel_stride + j * top_stride + i])
							/ scale_ptr[b*scale_batch_stride + (head - local_area) * scale_channel_stride + j * scale_stride + i];



						accum_ratio -= subs;

#if 0
						if (i == 3 && j == 0/* && (head - pad) == 12 */ && b == 3)
						{
							printf("C: %d %16.12f %16.12f %16.12f %16.12f %16.12f\n",
								head,
								accum_ratio,
								adder,
								top_df_ptr[b*top_df_batch_stride + head * top_df_channel_stride + j * top_df_stride + i],
								top_ptr[b*top_batch_stride + head * top_channel_stride + j * top_stride + i],
								scale_ptr[b*scale_batch_stride + head * scale_channel_stride + j * scale_stride + i]
								);
						}
#endif


						bot_df_v_ptr[b*bot_df_v_batch_stride + (head - pad) * bot_df_v_channel_stride + j * bot_df_v_stride + i] =
							top_df_ptr[b*top_df_batch_stride + (head - pad) * top_df_channel_stride + j * top_df_stride + i]
							* pow(scale_ptr[b*scale_batch_stride + (head - pad) * scale_channel_stride + j * scale_stride + i], negative_beta)
							- ratio_dta_bwd_ * bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i] * accum_ratio;


						++head;
					}
					// subtract only
					while (head < inputs + pad) {

						aDType subs = (top_df_ptr[b*top_df_batch_stride + (head - local_area) * top_df_channel_stride + j * top_df_stride + i]
							* top_ptr[b*top_batch_stride + (head - local_area) * top_channel_stride + j * top_stride + i])
							/ scale_ptr[b*scale_batch_stride + (head - local_area) * scale_channel_stride + j * scale_stride + i];

						accum_ratio -= subs;






						bot_df_v_ptr[b*bot_df_v_batch_stride + (head - pad) * bot_df_v_channel_stride + j * bot_df_v_stride + i] =
							top_df_ptr[b*top_df_batch_stride + (head - pad) * top_df_channel_stride + j * top_df_stride + i]
							* pow(scale_ptr[b*scale_batch_stride + (head - pad) * scale_channel_stride + j * scale_stride + i], negative_beta)
							- ratio_dta_bwd_ * bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i] * accum_ratio;


#if 0
						if (i == 5 && j == 11/* && (head - pad) == 12 */ && b == 10)
						{
							printf("C: %d %10.8f %10.8f %10.8f %10.8f %10.8f %10.8f %10.8f\n",
								head,
								bot_df_v_ptr[b*bot_df_v_batch_stride + (head - pad) * bot_df_v_channel_stride + j * bot_df_v_stride + i],
								top_df_ptr[b*top_df_batch_stride + (head - pad) * top_df_channel_stride + j * top_df_stride + i],
								pow(scale_ptr[b*scale_batch_stride + (head - pad) * scale_channel_stride + j * scale_stride + i], negative_beta),
								scale_ptr[b*scale_batch_stride + (head - pad) * scale_channel_stride + j * scale_stride + i],
								- ratio_dta_bwd_ * bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i] * accum_ratio,
								bot_ptr[b*bot_batch_stride + (head - pad) * bot_channel_stride + j * bot_stride + i],
								accum_ratio
								);
						}
#endif

						++head;
					}
				

				}
			}
		}
	}
	else
	{
		for (int b = 0; b < n_batchs; b++)
		{
			for (int o = 0; o < inputs; o++)
			{
				for (int j = 0; j < bot_height; j++)
				{

					for (int i = 0; i < bot_width; i++)
					{
						aDType accum_ratio = 0;

						int hstart = j - pad;
						int wstart = i - pad;
						int hend = min(hstart + local_area, top_height + pad);
						int wend = min(wstart + local_area, top_width + pad);
						int adj_area_size = (hend - hstart) * (wend - wstart);
						hstart = max(hstart, 0);
						wstart = max(wstart, 0);
						hend = min(hend, top_height);
						wend = min(wend, top_width);
						for (int h = hstart; h < hend; ++h)
						{
							for (int w = wstart; w < wend; ++w)
							{
								aDType adder = top_df_ptr[b*top_df_batch_stride + o * top_df_channel_stride + h * top_df_stride + w]
									* top_ptr[b*top_batch_stride + o * top_channel_stride + h * top_stride + w]
									/ scale_ptr[b*scale_batch_stride + o * scale_channel_stride + h * scale_stride + w];

								accum_ratio += adder;

							}
						}

						aDType ratio_dta_bwd = (aDType) 2. * alpha * beta / adj_area_size;

						bot_df_v_ptr[b*bot_df_v_batch_stride + o * bot_df_v_channel_stride + j * bot_df_v_stride + i] =
							top_df_ptr[b*top_df_batch_stride + o * top_df_channel_stride + j * top_df_stride + i]
							* pow(scale_ptr[b*scale_batch_stride + o * scale_channel_stride + j * scale_stride + i], negative_beta)
							- ratio_dta_bwd * bot_ptr[b*bot_batch_stride + o * bot_channel_stride + j * bot_stride + i] * accum_ratio;

					}
				}
			}
		}


	}

	bot.commitTensor();
	top.commitTensor();
	scale.commitTensor();
	bot_df_v.commitTensor();
	top_df.commitTensor();

	return(ret);
}

int CDNN_Dnet_layer_LRN::internalVerifyBwd(void)
{
	int ret = 0;
	ExecuteBwdHost();
	CDNN_Tensor<aDType> & bot_df = getBotDiff();
	CDNN_Tensor<aDType> & bot_df_v = *(CDNN_Tensor<aDType> *)findInternal(std::string("bot.diff.verify"));
	aDType * bot_df_v_ptr = bot_df_v.accessTensor(CL_MAP_READ);
	aDType * bot_df_ptr = bot_df.accessTensor(CL_MAP_READ);

	int n_outputs = (int)bot_df.getDim(ANN_TENSOR_DEPTH);
	int n_batchs = (int)bot_df.getDim(ANN_TENSOR_4THDIM);

	int bot_df_width = (int)bot_df.getDim(ANN_TENSOR_WIDTH);
	int bot_df_height = (int)bot_df.getDim(ANN_TENSOR_HEIGHT);
	int bot_df_stride = (int)bot_df.getStride(ANN_TENSOR_WIDTH);
	int bot_df_channel_stride = (int)bot_df.getStride(ANN_TENSOR_HEIGHT);
	int	bot_df_batch_stride = (int)bot_df.getStride(ANN_TENSOR_DEPTH);
	int bot_df_v_stride = (int)bot_df_v.getStride(ANN_TENSOR_WIDTH);
	int bot_df_v_channel_stride = (int)bot_df_v.getStride(ANN_TENSOR_HEIGHT);
	int	bot_df_v_batch_stride = (int)bot_df_v.getStride(ANN_TENSOR_DEPTH);

	double sqr_accum = 0;
	double max_err = -FLT_MIN;
	int max_b = 0, max_o = 0, max_i = 0, max_j = 0;

	for (int b = 0; b < n_batchs; b++)
	{
		for (int o = 0; o < n_outputs; o++)
		{
			for (int j = 0; j < bot_df_height; j++)
			{
				for (int i = 0; i < bot_df_width; i++)
				{
					aDType c_val = bot_df_v_ptr[b*bot_df_v_batch_stride + o * bot_df_v_channel_stride + j * bot_df_v_stride + i];
					aDType g_val = bot_df_ptr[b*bot_df_batch_stride + o * bot_df_channel_stride + j * bot_df_stride + i];
					sqr_accum += (c_val - g_val) * (c_val - g_val);
					if (abs(c_val - g_val) > max_err)
					{
						max_err = abs(c_val - g_val);
						max_b = b;
						max_o = o;
						max_i = i;
						max_j = j;
					}

				}
			}
		}
	}

	sqr_accum = sqrt(sqr_accum / ((double)n_batchs *n_outputs * bot_df_height *bot_df_width));

	int match = 1;

	if (sqr_accum > 0)
	{
		std::cout << "Error in LRN back-propagation " << getName() + " : " << std::fixed << std::setw(15) << std::setprecision(13) << sqr_accum <<
			" Max err: " << std::fixed << std::setw(15) << std::setprecision(13) << max_err << std::endl;

		if (sqr_accum > 1. / 1000000000)
		{
			double allowedEps = 4;

			for (int b = 0; b < n_batchs && match; b++)
			{
				for (int o = 0; o < n_outputs && match; o++)
				{
					for (int j = 0; j < bot_df_height && match; j++)
					{
						for (int i = 0; i < bot_df_width && match; i++)
						{
							aDType c_val = bot_df_v_ptr[b*bot_df_v_batch_stride + o * bot_df_v_channel_stride + j * bot_df_v_stride + i];
							aDType g_val = bot_df_ptr[b*bot_df_batch_stride + o * bot_df_channel_stride + j * bot_df_stride + i];
							double err = CalculateErr(c_val, g_val);
							if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
							{
								std::cout << "Difference in LRN back propagation: " << getName() + " " << err << " too large at " << b << ", " << o << ", " << i << ", " << j << " c_v = " << c_val << " vs g_val = " << g_val << std::endl;
								match = 0;
							}

						}
					}
				}
			}
		}
	}

	bot_df_v.commitTensor();
	bot_df.commitTensor();

	if (match)
	{
		std::cout << "Passed varifier: layer: LRN back-propagation: " << getName() << std::endl;
	}


	return(ret);
}


////////////////////////////////////////////////////////////////////////////////////////////////
//
//  SoftMax
//
//////////////////////////////////////////////////////////////////////////////////////////////
CDNN_Dnet_layer_softmax::CDNN_Dnet_layer_softmax(void)
{
	labels_ = NULL;
}

CDNN_Dnet_layer_softmax::CDNN_Dnet_layer_softmax(CDNN_OVX * parent, void * wrapper, std::string name, const void * parameters, CDNN_Dnet_layer * ref_layer) :
	CDNN_Dnet_layer(parent, wrapper, name, parameters, ref_layer)
{
	labels_ = NULL;
}

int CDNN_Dnet_layer_softmax::Execute(void)
{
	int ret = 0;
	std::string bot_nm = getBottom();
	std::string top_nm = getTop();
	CDNN_Tensor<aDType> * in = &getBotFwd();
	CDNN_Tensor<aDType> * out = (doNeedBackProp()) ? &getBotDiff() : &getTopFwd();

	if (in && out)
	{
		cl_command_queue convQ = getaDNNOCL().getClQueue(0);
		int iter = getNTimingIter();
		double s = 0, e = 0;
		if (IsDoTiming())
		{
			s = mach_absolute_time();
		}


		for (int i = 0; i < iter; i++)
		{
			std::string kernel_name = (!doNeedBackProp()) ? "aDNN_SM" : "aDNN_SM_withLoss";
			cl_kernel ocl_kernel = cl_kernels_[getName() + "." + kernel_name];
			cl_mem in_mem = in->getCLMem();
			cl_mem out_mem = out->getCLMem();
			int height = (int)in->getDim(ANN_TENSOR_4THDIM);
			int width = (int)in->getDim(ANN_TENSOR_WIDTH);
			if (width <= (1 << 11))
			{

				size_t l_wk[3] = { ocl_group_sz0_, ocl_group_sz1_, 1 };

				size_t g_wk[3] = { ((width + ocl_group_sz0_ - 1) / ocl_group_sz0_)*ocl_group_sz0_, height, 1 };

				int n_arg = 0;

				ret = clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &in_mem);
				if (doNeedBackProp())
				{
					cl_mem label_mem = getLabels().getCLMem();
					ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &label_mem);
				}

				ret |= clSetKernelArg(ocl_kernel, n_arg++, sizeof(cl_mem), &out_mem);

				CHECK_OPENCL_ERROR(ret, "parmeters failed.");

				int iter = getNTimingIter();
				ret = clEnqueueNDRangeKernel(convQ, ocl_kernel, 3, NULL, g_wk, l_wk, 0, NULL, NULL);

				CHECK_OPENCL_ERROR(ret, "kernel direct transform  failed.");
			}
			else
			{
				ret = -1;
				printf("ERROR: softmax is not implemented for more than 2K categories\n");
			}
		}

		if (IsDoTiming())
		{
			clFinish(convQ);
			e = mach_absolute_time();
		}


		// verify
#if CNN_VERIFY
		internalVerify();
#endif

		int width = (int)in->getDim(ANN_TENSOR_WIDTH);
		int batch_sz = (int)in->getDim(ANN_TENSOR_4THDIM);


		iter = (iter <= 0) ? 1 : iter;
		processing_time_ = subtractTimes(e, s);
		int ident = 4;
		if (IsOutMessages())
		{
			printf("Passed layer: softmax: \"%s\"\n", getName().c_str());
			printf("%*s" "Arguments: LxB: %dx%d\n", ident, " ", width, batch_sz);
			if (IsDoTiming())
			{
				printf("%*s" "Performance: %6.2f ms\n", ident, " ", processing_time_ / iter);
			}
		}
		ret = 0;

	}

	return(ret);
}

int CDNN_Dnet_layer_softmax::ExecuteHost(void)
{
	int ret = 0;
	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	CDNN_Tensor<aDType> * bot = &getBotFwd();


	if (top_verify && bot)
	{
		top_verify->allocTensor(_CBUF_MEM_SYS_ONLY);

		int bot_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);
		int top_stride = (int)top_verify->getStride(ANN_TENSOR_DEPTH);

		int top_rows = (int)top_verify->getDim(ANN_TENSOR_4THDIM);
		int top_cols = (int)top_verify->getDim(ANN_TENSOR_WIDTH);
		aDType * label_ptr = 0;
		if (doNeedBackProp())
		{
			label_ptr = getLabels().accessTensor(CL_MAP_READ);
		}
		aDType * bot_ptr = bot->accessTensor(CL_MAP_READ);
		aDType * top_ptr = top_verify->accessTensor(CL_MAP_WRITE);

		aDType* accum = new aDType[top_rows];
// find max
		for (int j = 0; j < top_rows; ++j)
		{
#if 0
			if (j == 0)
			{
				printf("C:in: %d  %f\n", 0, bot_ptr[j*bot_stride]);
			}
#endif
			accum[j] = bot_ptr[j*bot_stride];
			for (int i = 1; i < top_cols; ++i)
			{
#if 0
				if (j == 0)
				{
					printf("C:in: %d  %f\n", i, bot_ptr[j*bot_stride + i]);
				}
#endif
				accum[j] = max(accum[j], bot_ptr[j*bot_stride + i]);
			}
		}

// substruct and exp
		for (int j = 0; j < top_rows; ++j)
		{
#if 0
			printf("C:max: %d  %f\n", j, accum[j]);
#endif
			for (int i = 0; i < top_cols; ++i)
			{
				aDType sub_val = bot_ptr[j*bot_stride + i] - accum[j];
				top_ptr[j*top_stride + i] = exp(sub_val);
#if 0
				if (j == 21 && i ==0)
				{
					printf("C:exp: %d  %f %f\n", i, sub_val, top_ptr[j*top_stride + i]);
				}
#endif
			}
		}

// sum up
		for (int j = 0; j < top_rows; ++j)
		{
			accum[j] = 0;
			for (int i = 0; i < top_cols; ++i)
			{
				accum[j] += top_ptr[j*top_stride + i];
			}
		}


// divide
		for (int j = 0; j < top_rows; ++j)
		{
			aDType scaler = 1.f/accum[j];
#if 0
			printf("C:sum: %d  %f %f\n", j, accum[j], scaler);
#endif
			for (int i = 0; i < top_cols; ++i)
			{
#if 0
				if (j == 21 && i == 0)
				{
					printf("C:scl: %f %f\n", top_ptr[j*top_stride + i], scaler);
				}
#endif
				top_ptr[j*top_stride + i] *= scaler;

			}


	
		}


		for (int j = 0; label_ptr && j < top_rows; ++j)
		{

			int index = (int)label_ptr[j];
#if 0
			if (j == 21)
			{
				printf("C:idx: %f %d %f %f\n", label_ptr[j], index, top_ptr[j*top_stride + index] - 1.f, top_ptr[j*top_stride + 0]);
			}
#endif
			top_ptr[j*top_stride + index] -= 1;
			for (int i = 0; i < top_cols; ++i)
			{
				top_ptr[j*top_stride + i] /= top_rows;
#if 0
					if (j == 21 && i == 0)
					{
						printf("C: %f\n", top_ptr[j*top_stride + i]);
					}
#endif
			}
			

		}



		if (accum)
		{
			delete[] accum;
		}

		bot->commitTensor();
		top_verify->commitTensor();
		if (doNeedBackProp())
		{
			getLabels().commitTensor();
		}

	}
	return(ret);
}

int CDNN_Dnet_layer_softmax::internalSetup(void)
{
	int ret = 0;
	size_t batch_sz = 0;
	size_t inputs = 0;
	CDNN_Tensor<aDType> * bot = NULL;

	if (bottom_lyr_)
	{
//		std::string top_nm = getBottom();
		bot = &bottom_lyr_->getTopFwd();
		size_t w = bot->getDim(ANN_TENSOR_WIDTH);
		size_t h = bot->getDim(ANN_TENSOR_HEIGHT);
		size_t c = bot->getDim(ANN_TENSOR_DEPTH);
		inputs = w*h*c;
		batch_sz = bot->getDim(ANN_TENSOR_4THDIM);

	}


	CDNN_Tensor<aDType> * top = NULL;
// if no top layers (last layer in the net) and need back prop use Bot diff
	top = (!getTopLyr() && doNeedBackProp()) ? &getBotDiff() : &getTopFwd();

	if (top)
	{

		if (!top->IsInited())
		{
			top->setName(getTopName());
			top->setParent(parent_);
			size_t dims[4];
			dims[ANN_TENSOR_WIDTH] = inputs;
			dims[ANN_TENSOR_HEIGHT] = 1;
			dims[ANN_TENSOR_DEPTH] = 1;
			dims[ANN_TENSOR_4THDIM] = batch_sz;

			top->initTensor(4, dims);

		}

		if (doNeedBackProp())
		{

// TEMP create LABELs HERE artificially !!!
			CDNN_Tensor<aDType> * labels = new CDNN_Tensor<aDType>(parent_, NULL, "labels", batch_sz);
			addInput("labels", labels);
			labels->allocTensor();
			aDType * lbls = labels->accessTensor(CL_MAP_WRITE_INVALIDATE_REGION);
			// assign category randomly
			for (int i = 0; i < batch_sz; i++)
			{
				float cat = (float)rand() *  inputs / (float)RAND_MAX;
				lbls[i] = (aDType)(int)cat;
			}
			labels->commitTensor();
			labels_ = labels;
		}
	}

#if CNN_VERIFY
	if (top)
	{
		// add system only for verification
		CDNN_Tensor<aDType> * top_verify = new CDNN_Tensor<aDType>(&getParent(), NULL, std::string("top.verify"), top->getDim(ANN_TENSOR_WIDTH), top->getDim(ANN_TENSOR_HEIGHT), top->getDim(ANN_TENSOR_DEPTH), top->getDim(ANN_TENSOR_4THDIM));
		addInternal(std::string("top.verify"), top_verify);
	}

#endif

	int in_len = (int)inputs;
	ocl_group_sz1_ = 1;
	ocl_group_sz0_ = (in_len <= 64) ? 64 : (in_len <= 128) ? 128 : (in_len <= 192) ? 192 : 256;
	ocl_group_lg2sz0_ = (int)ceil(log((double)ocl_group_sz0_) / log(2.));
	int in_loop = ((int)inputs + ocl_group_sz0_ - 1) / ocl_group_sz0_;
	int in_align_len = in_loop * ocl_group_sz0_;
	int lcl_data_lg2len = (int)ceil(log((double)in_align_len) / log(2.));
	int lcl_data_len = (1 << lcl_data_lg2len);

	int in_stride = (int)bot->getStride(ANN_TENSOR_DEPTH);
	int	out_stride = (int)top->getStride(ANN_TENSOR_DEPTH);


	std::string comp_options =
		std::string(" -D _DNN_SM_GROUP_SZ0=") + std::to_string((long long)ocl_group_sz0_)
		+ std::string(" -D _DNN_SM_GROUP_SZ1=") + std::to_string((long long)ocl_group_sz1_)
		+ std::string(" -D _DNN_SM_GROUP_LG2SZ0=") + std::to_string((long long)ocl_group_lg2sz0_)
		+ std::string(" -D _DNN_SM_IN_LEN=") + std::to_string((long long)in_len)
		+ std::string(" -D _DNN_SM_IN_LOOP=") + std::to_string((long long)in_loop)
		+ std::string(" -D _DNN_SM_LCL_DATA_LEN=") + std::to_string((long long)lcl_data_len)
		+ std::string(" -D _DNN_SM_IN_STRIDE=") + std::to_string((long long)in_stride)
		+ std::string(" -D _DNN_SM_OUT_STRIDE=") + std::to_string((long long)out_stride)
		+ std::string(" -D _DNN_SM_LCL_DATA_LG2LEN=") + std::to_string((long long)lcl_data_lg2len)

		+ parent_->getGenericCompOptions()
		;

	std::string kernel_file = "aDNNSoftMax.cl";
	std::string kernel_name = (!doNeedBackProp()) ? "aDNN_SM" : "aDNN_SM_withLoss";


	cl_kernel ocl_kernel = getaDNNOCL().getKernel(kernel_file, kernel_name, comp_options);

	cl_kernels_[getName() + "." + kernel_name] = ocl_kernel;


	return(ret);
}
int CDNN_Dnet_layer_softmax::internalVerify(void)
{
	int ret = 0;
	ExecuteHost();

	CDNN_Tensor<aDType> * out = (doNeedBackProp()) ? &getBotDiff() : &getTopFwd();

	CDNN_Tensor<aDType> * top_verify = (CDNN_Tensor<aDType> *)findInternal(std::string("top.verify"));
	if (out && top_verify)
	{
		aDType * out_ptr = out->accessTensor(CL_MAP_READ);
		aDType * top_ptr = top_verify->accessTensor(CL_MAP_READ);
		int top_rows = (int)top_verify->getDim(ANN_TENSOR_4THDIM);
		int top_cols = (int)top_verify->getDim(ANN_TENSOR_WIDTH);
		int out_stride = (int)out->getStride(ANN_TENSOR_DEPTH);
		int top_stride = (int)top_verify->getStride(ANN_TENSOR_DEPTH);
		int match = 1;
		for (int j = 0; j < top_rows && match; ++j)
		{
			for (int i = 0; i < top_cols && match; i++)
			{
				aDType c_val = top_ptr[j * top_stride +i];
				aDType g_val = out_ptr[j * out_stride +i];
				double err = CalculateErr(c_val, g_val);
				double allowedEps = 3;
				if (err > allowedEps || std::isnan(c_val) || std::isnan(g_val))
				{
					std::cout << "Difference in softmax: " << getName() + "" << err << " too large at " << i << "," << j << " c_v = " << c_val << " vs g_v = " << g_val << std::endl;
					match = 0;
				}

			}
		}
		if (match)
		{
			std::cout << "Passed varifier: layer: softmax: " << getName() << std::endl;
		}

		out->commitTensor();
		top_verify->commitTensor();

	}


	return(ret);
}





///////////////////////////////////////////////////////
//
// DNN_DNet
//
//////////////////////////////////////////////////////

CDNN_Dnet::CDNN_Dnet() : CDNN_Object()
{
	layers_.clear();
	net_.clear();
	processing_time_ = 0;
}


CDNN_Dnet :: CDNN_Dnet(CDNN_OVX * parent, void * wrapper, std::string name, const dnet_parameters & parameters):
	CDNN_Object(parent, wrapper, name,	ADNN_OBJECT_DNET)
{

	parameters_ = parameters;
	layers_.clear();
	net_.clear();
	processing_time_ = 0;
}

CDNN_Dnet:: ~CDNN_Dnet()
{
}

void CDNN_Dnet :: addLayer(CDNN_Dnet_layer & _layer)
{
// broadcast parameters
	_layer.setDoTiming(false);
	_layer.setNTimingIter(1);
	_layer.setOutMessages(false);
	if (IsPerLayerTiming() && !IsNetTiming())
	{
		_layer.setDoTiming(IsPerLayerTiming());
		_layer.setNTimingIter(getNetPerLayerIter());

	}

	if (!IsNetTiming())
	{
		_layer.setOutMessages(IsPerLayerMessages());
	}

	layers_[_layer.getName()] = &_layer;
	_layer.setNet(this);
}

CDNN_Dnet_layer * CDNN_Dnet :: getNextLayer(CDNN_Dnet_layer * prev)
{
	const CDNN_Dnet_layer * ret = NULL;
	if ( !prev)
	{

		for(std::map<std::string, void*>::iterator l_it = layers_.begin(); l_it != layers_.end(); ++l_it)
		{
			std::string & ltr_nm = (std::string &)(*l_it).first;
			CDNN_Dnet_layer * lyr = (CDNN_Dnet_layer *)(*l_it).second;
			if (!lyr->getBottomLyr())
			{
				ret = lyr;
				break;
			}
		}

	}
	else
	{
		ret = prev->getTopLyr();
	}
	return((CDNN_Dnet_layer * )ret);
}

CDNN_Dnet_layer * CDNN_Dnet::getRNextLayer(CDNN_Dnet_layer * prev)
{
	const CDNN_Dnet_layer * ret = NULL;
	if (!prev)
	{

		for (std::map<std::string, void*>::iterator l_it = layers_.begin(); l_it != layers_.end(); ++l_it)
		{
			std::string & ltr_nm = (std::string &)(*l_it).first;
			CDNN_Dnet_layer * lyr = (CDNN_Dnet_layer *)(*l_it).second;
// last layer in the net
			if (lyr->IsLastLayer())
			{
				ret = lyr;
				break;
			}
		}

	}
	else
	{
		ret = prev->getBottomLyr();
	}
	return((CDNN_Dnet_layer *)ret);
}


int CDNN_Dnet :: buildNet(void)
{
	int ret = 0;
	
// find bottom layer:
//input bottom name is empty
	std::map<std::string, void*>::iterator bot_it;
	std::map<std::string, void*>::iterator top_it;
	
	CDNN_Dnet_layer * bot_layer = NULL;
// search the empty top
	int found_top = 0;
	for(std::map<std::string, void*>::iterator l_it = layers_.begin(); l_it != layers_.end() && !found_top; ++l_it)
	{
		std::string & lyr_nm = (std::string &)(*l_it).first;
		CDNN_Dnet_layer * lyr = (CDNN_Dnet_layer *)(*l_it).second;
		const std::string & top = lyr->getTop();
		if (!top.compare(""))
		{

			top_it = l_it;
			found_top = 1;
		}
	}
	
// found
	if (!found_top )
	{
	}
	else
	{
		std::map<std::string, void*>::iterator cur_it = top_it;
		while(true)
		{
			CDNN_Tensor<aDType> * top = NULL;
			CDNN_Dnet_layer * cur_lyr = (CDNN_Dnet_layer *)(*cur_it).second;

// OLD fasion
			std::string top_nm = cur_lyr->getTop();

			if (!cur_lyr->IsLastLayer())
			{
				top = (CDNN_Tensor<aDType> *)cur_lyr->findOutput(top_nm);
			}
	// artificial output or backward diff

			if (!top || cur_lyr->IsLastLayer())
			{
				top = new CDNN_Tensor<aDType>;
	// OLD fasion
				cur_lyr->addOutput(top_nm, top);

			}
			if (cur_lyr->doNeedBackProp())
			{
				CDNN_Tensor<aDType> * bot_diff = (cur_lyr->IsLastLayer()) ? top : new CDNN_Tensor < aDType >;
				assert(bot_diff);
				cur_lyr->setBotDiff(bot_diff);
			}


			cur_lyr->setTopFwd(top);

// find a layer with the bottom top == current bottom
		
			int found_bottom= 0;
			for(std::map<std::string, void*>::iterator l_it2 = layers_.begin(); l_it2 != layers_.end() && !found_bottom; ++l_it2)
			{
				std::string & ltr_nm = (std::string &)(*l_it2).first;
				CDNN_Dnet_layer * lyr = (CDNN_Dnet_layer *)(*l_it2).second;
				const std::string & bot = lyr->getTop();
				if (!bot.compare(cur_lyr->getBottom()) || !cur_lyr->getBottom().compare(""))
				{
					bot_it = l_it2;
					found_bottom = 1;
				}
			}

// found
			if (found_bottom )
			{
				const std::string & bot_nm = cur_lyr->getBotName();
				bot_layer = (cur_lyr->IsFirstLayer()) ? NULL : (CDNN_Dnet_layer *)(*bot_it).second;
// no input tensor - set input layer to 0
				if ( bot_layer )
				{

// instantiate input edge (data flow forward)

// do layer specific staff
					CDNN_Tensor<aDType> * bot = &cur_lyr->getBotFwd();
					if (!bot)
					{
						// 
						bot = new CDNN_Tensor < aDType > ;
						assert(bot);
					}


// OLD FASION
					cur_lyr->addInput(bot_nm, bot);
					bot_layer->addOutput(bot_nm, bot);

					cur_lyr->setBotFwd(bot);
					bot_layer->setTopFwd(bot);

// source data layer does not need diff 

// TO DO: HOW TO SKIP layers that does not need backward propagation?

					if (bot_layer->getType() != VX_LAYER_DATA && bot_layer->doNeedBackProp())
					{


						CDNN_Tensor<aDType> * top_diff = &cur_lyr->getBotDiff();
						bot_layer->setTopDiff(top_diff);

					}

					cur_lyr->setBottomLyr(bot_layer);
					bot_layer->setTopLyr(cur_lyr);

				}
				else
				{
					break;
				}

				cur_it = bot_it;

				
			}
			else
			{
				break;
			}
			

		}
	}
	return(ret);
}

int CDNN_Dnet::oclFinish(void)
{
	int ret = 0;
	cl_command_queue convQ = getaDNNOCL().getClQueue(0);
	clFinish(convQ);
	return(ret);
}



} // adnn






